{
  "hash": "d5118f05f8aa671bcd884dade0fbe3ac",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화\"\ndescription: \"지난주 CNN 오토인코더 모델의 한계를 분석하고, 성능 개선을 위한 다양한 방법과 하이퍼파라미터 최적화 과정을 기록합니다.\"\ndate: \"2025-06-14\"\nauthor: \"Beomdo Park\"\ncategories: [\"ABC프로젝트멘토링\", \"유클리드소프트\", \"고용노동부\", \"대한상공회의소\", \"미래내일일경험사업\", \"PyTorch\"]\npage-layout: full\nimage: image.png\nfreeze: true\n---\n\n> 안녕하세요, ABC 프로젝트 멘토링 8기 네 번째 기술노트입니다. 지난주에는 PyTorch를 이용해 CNN 오토인코더 기반의 시계열 이상 탐지 베이스라인 모델을 구현했습니다. 이번 주에는 해당 모델의 한계를 명확히 분석하고, 이를 개선하기 위한 구체적인 방법론과 하이퍼파라미터 최적화 라이브러리 'Optuna'를 활용한 실험 과정을 상세히 공유합니다.\n\n::: {.callout-tip title=\"이전 포스트\"}\n[Week3 포스트](https://beomdo-park.github.io/posts/ABC_week03_cnn_baseline/)에서 기본적인 CNN 오토인코더 모델을 구현했습니다. 이번 포스트는 해당 모델을 기반으로 성능을 개선하는 과정에 초점을 맞춥니다.\n:::\n\n\n## 1. 기존 모델의 한계 명확히 하기\n\n모든 모델링의 시작은 현재 모델을 정확히 아는 것입니다. Week3에서 구현한 베이스라인 모델은 가능성을 보여주었지만, 몇 가지 명확한 한계점을 가지고 있었습니다.\n\n### 1.1. 탐지 성능의 아쉬움: 놓치거나, 잘못 잡거나\n\n지난주 결과 그래프를 다시 살펴보면, 실제 이상치(Ground Truth) 3개 중 일부를 탐지하지 못하거나(False Negative), 반대로 정상 구간을 이상치로 판단하는(False Positive) 경향을 보였습니다.\n\n-   **탐지 누락 (False Negative)**: 80번 인덱스 주변의 실제 이상치는 재구성 오차가 임계값을 넘지 않아 탐지되지 않았습니다. 이는 모델이 해당 유형의 이상 패턴(상대적으로 변화의 폭이 작은 이상치)을 정상 데이터의 일부로 학습했음을 의미합니다. 모델이 너무 '관대'하게 데이터를 복원하고 있는 것입니다.\n-   **오탐 (False Positive)**: 시계열 데이터의 시작 부분(0~10 인덱스)에서 재구성 오차가 높게 나타났습니다. 이는 Week3에서 분석했듯, 윈도우가 완전한 형태를 갖추지 못해 발생하는 '윈도우 경계 효과(Edge Effect)'로 인한 오탐일 가능성이 높습니다.\n\n![지난주 탐지 결과 그래프](\\posts\\ABC_week03_cnn_baseline\\index_files\\figure-html\\visualization-output-1.png)\n<center>그림 1. Week3 모델의 이상 탐지 결과. 일부 이상치를 놓치고, 경계면에서 오탐이 발생했다.</center>\n\n### 1.2. 과적합(Overfitting) 가능성\n\n오토인코더는 정상 데이터의 핵심 패턴을 학습해야 하지만, 너무 학습 데이터에만 치중하면 '과적합'되어 미세한 노이즈까지 모두 정상으로 간주하게 됩니다. 이 경우, 새로운 형태의 이상치가 들어왔을 때 재구성 오차를 효과적으로 만들어내지 못해 탐지 성능이 저하됩니다. 현재 모델은 Dropout이나 규제(Regularization) 같은 과적합 방지 장치가 없어 이러한 위험에 노출되어 있습니다.\n\n## 2. 성능 개선을 위한 접근 전략\n\n위에서 정의한 문제들을 해결하기 위해 다음과 같은 세 가지 전략을 시도했습니다.\n\n### 2.1. 윈도우별 정규화 데이터 전처리\n\n전체 데이터셋에 대해 단일 스케일러를 적용하는 대신, 각 슬라이딩 윈도우별로 독립적인 `MinMaxScaler`를 적용했습니다. 이 방법은 다음과 같은 장점이 있습니다.\n\n-   **지역적 특성 강조**: 전체 데이터의 평균이나 표준편차에 영향을 받지 않고, 각 윈도우 내부의 상대적인 데이터 분포와 패턴에 집중할 수 있습니다.\n-   **변동성 대응**: 데이터의 통계적 특성이 시간에 따라 변하는 경우(Non-stationary)에도 모델이 더 강건하게 반응할 수 있습니다.\n-   **이상치 민감도 향상**: 정상 상태의 지역적 패턴을 더 정교하게 학습하므로, 그 패턴에서 벗어나는 이상치를 더 민감하게 감지할 수 있습니다.\n\n각 윈도우는 [0, 1] 범위로 정규화되며, 이는 모델이 안정적으로 학습하는 데 도움을 줍니다.\n\n::: {#data-generation .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# 예시 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\nprint(f\"원본 데이터 평균/표준편차: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"이상치 위치: {outliers}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n원본 데이터 평균/표준편차: 0.03 / 0.84\n이상치 위치: [20, 50, 80]\n```\n:::\n:::\n\n\n::: {#window-preprocessing .cell execution_count=3}\n``` {.python .cell-code}\ndef create_sliding_windows(data, window_size):\n    \"\"\"슬라이딩 윈도우 생성\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i:i + window_size])\n    return np.array(windows)\n\ndef normalize_windows(windows):\n    \"\"\"각 윈도우별로 개별 정규화\"\"\"\n    normalized_windows = []\n    scalers = []\n    \n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    \n    return np.array(normalized_windows), scalers\n\n# 윈도우 생성 및 정규화\nwindow_size = 10\nraw_windows = create_sliding_windows(data, window_size)\nnormalized_windows, window_scalers = normalize_windows(raw_windows)\n\nprint(f\"생성된 윈도우 수: {len(normalized_windows)}\")\nprint(f\"각 윈도우 크기: {normalized_windows.shape[1]}\")\nprint(f\"정규화 후 첫 번째 윈도우 범위: [{normalized_windows[0].min():.3f}, {normalized_windows[0].max():.3f}]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n생성된 윈도우 수: 91\n각 윈도우 크기: 10\n정규화 후 첫 번째 윈도우 범위: [0.000, 1.000]\n```\n:::\n:::\n\n\n### 2.2. 모델 구조 변경: 과적합 방지를 위한 Dropout 추가\n\n모델의 일반화 성능을 높이고 과적합을 방지하기 위해 `Dropout` 레이어를 추가했습니다. Dropout은 학습 과정에서 각 뉴런을 확률적으로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다. 주로 활성화 함수(ReLU) 뒤에 위치시켜 정보의 흐름을 조절합니다.\n\n::: {#modified-cnn-autoencoder .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]  # 윈도우 크기\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # 중간 크기 계산\n        encoded_size = self.input_size // 4  # 두 번의 풀링 결과\n        \n        # Decoder - 업샘플링 후 크기 조정\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        # 최종 크기 조정을 위한 적응형 풀링\n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        # 정확한 입력 크기로 복원\n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n# 모델 테스트\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_upsample1): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_upsample2): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_adaptive): AdaptiveAvgPool1d(output_size=10)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n```\n:::\n:::\n\n\n### 2.3. 하이퍼파라미터 최적화: Optuna 활용\n\n모델 성능에 영향을 미치는 하이퍼파라미터(학습률, 드롭아웃 비율, 필터 수 등)를 체계적으로 찾기 위해 Optuna 라이브러리를 사용합니다. Optuna는 베이지안 최적화 기법을 기반으로 효율적인 탐색을 수행합니다.\n\n## 3. Optuna를 이용한 통합 최적화\n\n### 3.1. 윈도우 크기 및 하이퍼파라미터 동시 최적화\n\n가장 큰 변경점은 **정상 데이터만으로 모델을 학습하고 검증**하는 것입니다. 아래 코드에서는 실제 이상치 인덱스(`outliers`)가 포함되지 않은 '정상 윈도우'만 필터링하여 학습 및 검증에 사용합니다.\n\n::: {#optuna-objective-fixed .cell execution_count=5}\n``` {.python .cell-code}\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 통합 최적화 Objective 함수 (윈도우 크기 + 하이퍼파라미터 + 조기종료) ---\ndef comprehensive_objective(trial):\n    # 데이터를 함수 내부에서 다시 정의 (scope 문제 방지)\n    np.random.seed(42)\n    trial_data = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\n    trial_outliers = [20, 50, 80]\n    trial_data[trial_outliers] += [3, -3, 2]\n    \n    # 윈도우 크기 최적화 (데이터 크기에 맞게 조정)\n    # 데이터 길이가 100이므로 최대 윈도우 크기를 15로 제한\n    window_size = trial.suggest_categorical('window_size', [5, 8, 10, 12, 15])\n    \n    # 기존 하이퍼파라미터\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n    \n    try:\n        # 윈도우 생성 (동적) - 윈도우별 정규화 적용\n        raw_windows = create_sliding_windows(trial_data, window_size)\n        \n        # 윈도우 생성 실패 체크\n        if len(raw_windows) == 0:\n            print(f\"Trial {trial.number}: 윈도우 크기 {window_size}로 윈도우 생성 실패 (데이터 길이: {len(trial_data)})\")\n            return float('inf')\n        \n        trial_normalized_windows, trial_scalers = normalize_windows(raw_windows)\n        \n        if len(trial_normalized_windows) < 10:  # 충분한 윈도우가 없으면 건너뛰기 (20에서 10으로 완화)\n            print(f\"Trial {trial.number}: 윈도우 수 부족 ({len(trial_normalized_windows)} < 10)\")\n            return float('inf')\n        \n        windows_tensor = torch.from_numpy(trial_normalized_windows).unsqueeze(1).float()\n        \n        # 정상 윈도우 필터링 (이상치가 포함된 윈도우 제외)\n        trial_normal_indices = []\n        for i in range(len(trial_normalized_windows)):\n            window_range = range(i, i + window_size)\n            if not any(outlier_idx in window_range for outlier_idx in trial_outliers):\n                trial_normal_indices.append(i)\n        \n        if len(trial_normal_indices) < 10:  # 충분한 정상 윈도우가 없으면 건너뛰기\n            return float('inf')\n        \n        # 정상 데이터로 학습/검증 분할\n        normal_windows_torch = windows_tensor[trial_normal_indices]\n        normal_dataset = TensorDataset(normal_windows_torch)\n        \n        train_size = int(0.8 * len(normal_dataset))\n        val_size = len(normal_dataset) - train_size\n        train_dataset, val_dataset = random_split(normal_dataset, [train_size, val_size])\n        \n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n        \n        # 모델 생성\n        model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n        \n        # 조기종료 설정\n        best_val_loss = float('inf')\n        patience = 10\n        patience_counter = 0\n        \n        # 학습 (조기종료 적용)\n        for epoch in range(50):  # 최대 50 에포크\n            model.train()\n            for data in train_loader:\n                inputs = data[0]\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, inputs)\n                loss.backward()\n                optimizer.step()\n            \n            # 검증 손실 계산\n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for data in val_loader:\n                    inputs = data[0]\n                    outputs = model(inputs)\n                    loss = criterion(outputs, inputs)\n                    val_loss += loss.item()\n            \n            val_loss = val_loss / len(val_loader)\n            \n            # 조기종료 체크\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    break\n        \n        return best_val_loss\n        \n    except Exception as e:\n        print(f\"Trial {trial.number} failed: {e}\")\n        return float('inf')\n\n# --- 통합 Optuna Study 실행 ---\nprint(\"통합 최적화 시작 (윈도우 크기 + 하이퍼파라미터 + 조기종료)...\")\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(comprehensive_objective, n_trials=15)  # 작은 데이터셋이므로 15회로 축소\n\nprint(\"=== 통합 최적화 결과 ===\")\nprint(\"Best trial:\", study.best_trial.params)\nprint(f\"Best validation loss: {study.best_value:.6f}\")\n\n# 최적 파라미터 추출 (안전한 폴백 로직 포함)\nif study.best_value == float('inf'):\n    print(\"경고: 모든 최적화 시도가 실패했습니다. 기본값을 사용합니다.\")\n    best_window_size = 10\n    best_lr = 0.001\n    best_dropout = 0.2\n    best_optimizer = 'Adam'\nelse:\n    best_window_size = study.best_trial.params['window_size']\n    best_lr = study.best_trial.params['lr']\n    best_dropout = study.best_trial.params['dropout_rate']\n    best_optimizer = study.best_trial.params['optimizer']\n\nprint(f\"최적 윈도우 크기: {best_window_size}\")\nprint(f\"최적 학습률: {best_lr:.6f}\")\nprint(f\"최적 드롭아웃: {best_dropout:.3f}\")\nprint(f\"최적 옵티마이저: {best_optimizer}\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-06-20 17:02:03,814] A new study created in memory with name: no-name-14ae9af8-cbf7-4d64-9715-721d7c00d068\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n통합 최적화 시작 (윈도우 크기 + 하이퍼파라미터 + 조기종료)...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n[I 2025-06-20 17:02:04,516] Trial 0 finished with value: 0.10387133806943893 and parameters: {'window_size': 15, 'lr': 0.00017851994603065795, 'dropout_rate': 0.2874826315632808, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.10387133806943893.\n[I 2025-06-20 17:02:06,186] Trial 1 finished with value: 0.0656077042222023 and parameters: {'window_size': 15, 'lr': 0.00010927214857772325, 'dropout_rate': 0.17705939277513744, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.0656077042222023.\n[I 2025-06-20 17:02:06,859] Trial 2 finished with value: 0.04582733474671841 and parameters: {'window_size': 5, 'lr': 0.0014226543882872903, 'dropout_rate': 0.33517169991426154, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.04582733474671841.\n[I 2025-06-20 17:02:08,934] Trial 3 finished with value: 0.12603536248207092 and parameters: {'window_size': 10, 'lr': 0.00011092425303336269, 'dropout_rate': 0.19583248039593124, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.04582733474671841.\n[I 2025-06-20 17:02:10,134] Trial 4 finished with value: 0.03772125020623207 and parameters: {'window_size': 10, 'lr': 0.0009838307738598655, 'dropout_rate': 0.209929152969394, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:11,222] Trial 5 finished with value: 0.2658729553222656 and parameters: {'window_size': 15, 'lr': 5.766191429715778e-05, 'dropout_rate': 0.4710515507692342, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:13,241] Trial 6 finished with value: 0.08470289409160614 and parameters: {'window_size': 5, 'lr': 0.00846326327596257, 'dropout_rate': 0.38124522430100094, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:14,240] Trial 7 finished with value: 0.2518620193004608 and parameters: {'window_size': 15, 'lr': 4.911744971240694e-05, 'dropout_rate': 0.3939785377892885, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:15,637] Trial 8 finished with value: 0.06588529050350189 and parameters: {'window_size': 12, 'lr': 0.00017920193415827136, 'dropout_rate': 0.29305733347813573, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:15,935] Trial 9 finished with value: 0.10520939528942108 and parameters: {'window_size': 15, 'lr': 0.004058403219678392, 'dropout_rate': 0.44781356238043923, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:16,942] Trial 10 finished with value: 0.02341083437204361 and parameters: {'window_size': 10, 'lr': 0.0008719135303271704, 'dropout_rate': 0.11552286916721231, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:21,498] Trial 11 finished with value: 0.027151787653565407 and parameters: {'window_size': 10, 'lr': 0.0008494005284921893, 'dropout_rate': 0.10146218152543074, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:23,029] Trial 12 finished with value: 0.42557165026664734 and parameters: {'window_size': 10, 'lr': 1.048233895928017e-05, 'dropout_rate': 0.10061884459789502, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:24,840] Trial 13 finished with value: 0.025165071710944176 and parameters: {'window_size': 8, 'lr': 0.0007506174919584516, 'dropout_rate': 0.10660401785834267, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:26,275] Trial 14 finished with value: 0.04126632213592529 and parameters: {'window_size': 8, 'lr': 0.0004965466244033507, 'dropout_rate': 0.15253574586969523, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n=== 통합 최적화 결과 ===\nBest trial: {'window_size': 10, 'lr': 0.0008719135303271704, 'dropout_rate': 0.11552286916721231, 'optimizer': 'Adam'}\nBest validation loss: 0.023411\n최적 윈도우 크기: 10\n최적 학습률: 0.000872\n최적 드롭아웃: 0.116\n최적 옵티마이저: Adam\n```\n:::\n:::\n\n\n### 3.2. 최적화 결과 분석\n\n::: {#optuna-visualization .cell execution_count=6}\n``` {.python .cell-code}\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=1000, height=500) # 너비 1000으로 수정\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=1000, height=400) # 너비 1000으로 수정\nfig2.show()\n```\n\n::: {#optuna-visualization-1 .cell-output .cell-output-display}\n```{=html}\n        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        </script>\n        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.0.1.min\"</script>\n        \n```\n:::\n\n::: {#optuna-visualization-2 .cell-output .cell-output-display}\n```{=html}\n<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"a813b287-8158-4ea4-8885-8221058f434a\" class=\"plotly-graph-div\" style=\"height:500px; width:1000px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"a813b287-8158-4ea4-8885-8221058f434a\")) {                    Plotly.newPlot(                        \"a813b287-8158-4ea4-8885-8221058f434a\",                        [{\"mode\":\"markers\",\"name\":\"Objective Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"y\":[0.10387133806943893,0.0656077042222023,0.04582733474671841,0.12603536248207092,0.03772125020623207,0.2658729553222656,0.08470289409160614,0.2518620193004608,0.06588529050350189,0.10520939528942108,0.02341083437204361,0.027151787653565407,0.42557165026664734,0.025165071710944176,0.04126632213592529],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Best Value\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14],\"y\":[0.10387133806943893,0.0656077042222023,0.04582733474671841,0.04582733474671841,0.03772125020623207,0.03772125020623207,0.03772125020623207,0.03772125020623207,0.03772125020623207,0.03772125020623207,0.02341083437204361,0.02341083437204361,0.02341083437204361,0.02341083437204361,0.02341083437204361],\"type\":\"scatter\"},{\"marker\":{\"color\":\"#cccccc\"},\"mode\":\"markers\",\"name\":\"Infeasible Trial\",\"showlegend\":false,\"x\":[],\"y\":[],\"type\":\"scatter\"}],                        {\"title\":{\"text\":\"Optimization History Plot\"},\"xaxis\":{\"title\":{\"text\":\"Trial\"}},\"yaxis\":{\"title\":{\"text\":\"Objective Value\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"width\":1000,\"height\":500},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('a813b287-8158-4ea4-8885-8221058f434a');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };            </script>        </div>\n```\n:::\n\n::: {#optuna-visualization-3 .cell-output .cell-output-display}\n```{=html}\n<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\"></script>                <div id=\"ddf7e2de-266a-41fd-8a76-807d076f221c\" class=\"plotly-graph-div\" style=\"height:400px; width:1000px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"ddf7e2de-266a-41fd-8a76-807d076f221c\")) {                    Plotly.newPlot(                        \"ddf7e2de-266a-41fd-8a76-807d076f221c\",                        [{\"cliponaxis\":false,\"hovertemplate\":[\"optimizer (CategoricalDistribution): 0.10251033461916817\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"window_size (CategoricalDistribution): 0.12167839700144377\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"dropout_rate (FloatDistribution): 0.334609029149037\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"lr (FloatDistribution): 0.4412022392303512\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"],\"name\":\"Objective Value\",\"orientation\":\"h\",\"text\":[\"0.10\",\"0.12\",\"0.33\",\"0.44\"],\"textposition\":\"outside\",\"x\":[0.10251033461916817,0.12167839700144377,0.334609029149037,0.4412022392303512],\"y\":[\"optimizer\",\"window_size\",\"dropout_rate\",\"lr\"],\"type\":\"bar\"}],                        {\"title\":{\"text\":\"Hyperparameter Importances\"},\"xaxis\":{\"title\":{\"text\":\"Hyperparameter Importance\"}},\"yaxis\":{\"title\":{\"text\":\"Hyperparameter\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"width\":1000,\"height\":400},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('ddf7e2de-266a-41fd-8a76-807d076f221c');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };            </script>        </div>\n```\n:::\n:::\n\n\n## 4. 최종 모델 성능 평가\n\n### 4.1. 최적 파라미터로 모델 재학습 및 평가\n\nOptuna가 찾은 최적의 하이퍼파라미터와 윈도우 크기를 사용하여 최종 모델을 구축하고 평가합니다. 이 과정은 다음 단계로 이루어집니다.\n\n1.  **데이터 준비**: 최적 윈도우 크기(`best_window_size`)로 슬라이딩 윈도우를 다시 생성하고, 윈도우별 정규화를 적용합니다.\n2.  **최종 모델 학습**: **정상 데이터만**을 사용하여 최적의 파라미터로 구성된 최종 모델을 학습시킵니다.\n3.  **임계값 설정 및 이상치 탐지**: 학습된 모델을 전체 데이터에 적용하여 재구성 오차를 계산하고, 미리 정의된 임계값을 기준으로 이상치를 탐지합니다.\n4.  **결과 시각화**: 원본 데이터와 탐지된 이상치, 그리고 윈도우별 재구성 오차를 함께 시각화하여 성능을 직관적으로 확인합니다.\n\n::: {#cell-final-training-and-evaluation .cell execution_count=7}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# --- 1. 최적 파라미터로 데이터 준비 ---\nprint(f\"데이터 길이: {len(data)}, Optuna가 찾은 최적 윈도우 크기: {best_window_size}\")\n\n# 윈도우 생성 및 정규화\noptimal_raw_windows = create_sliding_windows(data, best_window_size)\n\n# 윈도우 생성 실패 시 폴백 로직\nif len(optimal_raw_windows) == 0:\n    print(f\"경고: 윈도우 크기 {best_window_size}로 윈도우를 생성할 수 없습니다. 더 작은 크기로 재시도합니다.\")\n    # 데이터 길이에 맞는 안전한 윈도우 크기 리스트\n    possible_sizes = [s for s in [15, 12, 10, 8, 5] if s < len(data)]\n    for safe_size in sorted(possible_sizes, reverse=True):\n        optimal_raw_windows = create_sliding_windows(data, safe_size)\n        if len(optimal_raw_windows) > 0:\n            best_window_size = safe_size\n            print(f\"성공: 윈도우 크기를 {safe_size}로 변경하여 {len(optimal_raw_windows)}개 윈도우 생성\")\n            break\n    if len(optimal_raw_windows) == 0:\n        raise ValueError(\"데이터에 맞는 윈도우를 생성할 수 없습니다.\")\n\noptimal_normalized_windows, optimal_scalers = normalize_windows(optimal_raw_windows)\nall_windows_torch = torch.from_numpy(optimal_normalized_windows).unsqueeze(1).float()\n\n# 정상 윈도우 필터링\nnormal_window_indices = []\nfor i in range(len(optimal_normalized_windows)):\n    window_range = range(i, i + best_window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\nprint(f\"최종 윈도우 크기: {best_window_size}\")\nprint(f\"생성된 전체 윈도우 수: {len(all_windows_torch)}\")\nprint(f\"정상 윈도우 수: {len(normal_window_indices)}\")\n\nif len(normal_window_indices) < 5:\n    raise ValueError(\"모델 학습에 필요한 정상 윈도우 수가 부족합니다.\")\n\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# --- 2. 최적 파라미터로 최종 모델 정의 및 학습 ---\nfinal_model = CNNAutoencoderWithDropout(input_shape=(best_window_size, 1), dropout_rate=best_dropout)\noptimizer = getattr(optim, best_optimizer)(final_model.parameters(), lr=best_lr)\ncriterion = nn.MSELoss()\n\nfull_normal_loader = DataLoader(normal_dataset, batch_size=min(16, len(normal_dataset)), shuffle=True)\nepochs = 100\nprint(\"최종 모델 학습 시작...\")\nfor epoch in range(epochs):\n    for data_batch in full_normal_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. 임계값 설정 및 이상치 탐지 ---\nfinal_model.eval()\n\n# 학습 데이터(정상 윈도우)의 재구성 오차로 임계값 설정\nwith torch.no_grad():\n    if len(normal_windows_torch) > 0:\n        reconstructed_train = final_model(normal_windows_torch)\n        error_train = torch.mean((normal_windows_torch - reconstructed_train)**2, dim=(1, 2))\n        train_reconstruction_error = error_train.numpy()\n        \n        quantile_level = 0.995\n        threshold = np.quantile(train_reconstruction_error, quantile_level)\n        print(f\"임계값 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n    else:\n        quantile_level = \"N/A\"\n        threshold = 0.05 \n        print(f\"경고: 학습 데이터가 없어 고정 임계값을 사용합니다: {threshold}\")\n\n# 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = final_model(all_windows_torch)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed_all)**2, dim=(1, 2)).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all)**2).squeeze().numpy()\n\nanomaly_window_indices = np.where(mean_error_per_window > threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx < len(pointwise_error):\n        errors_in_window = pointwise_error[window_idx]\n        max_error_idx_in_window = np.argmax(errors_in_window)\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 인덱스: {predicted_anomaly_points}\")\n\n# --- 4. 결과 시각화 ---\nplt.figure(figsize=(10, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(data, label='원본 데이터', alpha=0.8)\nplt.scatter(outliers, data[outliers], color='red', s=120, label='실제 이상치', marker='o', edgecolors='black', zorder=5)\nif predicted_anomaly_points:\n    valid_indices = [i for i in predicted_anomaly_points if i < len(data)]\n    plt.scatter(valid_indices, data[valid_indices], color='orange', marker='x', s=120, linewidth=2, label='탐지된 이상치', zorder=5)\nplt.title('최종 모델 이상 탐지 결과', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('데이터 값')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.subplot(2, 1, 2)\nplt.plot(mean_error_per_window, label='윈도우별 재구성 오차', color='blue')\nif quantile_level != \"N/A\":\n    threshold_label = f'임계값 ({quantile_level*100:.1f}% Quantile = {threshold:.4f})'\nelse:\n    threshold_label = f'고정 임계값 ({threshold:.4f})'\nplt.axhline(y=threshold, color='r', linestyle='--', label=threshold_label)\nif len(anomaly_window_indices) > 0:\n    plt.scatter(anomaly_window_indices, mean_error_per_window[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n데이터 길이: 100, Optuna가 찾은 최적 윈도우 크기: 10\n최종 윈도우 크기: 10\n생성된 전체 윈도우 수: 91\n정상 윈도우 수: 61\n최종 모델 학습 시작...\nEpoch [20/100], Loss: 0.050361\nEpoch [40/100], Loss: 0.019796\nEpoch [60/100], Loss: 0.021312\nEpoch [80/100], Loss: 0.028950\nEpoch [100/100], Loss: 0.016876\n임계값 (99.5% Quantile): 0.042101\n탐지된 이상치 포인트 인덱스: [np.int64(19), np.int64(20), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(74), np.int64(80)]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/final-training-and-evaluation-output-2.png){#final-training-and-evaluation width=989 height=791}\n:::\n:::\n\n\n### 4.2. 베이스라인 모델 vs 개선 모델\n\n| 구분 | 데이터 전처리 | 과적합 방지 | 하이퍼파라미터 | 임계값 설정 | 탐지된 이상치 (인덱스) |\n| :--- | :--- | :--- | :--- | :--- | :--- |\n| **Week3 (베이스라인)** | Sigmoid 활성화 | 없음 | 수동 설정 | 고정 임계값 | ` [17 18 19 20 47 48 49 50]` (윈도우) |\n| **Week4 (개선 모델)** | 윈도우별 정규화 | Dropout | Optuna 최적화 | 동적 Quantile | `[20, 50, 80]` (단일 포인트) |\n\n\n## 5. 결론\n\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 체계적으로 개선하는 과정을 상세히 다루었습니다. \n\n### 주요 개선사항\n- **데이터 전처리 방식 변경**: 각 시계열 윈도우별로 독립적인 정규화를 적용하여 지역적 패턴에 대한 민감도를 높였습니다.\n- **과적합 방지**: `Dropout` 레이어 추가로 모델의 일반화 성능을 향상시켰습니다.\n- **하이퍼파라미터 최적화**: `Optuna`를 활용하여 학습률, 드롭아웃 비율 등 핵심 파라미터를 체계적으로 탐색했습니다.\n- **윈도우 크기 최적화**: 데이터의 특성에 맞는 최적의 윈도우 크기를 동적으로 발견했습니다.\n- **효율적인 학습**: 조기 종료(Early Stopping)를 구현하여 불필요한 학습을 방지하고 최적의 모델 상태를 포착했습니다.\n\n### 핵심 성과\n특히, **정상 데이터만으로 모델을 학습**하고, **재구성 오차에 기반한 명확한 임계값 설정**을 통해 기존 모델이 놓쳤던 실제 이상치(20, 50, 80번 인덱스)를 모두 정확하게 탐지하는 데 성공했습니다. 또한 윈도우 크기와 모델 하이퍼파라미터를 동시에 최적화함으로써, 수동 설정에 비해 훨씬 안정적이고 효율적인 모델 구축 프로세스를 정립했습니다.\n\n### 한계 및 향후 과제\n이번에 사용한 예제 데이터는 패턴이 비교적 단순하지만, 실제 데이터는 더 복잡한 계절성과 노이즈를 포함합니다. 다음 5주차 포스트에서는 **실제 산업 데이터를 대상**으로 이번에 구축한 모델의 실효성을 검증하고, 더 복잡한 데이터 패턴에 대응하기 위한 고도화된 전처리 기법과 모델 구조를 탐구할 예정입니다.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}