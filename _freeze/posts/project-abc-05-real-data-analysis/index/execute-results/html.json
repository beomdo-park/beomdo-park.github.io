{
  "hash": "b28ff329fbebb3f258ff2b675f6ee456",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지\"\ndescription: \"이전 주차에서 개발한 CNN 오토인코더 모델을 실제 Kaggle의 주택 전력 사용량 데이터에 적용하여, 현실 데이터에서 발생하는 이상 패턴을 탐지하는 과정을 다룹니다.\"\ndate: \"2025-06-19\"\nauthor: \"Beomdo Park\"\ncategories: [\"ABC프로젝트멘토링\", \"PyTorch\"]\ntags: [\"실제데이터\", \"Kaggle\", \"CNN오토인코더\", \"이상탐지\"]\npage-layout: full\nfreeze: true\n\n---\n\n> 안녕하세요, ABC 프로젝트 멘토링 8기 다섯 번째 기술노트입니다. Week04에서 개발한 CNN 오토인코더 모델을 실제 Kaggle 공개 데이터셋(주택 전력 사용량 3년치)에 적용해, 실전 환경에서의 이상 탐지 성능과 한계를 점검합니다. 이 과정을 통해, 이론적 모델이 실제 데이터에서 어떻게 동작하는지, 그리고 실무에서 마주칠 수 있는 문제와 해결책을 탐구합니다.\n\n::: {.callout-tip title=\"이전 포스트\"}\n[Week4 포스트](https://beomdo-park.github.io/posts/ABC_week04_model_optimization/)에서 CNN 오토인코더 모델의 성능 개선과 하이퍼파라미터 최적화 과정을 다뤘습니다. 이번 포스트는 해당 모델을 실제 데이터에 적용하는 실전 단계에 초점을 맞춥니다.\n:::\n\n## 1. 데이터 소개 및 준비\n\n이번 주에 사용할 데이터는 Kaggle에 공개된 **'Residential Power Usage 3-Years Data'**입니다. 한 가정의 3년간 전력 사용량이 분 단위로 기록된 시계열 데이터로, 실제 환경에서 발생하는 다양한 패턴과 이상 현상을 포함하고 있습니다.\n\n먼저, GitHub Raw URL을 통해 데이터를 불러오고, 시계열 분석을 위해 날짜 컬럼을 인덱스로 변환한 뒤, 전력 사용량 컬럼만 추출합니다.\n\n\n::: {#data-loading .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# GitHub Raw URL을 통해 데이터 불러오기\nurl = 'https://raw.githubusercontent.com/beomdo-park/ml-dl-by-dataset/main/datasets/power_usage_2016_to_2020.csv'\n\nprint(\"데이터 로딩 시작...\")\ndf = pd.read_csv(url)\nprint(\"데이터 로딩 완료.\")\n\ndf.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n데이터 로딩 시작...\n데이터 로딩 완료.\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 35952 entries, 0 to 35951\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   StartDate    35952 non-null  object \n 1   Value (kWh)  35952 non-null  float64\n 2   day_of_week  35952 non-null  int64  \n 3   notes        35952 non-null  object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n```\n:::\n:::\n\n\n데이터는 'StartDate', 'Value (kWh)', 'day_of_week', 'notes' 등의 컬럼으로 구성되어 있습니다. 시계열 분석을 위해 'StartDate'를 datetime 형식으로 변환하고 인덱스로 설정한 뒤, 'Value (kWh)' 컬럼만 사용하겠습니다. 전체 데이터를 사용하여 분석을 진행합니다.\n\n::: {#cell-data-preprocessing .cell execution_count=3}\n``` {.python .cell-code}\nprint(\"데이터 전처리 시작...\")\n# 'StartDate'를 datetime으로 변환하고 인덱스로 설정\ndf[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\ndf = df.set_index(\"StartDate\")\n\n# 시간순으로 정렬\ndf.sort_index(inplace=True)\nprint(\"데이터 시간순 정렬 완료.\")\n\n# 'Value (kWh)' 컬럼만 선택\nvalue_col = \"Value (kWh)\"\ndf_value = df[[value_col]].copy()\n\n# 시간 단위를 'H'로 재샘플링하고 누락된 값은 선형 보간\nprint(\"데이터를 시간 단위로 재샘플링하고 누락된 값을 보간합니다...\")\ndf_value = df_value.resample(\"H\").mean()\ndf_value[value_col] = df_value[value_col].interpolate(method=\"linear\")\nprint(\"재샘플링 및 보간 완료.\")\n\n# [수정] 분석 효율성을 위해 2019년 데이터만 사용\nprint(\"분석 효율성을 위해 2019년 데이터만 사용합니다...\")\ndf_value = df_value[df_value.index.year == 2019].copy()\nprint(\"데이터 슬라이싱 완료.\")\n\n\n# 데이터 시각화 (2019년)\nplt.figure(figsize=(10, 5))\nplt.plot(df_value.index, df_value[value_col], label=\"전력 사용량 (2019년)\")\nplt.title(\"시간에 따른 전력 사용량 (2019년)\")\nplt.xlabel(\"날짜\")\nplt.ylabel(\"사용량 (kWh)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"2019년 데이터 크기: {df_value.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n데이터 전처리 시작...\n데이터 시간순 정렬 완료.\n데이터를 시간 단위로 재샘플링하고 누락된 값을 보간합니다...\n재샘플링 및 보간 완료.\n분석 효율성을 위해 2019년 데이터만 사용합니다...\n데이터 슬라이싱 완료.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_8580\\3802000988.py:16: FutureWarning:\n\n'H' is deprecated and will be removed in a future version, please use 'h' instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/data-preprocessing-output-3.png){#data-preprocessing width=830 height=465}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n2019년 데이터 크기: (8760, 1)\n```\n:::\n:::\n\n\n## 2. 데이터 전처리 및 계절성 제거\n\n실제 전력 데이터는 강한 계절성과 일간 패턴을 보입니다. 효과적인 이상 탐지를 위해 다단계 계절성 제거를 적용한 후 윈도우 생성과 정규화를 수행합니다.\n\n::: {#data-processing-and-windowing .cell execution_count=4}\n``` {.python .cell-code}\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# --- Helper Functions ---\ndef create_sliding_windows(data, window_size):\n    \"\"\"슬라이딩 윈도우 생성\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i : i + window_size])\n    return np.array(windows)\n\n\ndef normalize_windows(windows):\n    \"\"\"각 윈도우별로 개별 정규화\"\"\"\n    normalized_windows = []\n    scalers = []\n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    return np.array(normalized_windows), scalers\n\n\ndef seasonal_decomposition_approach(data, index):\n    \"\"\"STL 분해를 통한 계절성 제거\"\"\"\n    # 데이터가 너무 짧으면 주기를 조정\n    period = 8760  # 연간 주기 (시간 단위)\n    if len(data) < 2 * period:\n        period = 24 * 365  # 근사치\n        if len(data) < 2 * period:\n            period = 24 * 30  # 월간 주기\n            if len(data) < 2 * period:\n                period = 24 * 7  # 주간 주기\n\n    # STL의 seasonal 파라미터는 반드시 홀수여야 함\n    if period % 2 == 0:\n        period += 1\n\n    ts = pd.Series(data, index=index[: len(data)])\n    stl = STL(ts, seasonal=period, robust=True)\n    result = stl.fit()\n    deseasonalized = ts - result.seasonal\n    return deseasonalized.values, result\n\n\ndef comprehensive_deseasonalize(data, index, window_size):\n    \"\"\"다단계 계절성 제거 및 윈도우 생성\"\"\"\n    print(\"1단계: STL 분해로 주요 계절성 제거...\")\n    deseason_data, decomp_result = seasonal_decomposition_approach(data, index)\n\n    print(\"2단계: 일간 패턴 제거 (24시간 이동평균)...\")\n    daily_smooth = pd.Series(deseason_data).rolling(24, center=True).mean()\n    deseason_data = deseason_data - daily_smooth.fillna(0).values\n\n    print(\"3단계: 윈도우 생성 및 정규화...\")\n    windows = create_sliding_windows(deseason_data, window_size)\n    normalized_windows, scalers = normalize_windows(windows)\n\n    return normalized_windows, scalers, decomp_result\n\n\n# --- Data Extraction ---\nraw_data = df_value[value_col].values\nprint(f\"원본 데이터 크기: {len(raw_data)}\")\n\n# --- 1. 계절성 분해 시각화 ---\n_, decomp_result_for_plot = seasonal_decomposition_approach(raw_data, df_value.index)\n\nplt.figure(figsize=(10, 8)) # 너비 10으로 수정\nplt.subplot(4, 1, 1)\nplt.plot(decomp_result_for_plot.observed)\nplt.title('원본 데이터 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 2)\nplt.plot(decomp_result_for_plot.trend)\nplt.title('트렌드 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 3)\nplt.plot(decomp_result_for_plot.seasonal)\nplt.title('계절성 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 4)\n# 계절성 제거된 데이터 계산\ndesasonalized_for_plot = (\n    decomp_result_for_plot.observed - decomp_result_for_plot.seasonal\n)\nplt.plot(desasonalized_for_plot)\nplt.title('계절성 제거 후 (2019년)')\nplt.ylabel(\"kWh\")\nplt.xlabel(\"시간\")\n\nplt.tight_layout()\nplt.show()\n\n# --- 2. 최종 데이터 처리 및 윈도우 생성 ---\nprint(\"\\n다단계 계절성 제거 및 윈도우 생성 시작...\")\nwindow_size = 10 # Week4 Optuna 최적값\nprocessed_windows, window_scalers, decomp_result = comprehensive_deseasonalize(\n    raw_data, df_value.index, window_size\n)\n\nprint(f\"계절성 제거 후 생성된 윈도우 수: {len(processed_windows)}\")\nprint(f\"각 윈도우 크기: {processed_windows.shape[1]}\")\n\n# PyTorch 텐서로 변환\nall_windows_torch = torch.from_numpy(processed_windows).unsqueeze(1).float()\nprint(f\"텐서 형태: {all_windows_torch.shape}\")\n\n# --- 3. 처리 전후 비교 시각화 ---\nplt.figure(figsize=(10, 6))\nplt.subplot(2, 1, 1)\nplt.plot(raw_data, alpha=0.8, label='원본 데이터 (2019년)')\nplt.title('원본 전력 사용량 데이터 (2019년)')\nplt.ylabel('kWh')\nplt.legend()\n\nplt.subplot(2, 1, 2)\ndeseason_full = raw_data - decomp_result.seasonal.values\nplt.plot(deseason_full, alpha=0.8, label='계절성 제거 후', color=\"orange\")\nplt.title('계절성 제거 후 데이터 (2019년)')\nplt.ylabel('kWh')\nplt.xlabel('시간')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n원본 데이터 크기: 8760\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/data-processing-and-windowing-output-2.png){#data-processing-and-windowing-1 width=989 height=788}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n다단계 계절성 제거 및 윈도우 생성 시작...\n1단계: STL 분해로 주요 계절성 제거...\n2단계: 일간 패턴 제거 (24시간 이동평균)...\n3단계: 윈도우 생성 및 정규화...\n계절성 제거 후 생성된 윈도우 수: 8751\n각 윈도우 크기: 10\n텐서 형태: torch.Size([8751, 1, 10])\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/data-processing-and-windowing-output-4.png){#data-processing-and-windowing-2 width=989 height=589}\n:::\n:::\n\n\n## 3. 모델 학습 및 이상치 탐지\n\nWeek4에서 최적화한 CNN 오토인코더 모델을 사용하여 실제 전력 데이터에 대한 이상치 탐지를 수행합니다. 학습, 평가, 시각화 과정을 하나의 논리적 흐름으로 통합하여 설명합니다.\n\n### 3.1. 모델 정의\n\nWeek4에서 `Optuna`를 통해 최적화한 모델 구조와 하이퍼파라미터를 그대로 사용합니다. 이 모델은 `Dropout`을 포함하여 과적합을 방지하고, `ConvTranspose1d` 대신 `Upsample`과 `AdaptiveAvgPool1d`를 사용하여 다양한 윈도우 크기에 유연하게 대응할 수 있도록 개선되었습니다.\n\n::: {#model-definition .cell execution_count=5}\n``` {.python .cell-code}\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n```\n:::\n\n\n### 3.2. 학습 및 평가\n\n실제 데이터에서는 패턴이 안정화된 구간을 '정상'으로 간주하고 학습하는 것이 중요합니다. 전체 데이터의 **20% ~ 80% 구간**을 정상 데이터로 정의하고, 이 데이터로만 모델을 학습시킵니다. 그 후, 학습된 모델을 전체 데이터에 적용하여 이상치를 탐지합니다.\n\n::: {#training-and-evaluation .cell execution_count=6}\n``` {.python .cell-code}\n# --- 1. 학습 데이터 분할 ---\ntotal_windows = len(all_windows_torch)\ntrain_start_idx = int(total_windows * 0.2)\ntrain_end_idx = int(total_windows * 0.8)\ntrain_windows_torch = all_windows_torch[train_start_idx:train_end_idx]\n\nprint(f\"전체 윈도우 수: {total_windows}\")\nprint(f\"학습 구간: {train_start_idx} ~ {train_end_idx}\")\nprint(f\"학습에 사용할 윈도우 수: {len(train_windows_torch)}\")\n\n# --- 2. 모델 초기화 및 학습 ---\n# Week4에서 찾은 최적 하이퍼파라미터 사용\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.137)\noptimizer = optim.RMSprop(model.parameters(), lr=0.000981)\ncriterion = nn.MSELoss()\n\ndataset = TensorDataset(train_windows_torch)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(\"\\n모델 학습 시작...\")\nepochs = 15\nfor epoch in range(epochs):\n    model.train()\n    for data_batch in data_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. 임계값 설정 및 이상치 탐지 ---\nmodel.eval()\n\n# 학습 데이터의 재구성 오차로 임계값 설정\nwith torch.no_grad():\n    reconstructed_train = model(train_windows_torch)\n    error_train = torch.mean(\n        (train_windows_torch - reconstructed_train) ** 2, dim=(1, 2)\n    )\n    train_reconstruction_error = error_train.numpy()\n\nquantile_level = 0.99\nthreshold = np.quantile(train_reconstruction_error, quantile_level)\nprint(f\"\\n임계값 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = model(all_windows_torch)\n    reconstruction_error = torch.mean(\n        (all_windows_torch - reconstructed_all) ** 2, dim=(1, 2)\n    ).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all) ** 2).squeeze().numpy()\n\n# 이상치 탐지\nanomaly_window_indices = np.where(reconstruction_error > threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx < len(pointwise_error):\n        max_error_idx_in_window = np.argmax(pointwise_error[window_idx])\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\npredicted_anomaly_points = [\n    idx for idx in predicted_anomaly_points if idx < len(raw_data)\n]\nprint(f\"탐지된 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n전체 윈도우 수: 8751\n학습 구간: 1750 ~ 7000\n학습에 사용할 윈도우 수: 5250\n\n모델 학습 시작...\nEpoch [5/15], Loss: 0.020301\nEpoch [10/15], Loss: 0.027192\nEpoch [15/15], Loss: 0.044883\n\n임계값 (99.0% Quantile): 0.123072\n탐지된 이상치 포인트 수: 79\n```\n:::\n:::\n\n\n### 3.3. 결과 시각화\n\n탐지된 이상치를 원본 데이터와 함께 시각화하여 모델의 성능을 직관적으로 확인합니다. 전체 기간과 최근 3개월 구간을 나누어 상세히 분석합니다.\n\n::: {#cell-final-visualization .cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 9))\n\n# 상단: 전체 데이터와 탐지 결과\nplt.subplot(3, 1, 1)\nplt.plot(raw_data, label='원본 전력 사용량 (2019년)', alpha=0.7, color='blue', linewidth=0.8)\nif predicted_anomaly_points:\n    plt.scatter(predicted_anomaly_points, raw_data[predicted_anomaly_points],\n                color='red', marker='x', s=80, linewidth=2, label=f'탐지된 이상치 ({len(predicted_anomaly_points)}개)', zorder=5)\nplt.title('실제 전력 사용량 데이터 이상 탐지 결과 (2019년)', fontsize=14, fontweight='bold')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 중간: 재구성 오차와 임계값\nplt.subplot(3, 1, 2)\nplt.plot(reconstruction_error, label='윈도우별 재구성 오차', color='steelblue', linewidth=1)\nplt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'임계값 ({threshold:.4f})')\nif anomaly_window_indices.any():\n    plt.scatter(anomaly_window_indices, reconstruction_error[anomaly_window_indices], \n               c='red', s=60, alpha=0.8, label=f'이상 윈도우 ({len(anomaly_window_indices)}개)', zorder=5)\nplt.title('윈도우별 재구성 오차 분포 (2019년)', fontsize=14, fontweight='bold')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 하단: 세분화된 구간 시각화 (맨 뒤 3개월치 데이터)\nplt.subplot(3, 1, 3)\nmonth_hours = 90 * 24\nstart_idx_viz = max(0, len(raw_data) - month_hours)\nend_idx_viz = len(raw_data)\n\nplt.plot(range(start_idx_viz, end_idx_viz), raw_data[start_idx_viz:end_idx_viz], \n         label=f'전력 사용량 (2019년, 최근 3개월)', alpha=0.8, color='navy', linewidth=1)\n\nmonth_anomalies = [i for i in predicted_anomaly_points if start_idx_viz <= i < end_idx_viz]\nif month_anomalies:\n    plt.scatter(month_anomalies, raw_data[month_anomalies],\n                color='red', marker='o', s=100, alpha=0.8, \n                label=f'최근 3개월 이상치 ({len(month_anomalies)}개)', zorder=5)\n\nplt.title('세분화된 이상 탐지 결과 (2019년, 최근 3개월)', fontsize=14, fontweight='bold')\nplt.xlabel('시간 스텝')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# 탐지 결과 요약 출력\nprint(f\"\\n=== 이상 탐지 결과 요약 (2019년) ===\")\nprint(f\"분석 데이터 길이: {len(raw_data):,} 시간\")\nprint(f\"전체 탐지된 이상치: {len(predicted_anomaly_points)}개\")\nprint(f\"최근 3개월 구간 이상치: {len(month_anomalies)}개\")\nprint(f\"이상치 비율: {len(predicted_anomaly_points) / len(raw_data) * 100:.3f}%\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/final-visualization-output-1.png){#final-visualization width=982 height=890}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=== 이상 탐지 결과 요약 (2019년) ===\n분석 데이터 길이: 8,760 시간\n전체 탐지된 이상치: 79개\n최근 3개월 구간 이상치: 26개\n이상치 비율: 0.902%\n```\n:::\n:::\n\n\n## 4. 결론\n\n이번 5주차에서는 지난 4주간 개발해온 CNN 오토인코더 모델을 실제 전력 사용량 데이터에 적용해봤습니다. 단순히 모델을 돌려보는 데서 그치지 않고, 실제 데이터가 가진 복잡함과 예측 불가능성을 직접 마주하며, 이론과 실전의 간극을 좁히는 과정을 경험했습니다.\n\n### 진행 과정\n\n- **체계적인 개선**  \n    Week 4에서 정립한 ‘윈도우별 정규화 → Dropout으로 과적합 방지 → Optuna 하이퍼파라미터 최적화 → Quantile 기반 임계값 설정’이라는 프로세스가 실제 데이터에서도 효과적이었습니다. 감에 의존하지 않고 논리적으로 접근하는 것이 얼마나 중요한지 다시 한 번 확인할 수 있었습니다.\n\n- **데이터를 중심에 두기**  \n    실제 데이터는 노이즈와 계절성이 강하게 섞여 있습니다. STL 분해와 이동평균을 결합한 다단계 전처리 덕분에, 모델이 본질적인 패턴에 집중할 수 있었고, 이상 신호도 더 명확하게 잡아낼 수 있었습니다. 결국 모델링의 핵심은 데이터를 얼마나 잘 이해하고 다루느냐에 달려 있다는 점을 실감했습니다.\n\n- **‘정상’의 기준 고민하기**  \n    실제 데이터에서는 ‘정상’이 무엇인지 정의하는 것부터 쉽지 않습니다. 데이터의 20%~80% 구간을 정상으로 간주해 학습에 사용했고, 덕분에 모델이 전체 변동성에 휘둘리지 않고 진짜 이상 신호에 집중할 수 있었습니다. 이상 탐지에서는 도메인 지식과 합리적인 가정이 필수라는 점도 다시 느꼈습니다.\n\nWeek 1의 데이터 탐색부터 Week 5의 실제 데이터 적용까지, 시계열 이상 탐지라는 목표를 향해 한 단계씩 나아갔습니다. 단순한 베이스라인에서 출발해 점진적으로 성능을 개선하고, 실제 데이터의 복잡성까지 다룰 수 있게 된 이번 과정은, 모델링이 단순히 코드를 짜는 일이 아니라 문제를 정의하고, 가설을 세우고, 실험하고, 검증하는 일련의 탐구라는 사실을 다시 한 번 확인했습니다.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}