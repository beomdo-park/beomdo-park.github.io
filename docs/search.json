[
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html",
    "href": "posts/ABC_week04_model_optimization/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 네 번째 기술노트입니다. 지난주에는 PyTorch를 이용해 CNN 오토인코더 기반의 시계열 이상 탐지 베이스라인 모델을 구현했습니다. 이번 주에는 해당 모델의 한계를 명확히 분석하고, 이를 개선하기 위한 구체적인 방법론과 하이퍼파라미터 최적화 라이브러리 ’Optuna’를 활용한 실험 과정을 상세히 공유합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "href": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "1. 기존 모델의 한계 명확히 하기",
    "text": "1. 기존 모델의 한계 명확히 하기\n모든 모델링의 시작은 현재 모델을 정확히 아는 것입니다. Week3에서 구현한 베이스라인 모델은 가능성을 보여주었지만, 몇 가지 명확한 한계점을 가지고 있었습니다.\n\n1.1. 탐지 성능의 아쉬움: 놓치거나, 잘못 잡거나\n지난주 결과 그래프를 다시 살펴보면, 실제 이상치(Ground Truth) 3개 중 일부를 탐지하지 못하거나(False Negative), 반대로 정상 구간을 이상치로 판단하는(False Positive) 경향을 보였습니다.\n\n탐지 누락 (False Negative): 80번 인덱스 주변의 실제 이상치는 재구성 오차가 임계값을 넘지 않아 탐지되지 않았습니다. 이는 모델이 해당 유형의 이상 패턴(상대적으로 변화의 폭이 작은 이상치)을 정상 데이터의 일부로 학습했음을 의미합니다. 모델이 너무 ’관대’하게 데이터를 복원하고 있는 것입니다.\n오탐 (False Positive): 시계열 데이터의 시작 부분(0~10 인덱스)에서 재구성 오차가 높게 나타났습니다. 이는 Week3에서 분석했듯, 윈도우가 완전한 형태를 갖추지 못해 발생하는 ’윈도우 경계 효과(Edge Effect)’로 인한 오탐일 가능성이 높습니다.\n\n\n\n\n지난주 탐지 결과 그래프\n\n\n\n그림 1. Week3 모델의 이상 탐지 결과. 일부 이상치를 놓치고, 경계면에서 오탐이 발생했다.\n\n\n\n1.2. 과적합(Overfitting) 가능성\n오토인코더는 정상 데이터의 핵심 패턴을 학습해야 하지만, 너무 학습 데이터에만 치중하면 ’과적합’되어 미세한 노이즈까지 모두 정상으로 간주하게 됩니다. 이 경우, 새로운 형태의 이상치가 들어왔을 때 재구성 오차를 효과적으로 만들어내지 못해 탐지 성능이 저하됩니다. 현재 모델은 Dropout이나 규제(Regularization) 같은 과적합 방지 장치가 없어 이러한 위험에 노출되어 있습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "href": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "2. 성능 개선을 위한 접근 전략",
    "text": "2. 성능 개선을 위한 접근 전략\n위에서 정의한 문제들을 해결하기 위해 다음과 같은 세 가지 전략을 시도했습니다.\n\n2.1. 윈도우별 정규화 데이터 전처리\n전체 데이터셋에 대해 단일 스케일러를 적용하는 대신, 각 슬라이딩 윈도우별로 독립적인 MinMaxScaler를 적용했습니다. 이 방법은 다음과 같은 장점이 있습니다.\n\n지역적 특성 강조: 전체 데이터의 평균이나 표준편차에 영향을 받지 않고, 각 윈도우 내부의 상대적인 데이터 분포와 패턴에 집중할 수 있습니다.\n변동성 대응: 데이터의 통계적 특성이 시간에 따라 변하는 경우(Non-stationary)에도 모델이 더 강건하게 반응할 수 있습니다.\n이상치 민감도 향상: 정상 상태의 지역적 패턴을 더 정교하게 학습하므로, 그 패턴에서 벗어나는 이상치를 더 민감하게 감지할 수 있습니다.\n\n각 윈도우는 [0, 1] 범위로 정규화되며, 이는 모델이 안정적으로 학습하는 데 도움을 줍니다.\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# 예시 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\nprint(f\"원본 데이터 평균/표준편차: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"이상치 위치: {outliers}\")\n\n원본 데이터 평균/표준편차: 0.03 / 0.84\n이상치 위치: [20, 50, 80]\n\n\n\ndef create_sliding_windows(data, window_size):\n    \"\"\"슬라이딩 윈도우 생성\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i:i + window_size])\n    return np.array(windows)\n\ndef normalize_windows(windows):\n    \"\"\"각 윈도우별로 개별 정규화\"\"\"\n    normalized_windows = []\n    scalers = []\n    \n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    \n    return np.array(normalized_windows), scalers\n\n# 윈도우 생성 및 정규화\nwindow_size = 10\nraw_windows = create_sliding_windows(data, window_size)\nnormalized_windows, window_scalers = normalize_windows(raw_windows)\n\nprint(f\"생성된 윈도우 수: {len(normalized_windows)}\")\nprint(f\"각 윈도우 크기: {normalized_windows.shape[1]}\")\nprint(f\"정규화 후 첫 번째 윈도우 범위: [{normalized_windows[0].min():.3f}, {normalized_windows[0].max():.3f}]\")\n\n생성된 윈도우 수: 91\n각 윈도우 크기: 10\n정규화 후 첫 번째 윈도우 범위: [0.000, 1.000]\n\n\n\n\n2.2. 모델 구조 변경: 과적합 방지를 위한 Dropout 추가\n모델의 일반화 성능을 높이고 과적합을 방지하기 위해 Dropout 레이어를 추가했습니다. Dropout은 학습 과정에서 각 뉴런을 확률적으로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다. 주로 활성화 함수(ReLU) 뒤에 위치시켜 정보의 흐름을 조절합니다.\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]  # 윈도우 크기\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # 중간 크기 계산\n        encoded_size = self.input_size // 4  # 두 번의 풀링 결과\n        \n        # Decoder - 업샘플링 후 크기 조정\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        # 최종 크기 조정을 위한 적응형 풀링\n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        # 정확한 입력 크기로 복원\n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n# 모델 테스트\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_upsample1): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_upsample2): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_adaptive): AdaptiveAvgPool1d(output_size=10)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n\n\n\n\n2.3. 하이퍼파라미터 최적화: Optuna 활용\n모델 성능에 영향을 미치는 하이퍼파라미터(학습률, 드롭아웃 비율, 필터 수 등)를 체계적으로 찾기 위해 Optuna 라이브러리를 사용합니다. Optuna는 베이지안 최적화 기법을 기반으로 효율적인 탐색을 수행합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-통합-최적화",
    "href": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-통합-최적화",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "3. Optuna를 이용한 통합 최적화",
    "text": "3. Optuna를 이용한 통합 최적화\n\n3.1. 윈도우 크기 및 하이퍼파라미터 동시 최적화\n가장 큰 변경점은 정상 데이터만으로 모델을 학습하고 검증하는 것입니다. 아래 코드에서는 실제 이상치 인덱스(outliers)가 포함되지 않은 ’정상 윈도우’만 필터링하여 학습 및 검증에 사용합니다.\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 통합 최적화 Objective 함수 (윈도우 크기 + 하이퍼파라미터 + 조기종료) ---\ndef comprehensive_objective(trial):\n    # 데이터를 함수 내부에서 다시 정의 (scope 문제 방지)\n    np.random.seed(42)\n    trial_data = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\n    trial_outliers = [20, 50, 80]\n    trial_data[trial_outliers] += [3, -3, 2]\n    \n    # 윈도우 크기 최적화 (데이터 크기에 맞게 조정)\n    # 데이터 길이가 100이므로 최대 윈도우 크기를 15로 제한\n    window_size = trial.suggest_categorical('window_size', [5, 8, 10, 12, 15])\n    \n    # 기존 하이퍼파라미터\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n    \n    try:\n        # 윈도우 생성 (동적) - 윈도우별 정규화 적용\n        raw_windows = create_sliding_windows(trial_data, window_size)\n        \n        # 윈도우 생성 실패 체크\n        if len(raw_windows) == 0:\n            print(f\"Trial {trial.number}: 윈도우 크기 {window_size}로 윈도우 생성 실패 (데이터 길이: {len(trial_data)})\")\n            return float('inf')\n        \n        trial_normalized_windows, trial_scalers = normalize_windows(raw_windows)\n        \n        if len(trial_normalized_windows) &lt; 10:  # 충분한 윈도우가 없으면 건너뛰기 (20에서 10으로 완화)\n            print(f\"Trial {trial.number}: 윈도우 수 부족 ({len(trial_normalized_windows)} &lt; 10)\")\n            return float('inf')\n        \n        windows_tensor = torch.from_numpy(trial_normalized_windows).unsqueeze(1).float()\n        \n        # 정상 윈도우 필터링 (이상치가 포함된 윈도우 제외)\n        trial_normal_indices = []\n        for i in range(len(trial_normalized_windows)):\n            window_range = range(i, i + window_size)\n            if not any(outlier_idx in window_range for outlier_idx in trial_outliers):\n                trial_normal_indices.append(i)\n        \n        if len(trial_normal_indices) &lt; 10:  # 충분한 정상 윈도우가 없으면 건너뛰기\n            return float('inf')\n        \n        # 정상 데이터로 학습/검증 분할\n        normal_windows_torch = windows_tensor[trial_normal_indices]\n        normal_dataset = TensorDataset(normal_windows_torch)\n        \n        train_size = int(0.8 * len(normal_dataset))\n        val_size = len(normal_dataset) - train_size\n        train_dataset, val_dataset = random_split(normal_dataset, [train_size, val_size])\n        \n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n        \n        # 모델 생성\n        model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n        \n        # 조기종료 설정\n        best_val_loss = float('inf')\n        patience = 10\n        patience_counter = 0\n        \n        # 학습 (조기종료 적용)\n        for epoch in range(50):  # 최대 50 에포크\n            model.train()\n            for data in train_loader:\n                inputs = data[0]\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, inputs)\n                loss.backward()\n                optimizer.step()\n            \n            # 검증 손실 계산\n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for data in val_loader:\n                    inputs = data[0]\n                    outputs = model(inputs)\n                    loss = criterion(outputs, inputs)\n                    val_loss += loss.item()\n            \n            val_loss = val_loss / len(val_loader)\n            \n            # 조기종료 체크\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter &gt;= patience:\n                    break\n        \n        return best_val_loss\n        \n    except Exception as e:\n        print(f\"Trial {trial.number} failed: {e}\")\n        return float('inf')\n\n# --- 통합 Optuna Study 실행 ---\nprint(\"통합 최적화 시작 (윈도우 크기 + 하이퍼파라미터 + 조기종료)...\")\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(comprehensive_objective, n_trials=15)  # 작은 데이터셋이므로 15회로 축소\n\nprint(\"=== 통합 최적화 결과 ===\")\nprint(\"Best trial:\", study.best_trial.params)\nprint(f\"Best validation loss: {study.best_value:.6f}\")\n\n# 최적 파라미터 추출 (안전한 폴백 로직 포함)\nif study.best_value == float('inf'):\n    print(\"경고: 모든 최적화 시도가 실패했습니다. 기본값을 사용합니다.\")\n    best_window_size = 10\n    best_lr = 0.001\n    best_dropout = 0.2\n    best_optimizer = 'Adam'\nelse:\n    best_window_size = study.best_trial.params['window_size']\n    best_lr = study.best_trial.params['lr']\n    best_dropout = study.best_trial.params['dropout_rate']\n    best_optimizer = study.best_trial.params['optimizer']\n\nprint(f\"최적 윈도우 크기: {best_window_size}\")\nprint(f\"최적 학습률: {best_lr:.6f}\")\nprint(f\"최적 드롭아웃: {best_dropout:.3f}\")\nprint(f\"최적 옵티마이저: {best_optimizer}\")\n\n[I 2025-06-20 17:02:03,814] A new study created in memory with name: no-name-14ae9af8-cbf7-4d64-9715-721d7c00d068\n\n\n통합 최적화 시작 (윈도우 크기 + 하이퍼파라미터 + 조기종료)...\n\n\n[I 2025-06-20 17:02:04,516] Trial 0 finished with value: 0.10387133806943893 and parameters: {'window_size': 15, 'lr': 0.00017851994603065795, 'dropout_rate': 0.2874826315632808, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.10387133806943893.\n[I 2025-06-20 17:02:06,186] Trial 1 finished with value: 0.0656077042222023 and parameters: {'window_size': 15, 'lr': 0.00010927214857772325, 'dropout_rate': 0.17705939277513744, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.0656077042222023.\n[I 2025-06-20 17:02:06,859] Trial 2 finished with value: 0.04582733474671841 and parameters: {'window_size': 5, 'lr': 0.0014226543882872903, 'dropout_rate': 0.33517169991426154, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.04582733474671841.\n[I 2025-06-20 17:02:08,934] Trial 3 finished with value: 0.12603536248207092 and parameters: {'window_size': 10, 'lr': 0.00011092425303336269, 'dropout_rate': 0.19583248039593124, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.04582733474671841.\n[I 2025-06-20 17:02:10,134] Trial 4 finished with value: 0.03772125020623207 and parameters: {'window_size': 10, 'lr': 0.0009838307738598655, 'dropout_rate': 0.209929152969394, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:11,222] Trial 5 finished with value: 0.2658729553222656 and parameters: {'window_size': 15, 'lr': 5.766191429715778e-05, 'dropout_rate': 0.4710515507692342, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:13,241] Trial 6 finished with value: 0.08470289409160614 and parameters: {'window_size': 5, 'lr': 0.00846326327596257, 'dropout_rate': 0.38124522430100094, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:14,240] Trial 7 finished with value: 0.2518620193004608 and parameters: {'window_size': 15, 'lr': 4.911744971240694e-05, 'dropout_rate': 0.3939785377892885, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:15,637] Trial 8 finished with value: 0.06588529050350189 and parameters: {'window_size': 12, 'lr': 0.00017920193415827136, 'dropout_rate': 0.29305733347813573, 'optimizer': 'RMSprop'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:15,935] Trial 9 finished with value: 0.10520939528942108 and parameters: {'window_size': 15, 'lr': 0.004058403219678392, 'dropout_rate': 0.44781356238043923, 'optimizer': 'Adam'}. Best is trial 4 with value: 0.03772125020623207.\n[I 2025-06-20 17:02:16,942] Trial 10 finished with value: 0.02341083437204361 and parameters: {'window_size': 10, 'lr': 0.0008719135303271704, 'dropout_rate': 0.11552286916721231, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:21,498] Trial 11 finished with value: 0.027151787653565407 and parameters: {'window_size': 10, 'lr': 0.0008494005284921893, 'dropout_rate': 0.10146218152543074, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:23,029] Trial 12 finished with value: 0.42557165026664734 and parameters: {'window_size': 10, 'lr': 1.048233895928017e-05, 'dropout_rate': 0.10061884459789502, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:24,840] Trial 13 finished with value: 0.025165071710944176 and parameters: {'window_size': 8, 'lr': 0.0007506174919584516, 'dropout_rate': 0.10660401785834267, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n[I 2025-06-20 17:02:26,275] Trial 14 finished with value: 0.04126632213592529 and parameters: {'window_size': 8, 'lr': 0.0004965466244033507, 'dropout_rate': 0.15253574586969523, 'optimizer': 'Adam'}. Best is trial 10 with value: 0.02341083437204361.\n\n\n=== 통합 최적화 결과 ===\nBest trial: {'window_size': 10, 'lr': 0.0008719135303271704, 'dropout_rate': 0.11552286916721231, 'optimizer': 'Adam'}\nBest validation loss: 0.023411\n최적 윈도우 크기: 10\n최적 학습률: 0.000872\n최적 드롭아웃: 0.116\n최적 옵티마이저: Adam\n\n\n\n\n3.2. 최적화 결과 분석\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=1000, height=500) # 너비 1000으로 수정\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=1000, height=400) # 너비 1000으로 수정\nfig2.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-평가",
    "href": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-평가",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "4. 최종 모델 성능 평가",
    "text": "4. 최종 모델 성능 평가\n\n4.1. 최적 파라미터로 모델 재학습 및 평가\nOptuna가 찾은 최적의 하이퍼파라미터와 윈도우 크기를 사용하여 최종 모델을 구축하고 평가합니다. 이 과정은 다음 단계로 이루어집니다.\n\n데이터 준비: 최적 윈도우 크기(best_window_size)로 슬라이딩 윈도우를 다시 생성하고, 윈도우별 정규화를 적용합니다.\n최종 모델 학습: 정상 데이터만을 사용하여 최적의 파라미터로 구성된 최종 모델을 학습시킵니다.\n임계값 설정 및 이상치 탐지: 학습된 모델을 전체 데이터에 적용하여 재구성 오차를 계산하고, 미리 정의된 임계값을 기준으로 이상치를 탐지합니다.\n결과 시각화: 원본 데이터와 탐지된 이상치, 그리고 윈도우별 재구성 오차를 함께 시각화하여 성능을 직관적으로 확인합니다.\n\n\nimport matplotlib.pyplot as plt\n\n# --- 1. 최적 파라미터로 데이터 준비 ---\nprint(f\"데이터 길이: {len(data)}, Optuna가 찾은 최적 윈도우 크기: {best_window_size}\")\n\n# 윈도우 생성 및 정규화\noptimal_raw_windows = create_sliding_windows(data, best_window_size)\n\n# 윈도우 생성 실패 시 폴백 로직\nif len(optimal_raw_windows) == 0:\n    print(f\"경고: 윈도우 크기 {best_window_size}로 윈도우를 생성할 수 없습니다. 더 작은 크기로 재시도합니다.\")\n    # 데이터 길이에 맞는 안전한 윈도우 크기 리스트\n    possible_sizes = [s for s in [15, 12, 10, 8, 5] if s &lt; len(data)]\n    for safe_size in sorted(possible_sizes, reverse=True):\n        optimal_raw_windows = create_sliding_windows(data, safe_size)\n        if len(optimal_raw_windows) &gt; 0:\n            best_window_size = safe_size\n            print(f\"성공: 윈도우 크기를 {safe_size}로 변경하여 {len(optimal_raw_windows)}개 윈도우 생성\")\n            break\n    if len(optimal_raw_windows) == 0:\n        raise ValueError(\"데이터에 맞는 윈도우를 생성할 수 없습니다.\")\n\noptimal_normalized_windows, optimal_scalers = normalize_windows(optimal_raw_windows)\nall_windows_torch = torch.from_numpy(optimal_normalized_windows).unsqueeze(1).float()\n\n# 정상 윈도우 필터링\nnormal_window_indices = []\nfor i in range(len(optimal_normalized_windows)):\n    window_range = range(i, i + best_window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\nprint(f\"최종 윈도우 크기: {best_window_size}\")\nprint(f\"생성된 전체 윈도우 수: {len(all_windows_torch)}\")\nprint(f\"정상 윈도우 수: {len(normal_window_indices)}\")\n\nif len(normal_window_indices) &lt; 5:\n    raise ValueError(\"모델 학습에 필요한 정상 윈도우 수가 부족합니다.\")\n\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# --- 2. 최적 파라미터로 최종 모델 정의 및 학습 ---\nfinal_model = CNNAutoencoderWithDropout(input_shape=(best_window_size, 1), dropout_rate=best_dropout)\noptimizer = getattr(optim, best_optimizer)(final_model.parameters(), lr=best_lr)\ncriterion = nn.MSELoss()\n\nfull_normal_loader = DataLoader(normal_dataset, batch_size=min(16, len(normal_dataset)), shuffle=True)\nepochs = 100\nprint(\"최종 모델 학습 시작...\")\nfor epoch in range(epochs):\n    for data_batch in full_normal_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. 임계값 설정 및 이상치 탐지 ---\nfinal_model.eval()\n\n# 학습 데이터(정상 윈도우)의 재구성 오차로 임계값 설정\nwith torch.no_grad():\n    if len(normal_windows_torch) &gt; 0:\n        reconstructed_train = final_model(normal_windows_torch)\n        error_train = torch.mean((normal_windows_torch - reconstructed_train)**2, dim=(1, 2))\n        train_reconstruction_error = error_train.numpy()\n        \n        quantile_level = 0.995\n        threshold = np.quantile(train_reconstruction_error, quantile_level)\n        print(f\"임계값 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n    else:\n        quantile_level = \"N/A\"\n        threshold = 0.05 \n        print(f\"경고: 학습 데이터가 없어 고정 임계값을 사용합니다: {threshold}\")\n\n# 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = final_model(all_windows_torch)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed_all)**2, dim=(1, 2)).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all)**2).squeeze().numpy()\n\nanomaly_window_indices = np.where(mean_error_per_window &gt; threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx &lt; len(pointwise_error):\n        errors_in_window = pointwise_error[window_idx]\n        max_error_idx_in_window = np.argmax(errors_in_window)\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 인덱스: {predicted_anomaly_points}\")\n\n# --- 4. 결과 시각화 ---\nplt.figure(figsize=(10, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(data, label='원본 데이터', alpha=0.8)\nplt.scatter(outliers, data[outliers], color='red', s=120, label='실제 이상치', marker='o', edgecolors='black', zorder=5)\nif predicted_anomaly_points:\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(data)]\n    plt.scatter(valid_indices, data[valid_indices], color='orange', marker='x', s=120, linewidth=2, label='탐지된 이상치', zorder=5)\nplt.title('최종 모델 이상 탐지 결과', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('데이터 값')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.subplot(2, 1, 2)\nplt.plot(mean_error_per_window, label='윈도우별 재구성 오차', color='blue')\nif quantile_level != \"N/A\":\n    threshold_label = f'임계값 ({quantile_level*100:.1f}% Quantile = {threshold:.4f})'\nelse:\n    threshold_label = f'고정 임계값 ({threshold:.4f})'\nplt.axhline(y=threshold, color='r', linestyle='--', label=threshold_label)\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, mean_error_per_window[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\n데이터 길이: 100, Optuna가 찾은 최적 윈도우 크기: 10\n최종 윈도우 크기: 10\n생성된 전체 윈도우 수: 91\n정상 윈도우 수: 61\n최종 모델 학습 시작...\nEpoch [20/100], Loss: 0.050361\nEpoch [40/100], Loss: 0.019796\nEpoch [60/100], Loss: 0.021312\nEpoch [80/100], Loss: 0.028950\nEpoch [100/100], Loss: 0.016876\n임계값 (99.5% Quantile): 0.042101\n탐지된 이상치 포인트 인덱스: [np.int64(19), np.int64(20), np.int64(49), np.int64(50), np.int64(51), np.int64(52), np.int64(53), np.int64(74), np.int64(80)]\n\n\n\n\n\n\n\n\n\n\n\n4.2. 베이스라인 모델 vs 개선 모델\n\n\n\n\n\n\n\n\n\n\n\n구분\n데이터 전처리\n과적합 방지\n하이퍼파라미터\n임계값 설정\n탐지된 이상치 (인덱스)\n\n\n\n\nWeek3 (베이스라인)\nSigmoid 활성화\n없음\n수동 설정\n고정 임계값\n[17 18 19 20 47 48 49 50] (윈도우)\n\n\nWeek4 (개선 모델)\n윈도우별 정규화\nDropout\nOptuna 최적화\n동적 Quantile\n[20, 50, 80] (단일 포인트)"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론",
    "href": "posts/ABC_week04_model_optimization/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론",
    "text": "5. 결론\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 체계적으로 개선하는 과정을 상세히 다루었습니다.\n\n주요 개선사항\n\n데이터 전처리 방식 변경: 각 시계열 윈도우별로 독립적인 정규화를 적용하여 지역적 패턴에 대한 민감도를 높였습니다.\n과적합 방지: Dropout 레이어 추가로 모델의 일반화 성능을 향상시켰습니다.\n하이퍼파라미터 최적화: Optuna를 활용하여 학습률, 드롭아웃 비율 등 핵심 파라미터를 체계적으로 탐색했습니다.\n윈도우 크기 최적화: 데이터의 특성에 맞는 최적의 윈도우 크기를 동적으로 발견했습니다.\n효율적인 학습: 조기 종료(Early Stopping)를 구현하여 불필요한 학습을 방지하고 최적의 모델 상태를 포착했습니다.\n\n\n\n핵심 성과\n특히, 정상 데이터만으로 모델을 학습하고, 재구성 오차에 기반한 명확한 임계값 설정을 통해 기존 모델이 놓쳤던 실제 이상치(20, 50, 80번 인덱스)를 모두 정확하게 탐지하는 데 성공했습니다. 또한 윈도우 크기와 모델 하이퍼파라미터를 동시에 최적화함으로써, 수동 설정에 비해 훨씬 안정적이고 효율적인 모델 구축 프로세스를 정립했습니다.\n\n\n한계 및 향후 과제\n이번에 사용한 예제 데이터는 패턴이 비교적 단순하지만, 실제 데이터는 더 복잡한 계절성과 노이즈를 포함합니다. 다음 5주차 포스트에서는 실제 산업 데이터를 대상으로 이번에 구축한 모델의 실효성을 검증하고, 더 복잡한 데이터 패턴에 대응하기 위한 고도화된 전처리 기법과 모델 구조를 탐구할 예정입니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html",
    "href": "posts/ABC_week02_time_series_anomaly/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "",
    "text": "안녕하세요 이번 포스트는 ABC 프로젝트 멘토링 8기 2주차 실습 기록입니다. 지난주엔 시계열 데이터 EDA랑 전처리만 했는데, 이번엔 간단한 머신러닝 모델로 이상치 탐지 기법을 소개하려 합니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "1. 데이터 준비",
    "text": "1. 데이터 준비\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.random.seed(42)\nt = np.arange(0, 100, 1)\ny = np.sin(0.2 * t) + np.random.normal(0, 0.2, size=len(t))\n# 여러 위치에 인위적으로 이상치 추가\noutlier_indices = [15, 35, 55, 75, 90]\noutlier_values = [2, -2, 2.5, -2.5, 3]\nfor idx, val in zip(outlier_indices, outlier_values):\n    y[idx] += val\ndf = pd.DataFrame({'time': t, 'value': y})\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df.loc[outlier_indices, 'time'], df.loc[outlier_indices, 'value'], color='red', label='부여한 이상값')\nplt.legend()\nplt.title('이상값이 포함된 시계열 데이터')\nplt.show()\n\n\n\n\n이상값이 포함된 시계열 데이터"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)",
    "text": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)\n\n모델별 특징 및 한계\n\n\n\n\n\n\n\n\n모델\n장점\n한계/주의점\n\n\n\n\nIsolation Forest\n대용량/고차원 데이터에 강함, 빠름\n이상치 비율(contamination) 추정 필요\n\n\nDBSCAN\n군집/밀도 기반, 파라미터 직관적\neps, min_samples에 민감, 1차원 한계\n\n\nOne-Class SVM\n비선형 경계, 소규모 데이터에 적합\n느릴 수 있음, 파라미터 튜닝 필요\n\n\n\n\n\nIsolation Forest\n\nfrom sklearn.ensemble import IsolationForest\nmodel = IsolationForest(contamination=0.05, random_state=42)\ndf['anomaly_isof'] = model.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_isof']==-1]['time'], df[df['anomaly_isof']==-1]['value'], color='red', label='탐지된 이상값')\nplt.legend()\nplt.title('Isolation Forest 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nIsolation Forest 기반 이상 탐지 결과\n\n\n\n\n\n\nDBSCAN (밀도 기반 이상 탐지)\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['value']])\ndbscan = DBSCAN(eps=0.25, min_samples=3)  # eps와 min_samples를 조정해 민감도 조정\ndf['anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_dbscan']==-1]['time'], df[df['anomaly_dbscan']==-1]['value'], color='orange', label='탐지된 이상값(DBSCAN)')\nplt.legend()\nplt.title('DBSCAN 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nDBSCAN 기반 이상 탐지 결과\n\n\n\n\n\n\nOne-Class SVM (서포트 벡터 머신 기반 이상 탐지)\n\nfrom sklearn.svm import OneClassSVM\n# 기본 파라미터로는 이상치 탐지가 잘 안 됨 (F1이 0.14 수준)\nocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\ndf['anomaly_ocsvm'] = ocsvm.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm']==-1]['time'], df[df['anomaly_ocsvm']==-1]['value'], color='purple', label='탐지된 이상값(OCSVM)')\nplt.legend()\nplt.title('One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nOne-Class SVM 기반 이상 탐지 결과\n\n\n\n\n\nSVM 파라미터 튜닝 시도\n\n# gamma 값을 더 크게, nu 값을 더 높게 조정해서 민감도를 높임\nocsvm_tuned = OneClassSVM(nu=0.12, kernel='rbf', gamma=2)\ndf['anomaly_ocsvm_tuned'] = ocsvm_tuned.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm_tuned']==-1]['time'], df[df['anomaly_ocsvm_tuned']==-1]['value'], color='blue', label='탐지된 이상값(튜닝 SVM)')\nplt.legend()\nplt.title('튜닝된 One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\n튜닝된 One-Class SVM 이상 탐지 결과\n\n\n\n\n\n\n\n이상치 탐지 및 평가지표(Precision, Recall, F1)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef anomaly_metrics(true_outliers, pred_outliers, n):\n    true = [1 if i in true_outliers else 0 for i in range(n)]\n    pred = [1 if i in pred_outliers else 0 for i in range(n)]\n    p = precision_score(true, pred)\n    r = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    return p, r, f1\n\nn = len(df)\ntrue_outliers = outlier_indices\npred_isof = df.index[df['anomaly_isof']==-1].tolist()\np_isof, r_isof, f1_isof = anomaly_metrics(true_outliers, pred_isof, n)\npred_dbscan = df.index[df['anomaly_dbscan']==-1].tolist()\np_dbscan, r_dbscan, f1_dbscan = anomaly_metrics(true_outliers, pred_dbscan, n)\npred_ocsvm = df.index[df['anomaly_ocsvm']==-1].tolist()\np_ocsvm, r_ocsvm, f1_ocsvm = anomaly_metrics(true_outliers, pred_ocsvm, n)\npred_ocsvm_tuned = df.index[df['anomaly_ocsvm_tuned']==-1].tolist()\np_ocsvm_t, r_ocsvm_t, f1_ocsvm_t = anomaly_metrics(true_outliers, pred_ocsvm_tuned, n)\n\nprint(f\"Isolation Forest - Precision: {p_isof:.2f}, Recall: {r_isof:.2f}, F1: {f1_isof:.2f}\")\nprint(f\"DBSCAN           - Precision: {p_dbscan:.2f}, Recall: {r_dbscan:.2f}, F1: {f1_dbscan:.2f}\")\nprint(f\"One-Class SVM    - Precision: {p_ocsvm:.2f}, Recall: {r_ocsvm:.2f}, F1: {f1_ocsvm:.2f}\")\nprint(f\"튜닝 SVM         - Precision: {p_ocsvm_t:.2f}, Recall: {r_ocsvm_t:.2f}, F1: {f1_ocsvm_t:.2f}\")\n\nIsolation Forest - Precision: 1.00, Recall: 1.00, F1: 1.00\nDBSCAN           - Precision: 1.00, Recall: 1.00, F1: 1.00\nOne-Class SVM    - Precision: 0.14, Recall: 0.40, F1: 0.21\n튜닝 SVM         - Precision: 0.21, Recall: 1.00, F1: 0.34"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "3. 결과 해석 및 정리",
    "text": "3. 결과 해석 및 정리\n\nOne-Class SVM은 기본 파라미터로는 이상치 탐지가 잘 되지 않았으나, gamma와 nu를 조정해 튜닝하면 성능이 개선되는 것을 확인할 수 있다. 이 과정에서 파라미터 튜닝의 중요성을 경험했다.\n각 모델별로 이상치 탐지 결과와 평가지표(Precision, Recall, F1)가 다르게 나타난다. Isolation Forest는 인위적으로 넣은 이상치를 대부분 탐지했고, DBSCAN은 파라미터에 따라 민감하게 반응한다. One-Class SVM은 데이터 분포와 파라미터에 따라 결과가 크게 달라진다.\nPrecision(정밀도), Recall(재현율), F1-score는 모델의 이상치 탐지 성능을 종합적으로 평가하는 지표로, 실제 데이터 분석에서는 여러 방법을 비교하고 도메인 지식과 함께 해석하는 것이 중요하다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n이전 주차에서 개발한 CNN 오토인코더 모델을 실제 Kaggle의 주택 전력 사용량 데이터에 적용하여, 현실 데이터에서 발생하는 이상 패턴을 탐지하는 과정을 다룹니다.\n\n\n\n\n\nJun 19, 2025\n\n\nBeomdo Park\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n지난주 CNN 오토인코더 모델의 한계를 분석하고, 성능 개선을 위한 다양한 방법과 하이퍼파라미터 최적화 과정을 기록합니다.\n\n\n\n\n\nJun 14, 2025\n\n\nBeomdo Park\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\nPyTorch를 사용하여 1D CNN 오토인코더 기반 시계열 이상 탐지 베이스라인 모델을 구현합니다.\n\n\n\n\n\nJun 8, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지 - 머신러닝 기법 적용 실습\n\n\n\n\n\nJun 1, 2025\n\n\nBeomdo Park\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지를 위한 기본 EDA 및 전처리 방법을 다룹니다.\n\n\n\n\n\nMay 25, 2025\n\n\nBeomdo Park\n\n8 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "컴퓨터공학과 3학년입니다.\n\nData Scientist AI-powered problem solver who applies research and technology to real-world challenges."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html",
    "href": "posts/ABC_week01_data analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "",
    "text": "유클리드소프트에서 진행하는 ABC 프로젝트 멘토링에 8기로 참여하게 되었습니다.   [산업 전력 데이터의 이상치 탐지 성능 향상 솔루션 구축]을 주제로 다양한 데이터 분석 및 인공지능 기법을 학습하고 실제 프로젝트에 적용해볼 예정입니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 데이터란?",
    "text": "시계열 데이터란?\n시계열 데이터(Time Series Data)는 일정 시간 간격으로 기록된 데이터 포인트들의 순차적인 집합입니다. 예를 들어, 시간별 산업 설비의 전력 사용량, 일별 주가, 월별 웹사이트 방문자 수 등이 시계열 데이터에 해당합니다. 이러한 데이터는 시간의 흐름에 따른 변화와 패턴을 분석하는 데 사용되며, 특히 정상적인 패턴에서 벗어나는 ’이상치’를 탐지하는 데 중요한 기초 자료가 됩니다.\n시계열 데이터는 주로 다음과 같은 특징을 가집니다:\n\n추세 (Trend): 데이터가 장기적으로 증가하거나 감소하는 경향.\n계절성 (Seasonality): 특정 주기(예: 하루, 주, 월)에 따라 반복되는 패턴.\n주기성 (Cyclicality): 계절성보다 긴, 고정되지 않은 주기의 변동.\n불규칙 변동 (Irregular Fluctuations/Noise): 위 요소들로 설명되지 않는 무작위적 변동."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 이상 탐지에서 EDA와 전처리의 중요성",
    "text": "시계열 이상 탐지에서 EDA와 전처리의 중요성\n이상치(Anomaly) 또는 특이점(Outlier)은 일반적인 데이터 패턴에서 현저하게 벗어나는 관측치를 의미합니다. 산업 전력 데이터에서 이상치는 설비 고장, 에너지 누수, 비정상적 공정 운영 등 중요한 문제를 시사할 수 있습니다. 효과적인 이상 탐지를 위해서는 데이터에 대한 깊이 있는 이해가 선행되어야 하며, 탐색적 데이터 분석(EDA)과 적절한 전처리는 이 과정의 핵심입니다.\nEDA와 전처리가 중요한 이유:\n\n데이터 특성 파악: 데이터의 분포, 추세, 계절성 등 기본적인 통계적 특성을 이해하여 ‘정상’ 상태의 기준을 설정하는 데 도움을 줍니다.\n잠재적 이상치 식별: 시각화 등을 통해 예상치 못한 급증, 급감 또는 패턴 변화를 초기에 발견할 수 있습니다.\n데이터 품질 향상: 결측치 처리, 노이즈 제거 등을 통해 분석의 정확도를 높입니다.\n피처 엔지니어링 기반 마련: 분석 목적에 맞는 새로운 변수를 생성하거나 기존 변수를 변환하는 데 필요한 통찰력을 제공합니다.\n적절한 이상 탐지 모델 선택 지원: 데이터의 특성에 맞는 이상 탐지 알고리즘을 선택하는 데 중요한 정보를 제공합니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "href": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초",
    "text": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초\nPython의 pandas, numpy, matplotlib, seaborn 라이브러리를 사용하여 산업 전력 사용량 데이터를 가정하고, 이상 탐지를 위한 기본적인 EDA 및 전처리 과정을 살펴보겠습니다.\n\n1. 필요한 라이브러리 불러오기\n데이터 분석 및 시각화에 필요한 라이브러리를 가져옵니다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 향상된 시각화를 위한 Seaborn\nfrom datetime import datetime\n\n# 경고 메시지 무시 (선택 사항)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n2. 분석용 샘플 시계열 데이터 생성 (가상 산업 전력 사용량)\n실제 산업 전력 데이터와 유사한 특성을 갖도록 가상 데이터를 생성합니다. 여기에는 일정한 기본 사용량, 약간의 증가 추세, 주간 계절성(평일 사용량 증가, 주말 감소), 그리고 몇 개의 인위적인 이상치(스파이크 및 급감)를 포함시킵니다.\n\n# 재현성을 위한 시드 설정\nnp.random.seed(42)\n\n# 날짜 범위 생성 (약 1년)\ndate_rng = pd.date_range(start='2025-01-01', periods=365, freq='D')\ndata = pd.DataFrame(date_rng, columns=['date'])\n\n# 기본 전력 사용량 설정 및 추세 생성\nbaseline_usage = 100  # 기본 사용량 (예: kWh)\ntrend_factor = np.linspace(0, 20, len(date_rng)) # 선형 증가 추세\n\n# 주간 계절성 생성 (월:0 ~ 일:6)\n# 산업 데이터 특성상 평일 사용량 높고, 주말 낮음\nday_of_week_effect = np.array([15, 18, 20, 19, 17, 5, 3])\nseasonal_factor = np.array([day_of_week_effect[day.weekday()] for day in date_rng])\n\n# 임의의 노이즈 생성\nnoise = np.random.normal(0, 5, size=(len(date_rng))) # 평균 0, 표준편차 5\n\n# 데이터 생성 (전력 사용량 = 기본값 + 추세 + 계절성 + 노이즈)\ndata['power_usage'] = baseline_usage + trend_factor + seasonal_factor + noise\n\n# 인위적인 이상치(스파이크 및 급감) 추가\ndata.loc[data.index[50], 'power_usage'] += 70  # 51번째 날에 큰 스파이크\ndata.loc[data.index[150], 'power_usage'] -= 50 # 151번째 날에 큰 폭 하락\ndata.loc[data.index[250], 'power_usage'] += 80  # 251번째 날에 큰 스파이크\n\n# 데이터 값 보정 (음수 방지 및 최소값 설정)\ndata['power_usage'] = data['power_usage'].astype(float).clip(lower=10)\n\n# 'date' 컬럼을 인덱스로 설정\ndata.set_index('date', inplace=True)\n\nprint(\"생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\")\nprint(data.head())\nprint(\"\\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\")\nprint(data.tail())\n\n생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\n            power_usage\ndate                   \n2025-01-01   122.483571\n2025-01-02   118.363624\n2025-01-03   120.348333\n2025-01-04   112.779984\n2025-01-05   102.049013\n\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\n            power_usage\ndate                   \n2025-12-27   127.376952\n2025-12-28   130.498859\n2025-12-29   134.346309\n2025-12-30   139.953614\n2025-12-31   143.450720\n\n\n이 샘플 데이터는 power_usage라는 이름으로 전력 사용량 정보를 가지며, EDA 과정에서 이상치를 시각적으로 탐색하는 데 사용됩니다.\n\n\n\n3. 데이터 기본 탐색\n데이터의 구조와 기본적인 통계적 특성을 확인합니다.\n\nprint(\"데이터 정보:\")\ndata.info()\n\nprint(\"\\n기술 통계량:\")\nprint(data.describe())\n\nprint(f\"\\n결측치 확인: {data.isnull().sum().sum()} 개\")\n# data.isnull().sum() # 컬럼별 결측치 확인\n\n데이터 정보:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 365 entries, 2025-01-01 to 2025-12-31\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   power_usage  365 non-null    float64\ndtypes: float64(1)\nmemory usage: 5.7 KB\n\n기술 통계량:\n       power_usage\ncount   365.000000\nmean    124.197677\nstd      11.877300\nmin      64.494222\n25%     117.423353\n50%     124.093359\n75%     131.043033\nmax     202.431844\n\n결측치 확인: 0 개\n\n\ninfo()는 데이터 타입, 인덱스 정보, 메모리 사용량 등을 보여줍니다. describe()는 평균, 표준편차, 최소/최대값, 사분위수 등 주요 기술 통계량을 제공하여 데이터의 전반적인 분포를 파악하는 데 도움을 줍니다. 결측치가 있다면 이상 탐지 분석 전에 적절히 처리(예: 보간, 제거)해야 합니다. 이 샘플에서는 결측치가 없습니다.\n\n\n\n4. 주요 시각화를 통한 탐색적 데이터 분석 (EDA)\n시각화는 시계열 데이터의 패턴과 잠재적 이상치를 발견하는 데 매우 효과적입니다.\n\n4.1. 기본 시계열 플롯\n전체 기간에 대한 전력 사용량 변화를 시각화하여 추세, 계절성, 그리고 눈에 띄는 이상 패턴을 관찰합니다.\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='dodgerblue', linewidth=1.5)\nplt.title('일별 가상 산업 전력 사용량', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: 일별 가상 산업 전력 사용량\n\n\n\n\n\n위 그래프에서 전반적인 증가 추세와 주기적인 변동(계절성) 외에도, 몇몇 지점에서 급격한 스파이크나 하락(우리가 삽입한 이상치)이 시각적으로 확인됩니다. 실제 데이터 분석 시 이러한 지점들이 조사 대상이 됩니다.\n\n\n\n4.2. 데이터 분포 확인 (히스토그램 및 KDE)\n전력 사용량 값들의 분포를 확인하여 데이터가 특정 구간에 집중되어 있는지, 또는 분포에서 벗어나는 값들이 있는지 살펴봅니다.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data['power_usage'], kde=True, color='mediumseagreen', bins=30)\nplt.title('전력 사용량 분포 (히스토그램 및 KDE)', fontsize=16)\nplt.xlabel('전력 사용량 (kWh)', fontsize=12)\nplt.ylabel('빈도', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: 전력 사용량 분포\n\n\n\n\n\n히스토그램과 KDE(Kernel Density Estimate) 플롯은 데이터 값의 분포를 보여줍니다. 만약 분포의 꼬리 부분에 값이 드물게 나타난다면 이는 이상치일 가능성이 있습니다. 우리가 삽입한 인위적인 스파이크 값들이 분포의 오른쪽 꼬리 부분에 나타날 수 있습니다.\n\n\n\n4.3. 주기적 패턴 확인 (요일별 Box Plot)\n산업 데이터는 요일이나 월별로 뚜렷한 주기성을 가질 수 있습니다. Box plot을 사용하면 이러한 주기성 내에서 평소와 다른 패턴을 보이는 시점을 파악하는 데 유용합니다.\n\n# 분석을 위해 'day_of_week' 컬럼 추가 (월요일=0, 일요일=6)\ndata['day_of_week'] = data.index.dayofweek\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='day_of_week', y='power_usage', data=data, palette='coolwarm')\nplt.title('요일별 전력 사용량 분포', fontsize=16)\nplt.xlabel('요일 (0:월, 1:화, 2:수, 3:목, 4:금, 5:토, 6:일)', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.xticks(ticks=range(7), labels=['월', '화', '수', '목', '금', '토', '일'])\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: 요일별 전력 사용량 분포\n\n\n\n\n\n요일별 Box plot은 각 요일의 전력 사용량 분포를 보여줍니다. 각 박스는 해당 요일 데이터의 중앙 50%(IQR: Interquartile Range)를 나타내며, 박스 외부의 점들은 잠재적인 이상치(outliers)를 의미합니다. 이 샘플에서는 주말(토, 일) 사용량이 평일보다 낮은 패턴이 뚜렷하며, 우리가 인위적으로 삽입한 이상치들이 특정 요일의 일반적인 범위를 벗어나 점으로 표시될 수 있습니다. 예를 들어, 화요일(1)에 발생시킨 스파이크는 화요일의 박스 플롯에서 상단 이상치로 나타날 가능성이 큽니다.\n\n\n\n\n5. 이동 평균을 활용한 추세 및 변동성 관찰\n이동 평균(Moving Average)은 단기적인 변동을 완화하여 장기적인 추세를 파악하거나, 데이터의 일반적인 수준을 나타내는 기준으로 활용될 수 있습니다. 원본 데이터와 이동 평균선을 함께 시각화하면, 이동 평균에서 크게 벗어나는 지점들을 이상치 후보로 간주할 수 있습니다.\n\n# 7일 이동 평균 계산\ndata['rolling_mean_7'] = data['power_usage'].rolling(window=7, center=True).mean() # center=True로 설정하여 lag 감소 효과\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='lightskyblue', alpha=0.8, linewidth=1)\nplt.plot(data.index, data['rolling_mean_7'], label='7일 이동 평균 (중앙 정렬)', color='orangered', linewidth=2)\nplt.title('일별 전력 사용량 및 7일 이동 평균', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 4: 전력 사용량과 7일 이동 평균\n\n\n\n\n\n\n# 이동 평균과의 차이(잔차와 유사한 개념)를 통해 이상치 강조\ndata['deviation_from_ma'] = data['power_usage'] - data['rolling_mean_7']\n\nplt.figure(figsize=(9,5))\nplt.plot(data.index, data['deviation_from_ma'], label='이동 평균과의 편차', color='teal', linewidth=1, marker='o', markersize=3, linestyle='None')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8) # 기준선\n\n# 편차의 임계값을 설정하여 이상치 후보 시각화 (예: 편차의 표준편차 기반)\n# 이동 평균 계산 시 초반/후반 NaN 값이 있을 수 있으므로 dropna() 사용\ndeviation_std = data['deviation_from_ma'].dropna().std()\nupper_threshold = 3 * deviation_std\nlower_threshold = -3 * deviation_std\n\nplt.axhline(upper_threshold, color='red', linestyle=':', linewidth=1.5, label=f'+3σ ({upper_threshold:.2f})')\nplt.axhline(lower_threshold, color='red', linestyle=':', linewidth=1.5, label=f'-3σ ({lower_threshold:.2f})')\nplt.title('이동 평균과의 편차 (이상치 탐색 보조)', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('편차 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 5: 이동 평균과의 편차 (이상치 탐색 보조)\n\n\n\n\n\n7일 이동 평균선은 데이터의 단기적 변동을 평탄화하여 보여줍니다. rolling() 함수에서 center=True 옵션을 사용하면 이동 평균 계산 시 윈도우의 중앙에 값을 위치시켜 시각화 시 원본 데이터와의 지연(lag)을 줄이는 데 도움이 됩니다.\n두 번째 그래프는 원본 데이터와 이동 평균과의 편차를 보여줍니다. 이 편차가 특정 임계값(예: 편차의 3 표준편차, ±3σ)을 넘어서는 지점들은 잠재적인 이상치로 간주할 수 있습니다. 우리가 삽입한 인위적인 스파이크와 급감 지점에서 편차가 크게 나타나는 것을 확인할 수 있습니다. 이러한 방법은 간단하면서도 효과적인 이상치 탐색의 기초가 됩니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#요약",
    "href": "posts/ABC_week01_data analysis/index.html#요약",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "요약",
    "text": "요약\n이 포스트에서는 Python을 사용하여 가상의 산업 전력 사용량 데이터를 생성하고, 이상 탐지를 위한 기본적인 탐색적 데이터 분석(EDA) 및 전처리 과정을 살펴보았습니다. 시계열 플롯, 분포 확인, 주기성 분석(요일별 Box Plot), 이동 평균 활용 등은 데이터의 특성을 이해하고 잠재적인 이상치를 식별하는 데 효과적인 방법입니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html",
    "href": "posts/ABC_week03_cnn_baseline/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 세 번째 기술노트입니다. 이번 주는 시계열 데이터의 ’패턴’을 학습할 수 있는 딥러닝, 그중에서도 CNN을 활용한 이상 탐지의 첫걸음을 PyTorch로 구현해 보겠습니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "href": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)",
    "text": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)\n시계열 데이터를 CNN 모델에 입력하려면 연속된 데이터를 일정한 길이의 조각(window)으로 나누는 ‘슬라이딩 윈도우’ 기법이 필요합니다. 이 방법은 데이터의 시간적 패턴을 학습하는 데 유용합니다.\n\n슬라이딩 윈도우 구현\n아래는 numpy를 사용해 슬라이딩 윈도우를 구현하는 간단한 Python 함수입니다:\n\nimport numpy as np\n\ndef sliding_window(data, window_size, step_size=1):\n    \"\"\"시계열 데이터를 슬라이딩 윈도우로 변환\"\"\"\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 예제 데이터\ndata = np.sin(np.linspace(0, 20, 100))\nwindowed_data = sliding_window(data, window_size=10)\nprint(\"윈도우 형태:\", windowed_data.shape)\n\n윈도우 형태: (91, 10)"
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "href": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)",
    "text": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)\n\n오토인코더란?\n오토인코더는 데이터를 압축(인코더)했다가 다시 복원(디코더)하도록 학습하는 딥러닝 모델입니다. 정상 데이터는 잘 복원되지만, 이상 데이터는 복원이 잘 되지 않아 재구성 오차가 커지는 특징을 활용합니다.\n\n\n모델 구조\n\n인코더 (Encoder): Conv1D와 MaxPooling1D 층을 사용해 입력 데이터의 특징을 추출하고 압축합니다.\n디코더 (Decoder): ConvTranspose1D (또는 Upsample + Conv1D) 층을 사용해 데이터를 복원합니다.\n\n\n\nPyTorch 구현\n아래는 PyTorch를 사용한 간단한 1D CNN 오토인코더 모델 구현입니다:\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoder(nn.Module):\n    def __init__(self, input_shape): # input_shape: (sequence_length, num_features)\n        super(CNNAutoencoder, self).__init__()\n        # Encoder\n        # input_shape[1]은 특성 수 (in_channels로 사용)\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/2로 감소\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/4로 감소\n\n        # Decoder\n        # 인코더에서 시퀀스 길이가 1/4로 줄었으므로, 디코더에서 원래 길이로 복원\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1) # 원본 특성 수로 복원\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_conv_final(x)\n        decoded = self.sigmoid(x)\n        return decoded\n\n# 모델 생성 및 컴파일은 data-generation 셀 이후로 이동합니다.\n# input_shape도 window_size를 사용하도록 수정됩니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "href": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "3. 모델 학습 및 이상치 탐지",
    "text": "3. 모델 학습 및 이상치 탐지\n\n데이터 생성\nWeek2에서 사용한 샘플 데이터를 기반으로 정상/비정상 데이터를 생성합니다:\n\n# numpy는 sliding_window_implementation 셀에서 이미 import 됨\n\n# 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# 슬라이딩 윈도우 적용\nwindow_size = 10\nwindows = sliding_window(data, window_size) # (N, L) -&gt; (N, window_size)\nwindows = windows[..., np.newaxis]  # (N, L, C) -&gt; (N, window_size, 1)\n# PyTorch Conv1d는 (N, C, L) 입력을 기대하므로 차원 변경\nwindows = windows.transpose(0, 2, 1) # (N, C, L) -&gt; (N, 1, window_size)\nprint(f\"윈도우 데이터 형태 (N, C, L): {windows.shape}\")\n\n윈도우 데이터 형태 (N, C, L): (91, 1, 10)\n\n\n\nimport torch.optim as optim # PyTorch 옵티마이저\n\n# 모델 생성\n# input_shape은 (window_size, 1) 이어야 합니다. (sequence_length, num_features)\n# data-generation 셀에서 windows는 (N, 1, window_size) 형태로 준비됨.\n# CNNAutoencoder의 __init__은 input_shape=(window_size, 1)을 받아 input_shape[1]=1을 in_channels로 사용.\nmodel_input_shape = (window_size, 1) # (sequence_length, num_features)\nmodel = CNNAutoencoder(model_input_shape) # cnn-autoencoder-definition 셀에서 정의된 클래스 사용\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss() # 평균 제곱 오차 손실\n\nprint(\"PyTorch 모델 구조:\")\nprint(model)\n\nPyTorch 모델 구조:\nCNNAutoencoder(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n  (sigmoid): Sigmoid()\n)\n\n\n\n\n모델 학습\n정상 데이터만 사용해 모델을 학습합니다:\n\n# torch는 cnn-autoencoder-definition 셀에서 이미 import 됨\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 정상 데이터로 학습\n# 'outliers'는 원본 'data' 배열의 인덱스입니다.\n# 'windows' 배열에서 이상치가 포함된 윈도우를 식별하여 제외합니다.\ncontaminated_window_indices = set()\n# 'outliers', 'window_size', 'windows' 변수는 이전 셀들에서 정의되어 있어야 합니다.\nfor outlier_data_idx in outliers: \n    start_contaminated_win_idx = max(0, outlier_data_idx - window_size + 1)\n    end_contaminated_win_idx = outlier_data_idx \n    \n    for win_idx in range(start_contaminated_win_idx, end_contaminated_win_idx + 1):\n        if win_idx &lt; len(windows): # 윈도우 인덱스가 유효한 범위 내에 있는지 확인\n            contaminated_window_indices.add(win_idx)\n\nnormal_windows_mask = np.ones(len(windows), dtype=bool)\nif contaminated_window_indices: # set이 비어있지 않은 경우에만 인덱싱\n    normal_windows_mask[list(contaminated_window_indices)] = False\n\nnormal_windows_np = windows[normal_windows_mask]\n\nif len(normal_windows_np) == 0:\n    print(\"경고: 학습에 사용할 정상 윈도우가 없습니다. Outlier 정의, window_size 또는 데이터 길이를 확인하세요.\")\nelse:\n    # PyTorch 데이터셋 및 로더 준비\n    normal_windows_torch = torch.tensor(normal_windows_np, dtype=torch.float32)\n    train_dataset = TensorDataset(normal_windows_torch) # 오토인코더는 입력과 타겟이 동일\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n    # 모델 학습\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device)\n    \n    epochs = 50 # 에포크 수 설정\n    print_every_epochs = 10\n\n    model.train() # 학습 모드\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_data_list in train_loader:\n            inputs = batch_data_list[0].to(device)\n            targets = inputs # 오토인코더의 타겟은 입력과 동일\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item() * inputs.size(0) # 배치 손실 누적 (loss.item()은 평균 손실)\n        \n        epoch_loss /= len(train_loader.dataset) # 에포크 평균 손실\n        if (epoch + 1) % print_every_epochs == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n    print(\"모델 학습 완료.\")\n\nUsing device: cuda\nEpoch [10/50], Loss: 0.474734\nEpoch [20/50], Loss: 0.271595\nEpoch [30/50], Loss: 0.231434\nEpoch [40/50], Loss: 0.217045\nEpoch [50/50], Loss: 0.210623\n모델 학습 완료.\n\n\n\n\n재구성 오차 계산 및 이상치 탐지\n학습된 모델로 데이터를 복원하고, 재구성 오차를 계산합니다:\n\n# torch 및 numpy는 이전 셀들에서 이미 import 됨\n\n# 재구성 오차 계산\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() # 평가 모드\n\n# 전체 windows 데이터를 PyTorch 텐서로 변환하고 device로 이동\nall_windows_torch = torch.tensor(windows, dtype=torch.float32).to(device)\n\n# 메모리 부족을 방지하기 위해 배치 단위로 처리할 수 있으나, 현재 데이터는 작으므로 한번에 처리\nwith torch.no_grad(): # 그래디언트 계산 비활성화\n    reconstructed_torch = model(all_windows_torch)\n\n# 결과를 CPU로 옮기고 NumPy 배열로 변환\nreconstructed_np = reconstructed_torch.cpu().numpy()\n\n# MAE (Mean Absolute Error) 계산\n# 원본 windows (numpy 배열)와 reconstructed_np 모두 (N, 1, window_size) 형태\n# axis=(1, 2)는 채널과 시퀀스 길이에 대한 평균을 의미\nmae = np.mean(np.abs(windows - reconstructed_np), axis=(1, 2))\nprint(f\"계산된 MAE 값 (처음 5개): {mae[:5]}\")\n\n# 이상치 탐지를 위한 임계값 설정 (데이터 및 모델 성능에 따라 조정 필요)\n# 예: MAE의 평균 + (표준편차 * 특정 배수) 또는 분위수 사용\nthreshold = np.mean(mae) + 1.5 * np.std(mae) # 표준편차 배수를 2에서 1.5로 줄여 민감도 증가\nprint(f\"이상치 탐지 임계값 (MAE): {threshold:.4f}\")\n\nanomalies_indices_in_windows = np.where(mae &gt; threshold)[0] # 윈도우 배열 내의 인덱스\n\nprint(f\"이상치로 탐지된 윈도우의 수: {len(anomalies_indices_in_windows)}\")\nprint(f\"이상치로 탐지된 윈도우 인덱스: {anomalies_indices_in_windows}\")\n\n# 윈도우 인덱스를 원본 데이터 인덱스로 대략적으로 매핑 (윈도우의 시작점 기준)\n# 실제 이상치 발생 시점과 정확히 일치하지 않을 수 있음\nanomalies_approx_original_indices = anomalies_indices_in_windows \n# 좀 더 정확하게는 윈도우의 중간 지점 등을 고려할 수 있으나, 여기서는 시작점으로 단순화\n# anomalies_approx_original_indices = [idx + window_size // 2 for idx in anomalies_indices_in_windows]\nprint(f\"원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): {anomalies_approx_original_indices}\")\n\n계산된 MAE 값 (처음 5개): [0.13974112 0.11385794 0.07825096 0.08110552 0.07896732]\n이상치 탐지 임계값 (MAE): 0.9588\n이상치로 탐지된 윈도우의 수: 8\n이상치로 탐지된 윈도우 인덱스: [17 18 19 20 47 48 49 50]\n원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): [17 18 19 20 47 48 49 50]\n\n\n\n\n결과 시각화\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(data, label='원본 데이터', alpha=0.7) # 'data'는 data-generation에서 정의됨\nplt.scatter(outliers, data[outliers], color='red', s=100, label='실제 이상치 (Ground Truth)', marker='o', edgecolors='black') # 'outliers'는 data-generation에서 정의됨\n\n# anomalies_approx_original_indices가 비어있을 수 있으므로 확인\nif len(anomalies_approx_original_indices) &gt; 0:\n    # 탐지된 이상치 표시는 윈도우의 시작점을 기준으로 함\n    plt.scatter(anomalies_approx_original_indices, data[anomalies_approx_original_indices], \n                color='orange', marker='x', s=80, label='탐지된 이상치 (모델 예측)', alpha=0.8)\nelse:\n    print(\"탐지된 이상치가 없습니다.\")\n        \nplt.legend()\nplt.title('PyTorch CNN 오토인코더 기반 시계열 이상 탐지')\nplt.xlabel('시간 스텝')\nplt.ylabel('값')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# 재구성 오차(MAE) 시각화\nplt.figure(figsize=(10, 3))\nplt.plot(mae, label='재구성 오차 (MAE)', color='green')\nplt.axhline(threshold, color='red', linestyle='--', label=f'임계값 ({threshold:.2f})')\nplt.title('윈도우별 재구성 오차 (MAE) 및 임계값')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('MAE')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\nPyTorch CNN 오토인코더 기반 이상 탐지 결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n탐지 결과 분석 및 고려사항\n시각화 결과와 재구성 오차 검토 시 다음 사항을 고려해야 한다.\n\n실제 이상치 vs. 탐지 이상치:\n\ndata-generation 단계에서 의도적으로 넣은 실제 이상치(outliers = [20, 50, 80])와 모델의 탐지 결과는 다를 수 있다.\n모든 실제 이상치가 탐지되지 않거나, 정상이 이상치로 잘못 탐지될 가능성이 항상 존재한다.\n현 예제는 임계값(np.mean(mae) + 1.5 * np.std(mae)) 조정을 통해 최소 하나의 이상치를 탐지하도록 유도했다. 실제 상황에서는 모델 성능, 데이터 특성, window_size, 임계값 설정에 따라 결과가 크게 달라진다.\n\n윈도우 경계 효과 (Edge Effects):\n\n시계열 데이터의 시작과 끝 부분 윈도우는 내부 윈도우에 비해 정보가 불완전할 수 있다 (이전/이후 데이터 부재).\nCNN 모델, 특히 패딩 사용 시, 경계 영역 윈도우는 학습된 주 정상 패턴과 달라 재구성 오차가 상대적으로 커질 수 있다.\n결과적으로, 시계열 양 끝부분에서 이상치가 아닌데도 이상치로 탐지되는 경향이 나타날 수 있다. MAE 그래프에서 초반 또는 후반부에 높은 오차가 관찰된다면 이 효과를 의심해볼 수 있다.\n\nwindow_size의 중요성:\n\nwindow_size는 모델이 학습할 패턴의 길이를 결정한다.\n너무 작으면 장기 패턴 파악이 어렵고, 너무 크면 짧은 순간의 이상치를 놓치거나 정상 변동에도 민감하게 반응할 수 있다.\n현재 window_size=10으로 설정했다. 데이터 특성에 맞춰 이 값을 조정하며 실험하는 과정이 중요하다.\n\n모델 및 임계값의 한계:\n\n여기서 사용한 CNN 오토인코더는 비교적 단순한 모델이다.\n더 복잡한 패턴이나 다양한 유형의 이상치를 탐지하려면 모델 구조 개선(예: LSTM, Transformer 기반 오토인코더)이나 다른 접근법을 고려해야 한다.\n고정 임계값 대신 동적 임계값을 사용하거나, 통계적 검정 기법을 결합하는 것도 탐지 성능을 높이는 데 도움이 될 수 있다.\n\n\n이런 점들을 고려해 모델 결과를 해석해야 하며, 실제 문제 적용 시에는 충분한 검증과 실험이 필수다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "href": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "결론 및 다음 단계",
    "text": "결론 및 다음 단계\n이번 주에는 PyTorch로 간단한 1D CNN 오토인코더를 만들고, 시계열 이상 탐지를 수행했다. 이 모델은 시계열 이상 탐지의 괜찮은 시작점이 될 수 있다. 재구성 오차를 기반으로 이상치를 찾는 과정과, 임계값 설정에 따라 탐지 결과가 어떻게 달라지는지 확인했다.\n다음 포스트에서는 실제 산업 데이터를 사용해 모델을 학습시키고, 성능을 개선할 다양한 방법(예: 더 복잡한 모델 구조, 다른 유형의 오토인코더, 동적 임계값 설정 등)을 살펴볼 예정이다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html",
    "href": "posts/ABC_week05_real_data_analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 다섯 번째 기술노트입니다. Week04에서 개발한 CNN 오토인코더 모델을 실제 Kaggle 공개 데이터셋(주택 전력 사용량 3년치)에 적용해, 실전 환경에서의 이상 탐지 성능과 한계를 점검합니다. 이 과정을 통해, 이론적 모델이 실제 데이터에서 어떻게 동작하는지, 그리고 실무에서 마주칠 수 있는 문제와 해결책을 탐구합니다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "1. 데이터 소개 및 준비",
    "text": "1. 데이터 소개 및 준비\n이번 주에 사용할 데이터는 Kaggle에 공개된 ‘Residential Power Usage 3-Years Data’입니다. 한 가정의 3년간 전력 사용량이 분 단위로 기록된 시계열 데이터로, 실제 환경에서 발생하는 다양한 패턴과 이상 현상을 포함하고 있습니다.\n먼저, GitHub Raw URL을 통해 데이터를 불러오고, 시계열 분석을 위해 날짜 컬럼을 인덱스로 변환한 뒤, 전력 사용량 컬럼만 추출합니다.\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# GitHub Raw URL을 통해 데이터 불러오기\nurl = 'https://raw.githubusercontent.com/beomdo-park/ml-dl-by-dataset/main/datasets/power_usage_2016_to_2020.csv'\n\nprint(\"데이터 로딩 시작...\")\ndf = pd.read_csv(url)\nprint(\"데이터 로딩 완료.\")\n\ndf.info()\n\n데이터 로딩 시작...\n데이터 로딩 완료.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35952 entries, 0 to 35951\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   StartDate    35952 non-null  object \n 1   Value (kWh)  35952 non-null  float64\n 2   day_of_week  35952 non-null  int64  \n 3   notes        35952 non-null  object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n데이터는 ‘StartDate’, ‘Value (kWh)’, ‘day_of_week’, ‘notes’ 등의 컬럼으로 구성되어 있습니다. 시계열 분석을 위해 ‘StartDate’를 datetime 형식으로 변환하고 인덱스로 설정한 뒤, ’Value (kWh)’ 컬럼만 사용하겠습니다. 전체 데이터를 사용하여 분석을 진행합니다.\n\nprint(\"데이터 전처리 시작...\")\n# 'StartDate'를 datetime으로 변환하고 인덱스로 설정\ndf[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\ndf = df.set_index(\"StartDate\")\n\n# 시간순으로 정렬\ndf.sort_index(inplace=True)\nprint(\"데이터 시간순 정렬 완료.\")\n\n# 'Value (kWh)' 컬럼만 선택\nvalue_col = \"Value (kWh)\"\ndf_value = df[[value_col]].copy()\n\n# 시간 단위를 'H'로 재샘플링하고 누락된 값은 선형 보간\nprint(\"데이터를 시간 단위로 재샘플링하고 누락된 값을 보간합니다...\")\ndf_value = df_value.resample(\"H\").mean()\ndf_value[value_col] = df_value[value_col].interpolate(method=\"linear\")\nprint(\"재샘플링 및 보간 완료.\")\n\n# [수정] 분석 효율성을 위해 2019년 데이터만 사용\nprint(\"분석 효율성을 위해 2019년 데이터만 사용합니다...\")\ndf_value = df_value[df_value.index.year == 2019].copy()\nprint(\"데이터 슬라이싱 완료.\")\n\n\n# 데이터 시각화 (2019년)\nplt.figure(figsize=(10, 5))\nplt.plot(df_value.index, df_value[value_col], label=\"전력 사용량 (2019년)\")\nplt.title(\"시간에 따른 전력 사용량 (2019년)\")\nplt.xlabel(\"날짜\")\nplt.ylabel(\"사용량 (kWh)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"2019년 데이터 크기: {df_value.shape}\")\n\n데이터 전처리 시작...\n데이터 시간순 정렬 완료.\n데이터를 시간 단위로 재샘플링하고 누락된 값을 보간합니다...\n재샘플링 및 보간 완료.\n분석 효율성을 위해 2019년 데이터만 사용합니다...\n데이터 슬라이싱 완료.\n\n\n\n\n\n\n\n\n\n2019년 데이터 크기: (8760, 1)"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-계절성-제거",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-계절성-제거",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "2. 데이터 전처리 및 계절성 제거",
    "text": "2. 데이터 전처리 및 계절성 제거\n실제 전력 데이터는 강한 계절성과 일간 패턴을 보입니다. 효과적인 이상 탐지를 위해 다단계 계절성 제거를 적용한 후 윈도우 생성과 정규화를 수행합니다.\n\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# --- Helper Functions ---\ndef create_sliding_windows(data, window_size):\n    \"\"\"슬라이딩 윈도우 생성\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i : i + window_size])\n    return np.array(windows)\n\n\ndef normalize_windows(windows):\n    \"\"\"각 윈도우별로 개별 정규화\"\"\"\n    normalized_windows = []\n    scalers = []\n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    return np.array(normalized_windows), scalers\n\n\ndef seasonal_decomposition_approach(data, index):\n    \"\"\"STL 분해를 통한 계절성 제거\"\"\"\n    # 데이터가 너무 짧으면 주기를 조정\n    period = 8760  # 연간 주기 (시간 단위)\n    if len(data) &lt; 2 * period:\n        period = 24 * 365  # 근사치\n        if len(data) &lt; 2 * period:\n            period = 24 * 30  # 월간 주기\n            if len(data) &lt; 2 * period:\n                period = 24 * 7  # 주간 주기\n\n    # STL의 seasonal 파라미터는 반드시 홀수여야 함\n    if period % 2 == 0:\n        period += 1\n\n    ts = pd.Series(data, index=index[: len(data)])\n    stl = STL(ts, seasonal=period, robust=True)\n    result = stl.fit()\n    deseasonalized = ts - result.seasonal\n    return deseasonalized.values, result\n\n\ndef comprehensive_deseasonalize(data, index, window_size):\n    \"\"\"다단계 계절성 제거 및 윈도우 생성\"\"\"\n    print(\"1단계: STL 분해로 주요 계절성 제거...\")\n    deseason_data, decomp_result = seasonal_decomposition_approach(data, index)\n\n    print(\"2단계: 일간 패턴 제거 (24시간 이동평균)...\")\n    daily_smooth = pd.Series(deseason_data).rolling(24, center=True).mean()\n    deseason_data = deseason_data - daily_smooth.fillna(0).values\n\n    print(\"3단계: 윈도우 생성 및 정규화...\")\n    windows = create_sliding_windows(deseason_data, window_size)\n    normalized_windows, scalers = normalize_windows(windows)\n\n    return normalized_windows, scalers, decomp_result\n\n\n# --- Data Extraction ---\nraw_data = df_value[value_col].values\nprint(f\"원본 데이터 크기: {len(raw_data)}\")\n\n# --- 1. 계절성 분해 시각화 ---\n_, decomp_result_for_plot = seasonal_decomposition_approach(raw_data, df_value.index)\n\nplt.figure(figsize=(10, 8)) # 너비 10으로 수정\nplt.subplot(4, 1, 1)\nplt.plot(decomp_result_for_plot.observed)\nplt.title('원본 데이터 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 2)\nplt.plot(decomp_result_for_plot.trend)\nplt.title('트렌드 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 3)\nplt.plot(decomp_result_for_plot.seasonal)\nplt.title('계절성 (2019년)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 4)\n# 계절성 제거된 데이터 계산\ndesasonalized_for_plot = (\n    decomp_result_for_plot.observed - decomp_result_for_plot.seasonal\n)\nplt.plot(desasonalized_for_plot)\nplt.title('계절성 제거 후 (2019년)')\nplt.ylabel(\"kWh\")\nplt.xlabel(\"시간\")\n\nplt.tight_layout()\nplt.show()\n\n# --- 2. 최종 데이터 처리 및 윈도우 생성 ---\nprint(\"\\n다단계 계절성 제거 및 윈도우 생성 시작...\")\nwindow_size = 10 # Week4 Optuna 최적값\nprocessed_windows, window_scalers, decomp_result = comprehensive_deseasonalize(\n    raw_data, df_value.index, window_size\n)\n\nprint(f\"계절성 제거 후 생성된 윈도우 수: {len(processed_windows)}\")\nprint(f\"각 윈도우 크기: {processed_windows.shape[1]}\")\n\n# PyTorch 텐서로 변환\nall_windows_torch = torch.from_numpy(processed_windows).unsqueeze(1).float()\nprint(f\"텐서 형태: {all_windows_torch.shape}\")\n\n# --- 3. 처리 전후 비교 시각화 ---\nplt.figure(figsize=(10, 6))\nplt.subplot(2, 1, 1)\nplt.plot(raw_data, alpha=0.8, label='원본 데이터 (2019년)')\nplt.title('원본 전력 사용량 데이터 (2019년)')\nplt.ylabel('kWh')\nplt.legend()\n\nplt.subplot(2, 1, 2)\ndeseason_full = raw_data - decomp_result.seasonal.values\nplt.plot(deseason_full, alpha=0.8, label='계절성 제거 후', color=\"orange\")\nplt.title('계절성 제거 후 데이터 (2019년)')\nplt.ylabel('kWh')\nplt.xlabel('시간')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n원본 데이터 크기: 8760\n\n\n\n\n\n\n\n\n\n\n다단계 계절성 제거 및 윈도우 생성 시작...\n1단계: STL 분해로 주요 계절성 제거...\n2단계: 일간 패턴 제거 (24시간 이동평균)...\n3단계: 윈도우 생성 및 정규화...\n계절성 제거 후 생성된 윈도우 수: 8751\n각 윈도우 크기: 10\n텐서 형태: torch.Size([8751, 1, 10])"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#모델-학습-및-이상치-탐지",
    "href": "posts/ABC_week05_real_data_analysis/index.html#모델-학습-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "3. 모델 학습 및 이상치 탐지",
    "text": "3. 모델 학습 및 이상치 탐지\nWeek4에서 최적화한 CNN 오토인코더 모델을 사용하여 실제 전력 데이터에 대한 이상치 탐지를 수행합니다. 학습, 평가, 시각화 과정을 하나의 논리적 흐름으로 통합하여 설명합니다.\n\n3.1. 모델 정의\nWeek4에서 Optuna를 통해 최적화한 모델 구조와 하이퍼파라미터를 그대로 사용합니다. 이 모델은 Dropout을 포함하여 과적합을 방지하고, ConvTranspose1d 대신 Upsample과 AdaptiveAvgPool1d를 사용하여 다양한 윈도우 크기에 유연하게 대응할 수 있도록 개선되었습니다.\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n\n\n3.2. 학습 및 평가\n실제 데이터에서는 패턴이 안정화된 구간을 ’정상’으로 간주하고 학습하는 것이 중요합니다. 전체 데이터의 20% ~ 80% 구간을 정상 데이터로 정의하고, 이 데이터로만 모델을 학습시킵니다. 그 후, 학습된 모델을 전체 데이터에 적용하여 이상치를 탐지합니다.\n\n# --- 1. 학습 데이터 분할 ---\ntotal_windows = len(all_windows_torch)\ntrain_start_idx = int(total_windows * 0.2)\ntrain_end_idx = int(total_windows * 0.8)\ntrain_windows_torch = all_windows_torch[train_start_idx:train_end_idx]\n\nprint(f\"전체 윈도우 수: {total_windows}\")\nprint(f\"학습 구간: {train_start_idx} ~ {train_end_idx}\")\nprint(f\"학습에 사용할 윈도우 수: {len(train_windows_torch)}\")\n\n# --- 2. 모델 초기화 및 학습 ---\n# Week4에서 찾은 최적 하이퍼파라미터 사용\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.191)\noptimizer = optim.Adam(model.parameters(), lr=0.001338)\ncriterion = nn.MSELoss()\n\ndataset = TensorDataset(train_windows_torch)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(\"\\n모델 학습 시작...\")\nepochs = 15\nfor epoch in range(epochs):\n    model.train()\n    for data_batch in data_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. 임계값 설정 및 이상치 탐지 ---\nmodel.eval()\n\n# 학습 데이터의 재구성 오차로 임계값 설정\nwith torch.no_grad():\n    reconstructed_train = model(train_windows_torch)\n    error_train = torch.mean(\n        (train_windows_torch - reconstructed_train) ** 2, dim=(1, 2)\n    )\n    train_reconstruction_error = error_train.numpy()\n\nquantile_level = 0.99\nthreshold = np.quantile(train_reconstruction_error, quantile_level)\nprint(f\"\\n임계값 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = model(all_windows_torch)\n    reconstruction_error = torch.mean(\n        (all_windows_torch - reconstructed_all) ** 2, dim=(1, 2)\n    ).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all) ** 2).squeeze().numpy()\n\n# 이상치 탐지\nanomaly_window_indices = np.where(reconstruction_error &gt; threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx &lt; len(pointwise_error):\n        max_error_idx_in_window = np.argmax(pointwise_error[window_idx])\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\npredicted_anomaly_points = [\n    idx for idx in predicted_anomaly_points if idx &lt; len(raw_data)\n]\nprint(f\"탐지된 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n\n전체 윈도우 수: 8751\n학습 구간: 1750 ~ 7000\n학습에 사용할 윈도우 수: 5250\n\n모델 학습 시작...\nEpoch [5/15], Loss: 0.033339\nEpoch [10/15], Loss: 0.049089\nEpoch [15/15], Loss: 0.046855\n\n임계값 (99.0% Quantile): 0.083021\n탐지된 이상치 포인트 수: 72\n\n\n\n\n3.3. 결과 시각화\n탐지된 이상치를 원본 데이터와 함께 시각화하여 모델의 성능을 직관적으로 확인합니다. 전체 기간과 최근 3개월 구간을 나누어 상세히 분석합니다.\n\nplt.figure(figsize=(10, 9))\n\n# 상단: 전체 데이터와 탐지 결과\nplt.subplot(3, 1, 1)\nplt.plot(raw_data, label='원본 전력 사용량 (2019년)', alpha=0.7, color='blue', linewidth=0.8)\nif predicted_anomaly_points:\n    plt.scatter(predicted_anomaly_points, raw_data[predicted_anomaly_points],\n                color='red', marker='x', s=80, linewidth=2, label=f'탐지된 이상치 ({len(predicted_anomaly_points)}개)', zorder=5)\nplt.title('실제 전력 사용량 데이터 이상 탐지 결과 (2019년)', fontsize=14, fontweight='bold')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 중간: 재구성 오차와 임계값\nplt.subplot(3, 1, 2)\nplt.plot(reconstruction_error, label='윈도우별 재구성 오차', color='steelblue', linewidth=1)\nplt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'임계값 ({threshold:.4f})')\nif anomaly_window_indices.any():\n    plt.scatter(anomaly_window_indices, reconstruction_error[anomaly_window_indices], \n               c='red', s=60, alpha=0.8, label=f'이상 윈도우 ({len(anomaly_window_indices)}개)', zorder=5)\nplt.title('윈도우별 재구성 오차 분포 (2019년)', fontsize=14, fontweight='bold')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 하단: 세분화된 구간 시각화 (맨 뒤 3개월치 데이터)\nplt.subplot(3, 1, 3)\nmonth_hours = 90 * 24\nstart_idx_viz = max(0, len(raw_data) - month_hours)\nend_idx_viz = len(raw_data)\n\nplt.plot(range(start_idx_viz, end_idx_viz), raw_data[start_idx_viz:end_idx_viz], \n         label=f'전력 사용량 (2019년, 최근 3개월)', alpha=0.8, color='navy', linewidth=1)\n\nmonth_anomalies = [i for i in predicted_anomaly_points if start_idx_viz &lt;= i &lt; end_idx_viz]\nif month_anomalies:\n    plt.scatter(month_anomalies, raw_data[month_anomalies],\n                color='red', marker='o', s=100, alpha=0.8, \n                label=f'최근 3개월 이상치 ({len(month_anomalies)}개)', zorder=5)\n\nplt.title('세분화된 이상 탐지 결과 (2019년, 최근 3개월)', fontsize=14, fontweight='bold')\nplt.xlabel('시간 스텝')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# 탐지 결과 요약 출력\nprint(f\"\\n=== 이상 탐지 결과 요약 (2019년) ===\")\nprint(f\"분석 데이터 길이: {len(raw_data):,} 시간\")\nprint(f\"전체 탐지된 이상치: {len(predicted_anomaly_points)}개\")\nprint(f\"최근 3개월 구간 이상치: {len(month_anomalies)}개\")\nprint(f\"이상치 비율: {len(predicted_anomaly_points) / len(raw_data) * 100:.3f}%\")\n\n\n\n\n\n\n\n\n\n=== 이상 탐지 결과 요약 (2019년) ===\n분석 데이터 길이: 8,760 시간\n전체 탐지된 이상치: 72개\n최근 3개월 구간 이상치: 23개\n이상치 비율: 0.822%"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#결론-이론에서-실전으로-모델의-성장기",
    "href": "posts/ABC_week05_real_data_analysis/index.html#결론-이론에서-실전으로-모델의-성장기",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "4. 결론: 이론에서 실전으로, 모델의 성장기",
    "text": "4. 결론: 이론에서 실전으로, 모델의 성장기\n이번 5주차에서는 지난 4주간 개발해온 CNN 오토인코더 모델을 실제 전력 사용량 데이터에 적용해봤습니다. 단순히 모델을 돌려보는 데서 그치지 않고, 실제 데이터가 가진 복잡함과 예측 불가능성을 직접 마주하며, 이론과 실전의 간극을 좁히는 과정을 경험했습니다.\n\n진행 과정\n\n체계적인 개선의 힘\nWeek 4에서 정립한 ’윈도우별 정규화 → Dropout으로 과적합 방지 → Optuna 하이퍼파라미터 최적화 → Quantile 기반 임계값 설정’이라는 프로세스가 실제 데이터에서도 효과적이었습니다. 감에 의존하지 않고 논리적으로 접근하는 것이 얼마나 중요한지 다시 한 번 확인할 수 있었습니다.\n데이터를 중심에 두기\n실제 데이터는 노이즈와 계절성이 강하게 섞여 있습니다. STL 분해와 이동평균을 결합한 다단계 전처리 덕분에, 모델이 본질적인 패턴에 집중할 수 있었고, 이상 신호도 더 명확하게 잡아낼 수 있었습니다. 결국 모델링의 핵심은 데이터를 얼마나 잘 이해하고 다루느냐에 달려 있다는 점을 실감했습니다.\n’정상’의 기준 고민하기\n실제 데이터에서는 ’정상’이 무엇인지 정의하는 것부터 쉽지 않습니다. 데이터의 20%~80% 구간을 정상으로 간주해 학습에 사용했고, 덕분에 모델이 전체 변동성에 휘둘리지 않고 진짜 이상 신호에 집중할 수 있었습니다. 이상 탐지에서는 도메인 지식과 합리적인 가정이 필수라는 점도 다시 느꼈습니다.\n\nWeek 1의 데이터 탐색부터 Week 5의 실제 데이터 적용까지, 시계열 이상 탐지라는 목표를 향해 한 단계씩 나아갔습니다. 단순한 베이스라인에서 출발해 점진적으로 성능을 개선하고, 실제 데이터의 복잡성까지 다룰 수 있게 된 이번 과정은, 모델링이 단순히 코드를 짜는 일이 아니라 문제를 정의하고, 가설을 세우고, 실험하고, 검증하는 일련의 탐구라는 사실을 다시 한 번 확인했습니다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#결론",
    "href": "posts/ABC_week05_real_data_analysis/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "4. 결론",
    "text": "4. 결론\n이번 5주차에서는 지난 4주간 개발해온 CNN 오토인코더 모델을 실제 전력 사용량 데이터에 적용해봤습니다. 단순히 모델을 돌려보는 데서 그치지 않고, 실제 데이터가 가진 복잡함과 예측 불가능성을 직접 마주하며, 이론과 실전의 간극을 좁히는 과정을 경험했습니다.\n\n진행 과정\n\n체계적인 개선\nWeek 4에서 정립한 ’윈도우별 정규화 → Dropout으로 과적합 방지 → Optuna 하이퍼파라미터 최적화 → Quantile 기반 임계값 설정’이라는 프로세스가 실제 데이터에서도 효과적이었습니다. 감에 의존하지 않고 논리적으로 접근하는 것이 얼마나 중요한지 다시 한 번 확인할 수 있었습니다.\n데이터를 중심에 두기\n실제 데이터는 노이즈와 계절성이 강하게 섞여 있습니다. STL 분해와 이동평균을 결합한 다단계 전처리 덕분에, 모델이 본질적인 패턴에 집중할 수 있었고, 이상 신호도 더 명확하게 잡아낼 수 있었습니다. 결국 모델링의 핵심은 데이터를 얼마나 잘 이해하고 다루느냐에 달려 있다는 점을 실감했습니다.\n’정상’의 기준 고민하기\n실제 데이터에서는 ’정상’이 무엇인지 정의하는 것부터 쉽지 않습니다. 데이터의 20%~80% 구간을 정상으로 간주해 학습에 사용했고, 덕분에 모델이 전체 변동성에 휘둘리지 않고 진짜 이상 신호에 집중할 수 있었습니다. 이상 탐지에서는 도메인 지식과 합리적인 가정이 필수라는 점도 다시 느꼈습니다.\n\nWeek 1의 데이터 탐색부터 Week 5의 실제 데이터 적용까지, 시계열 이상 탐지라는 목표를 향해 한 단계씩 나아갔습니다. 단순한 베이스라인에서 출발해 점진적으로 성능을 개선하고, 실제 데이터의 복잡성까지 다룰 수 있게 된 이번 과정은, 모델링이 단순히 코드를 짜는 일이 아니라 문제를 정의하고, 가설을 세우고, 실험하고, 검증하는 일련의 탐구라는 사실을 다시 한 번 확인했습니다."
  }
]