[
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html",
    "href": "posts/ABC_week03_cnn_baseline/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 세 번째 기술노트입니다. 이번 주는 시계열 데이터의 ’패턴’을 학습할 수 있는 딥러닝, 그중에서도 CNN을 활용한 이상 탐지의 첫걸음을 PyTorch로 구현해 보겠습니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "href": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)",
    "text": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)\n시계열 데이터를 CNN 모델에 입력하려면 연속된 데이터를 일정한 길이의 조각(window)으로 나누는 ‘슬라이딩 윈도우’ 기법이 필요합니다. 이 방법은 데이터의 시간적 패턴을 학습하는 데 유용합니다.\n\n슬라이딩 윈도우 구현\n아래는 numpy를 사용해 슬라이딩 윈도우를 구현하는 간단한 Python 함수입니다:\n\nimport numpy as np\n\ndef sliding_window(data, window_size, step_size=1):\n    \"\"\"시계열 데이터를 슬라이딩 윈도우로 변환\"\"\"\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 예제 데이터\ndata = np.sin(np.linspace(0, 20, 100))\nwindowed_data = sliding_window(data, window_size=10)\nprint(\"윈도우 형태:\", windowed_data.shape)\n\n윈도우 형태: (91, 10)"
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "href": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)",
    "text": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)\n\n오토인코더란?\n오토인코더는 데이터를 압축(인코더)했다가 다시 복원(디코더)하도록 학습하는 딥러닝 모델입니다. 정상 데이터는 잘 복원되지만, 이상 데이터는 복원이 잘 되지 않아 재구성 오차가 커지는 특징을 활용합니다.\n\n\n모델 구조\n\n인코더 (Encoder): Conv1D와 MaxPooling1D 층을 사용해 입력 데이터의 특징을 추출하고 압축합니다.\n디코더 (Decoder): ConvTranspose1D (또는 Upsample + Conv1D) 층을 사용해 데이터를 복원합니다.\n\n\n\nPyTorch 구현\n아래는 PyTorch를 사용한 간단한 1D CNN 오토인코더 모델 구현입니다:\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoder(nn.Module):\n    def __init__(self, input_shape): # input_shape: (sequence_length, num_features)\n        super(CNNAutoencoder, self).__init__()\n        # Encoder\n        # input_shape[1]은 특성 수 (in_channels로 사용)\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/2로 감소\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/4로 감소\n\n        # Decoder\n        # 인코더에서 시퀀스 길이가 1/4로 줄었으므로, 디코더에서 원래 길이로 복원\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1) # 원본 특성 수로 복원\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_conv_final(x)\n        decoded = self.sigmoid(x)\n        return decoded\n\n# 모델 생성 및 컴파일은 data-generation 셀 이후로 이동합니다.\n# input_shape도 window_size를 사용하도록 수정됩니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "href": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "3. 모델 학습 및 이상치 탐지",
    "text": "3. 모델 학습 및 이상치 탐지\n\n데이터 생성\nWeek2에서 사용한 샘플 데이터를 기반으로 정상/비정상 데이터를 생성합니다:\n\n# numpy는 sliding_window_implementation 셀에서 이미 import 됨\n\n# 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# 슬라이딩 윈도우 적용\nwindow_size = 10\nwindows = sliding_window(data, window_size) # (N, L) -&gt; (N, window_size)\nwindows = windows[..., np.newaxis]  # (N, L, C) -&gt; (N, window_size, 1)\n# PyTorch Conv1d는 (N, C, L) 입력을 기대하므로 차원 변경\nwindows = windows.transpose(0, 2, 1) # (N, C, L) -&gt; (N, 1, window_size)\nprint(f\"윈도우 데이터 형태 (N, C, L): {windows.shape}\")\n\n윈도우 데이터 형태 (N, C, L): (91, 1, 10)\n\n\n\nimport torch.optim as optim # PyTorch 옵티마이저\n\n# 모델 생성\n# input_shape은 (window_size, 1) 이어야 합니다. (sequence_length, num_features)\n# data-generation 셀에서 windows는 (N, 1, window_size) 형태로 준비됨.\n# CNNAutoencoder의 __init__은 input_shape=(window_size, 1)을 받아 input_shape[1]=1을 in_channels로 사용.\nmodel_input_shape = (window_size, 1) # (sequence_length, num_features)\nmodel = CNNAutoencoder(model_input_shape) # cnn-autoencoder-definition 셀에서 정의된 클래스 사용\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss() # 평균 제곱 오차 손실\n\nprint(\"PyTorch 모델 구조:\")\nprint(model)\n\nPyTorch 모델 구조:\nCNNAutoencoder(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n  (sigmoid): Sigmoid()\n)\n\n\n\n\n모델 학습\n정상 데이터만 사용해 모델을 학습합니다:\n\n# torch는 cnn-autoencoder-definition 셀에서 이미 import 됨\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 정상 데이터로 학습\n# 'outliers'는 원본 'data' 배열의 인덱스입니다.\n# 'windows' 배열에서 이상치가 포함된 윈도우를 식별하여 제외합니다.\ncontaminated_window_indices = set()\n# 'outliers', 'window_size', 'windows' 변수는 이전 셀들에서 정의되어 있어야 합니다.\nfor outlier_data_idx in outliers: \n    start_contaminated_win_idx = max(0, outlier_data_idx - window_size + 1)\n    end_contaminated_win_idx = outlier_data_idx \n    \n    for win_idx in range(start_contaminated_win_idx, end_contaminated_win_idx + 1):\n        if win_idx &lt; len(windows): # 윈도우 인덱스가 유효한 범위 내에 있는지 확인\n            contaminated_window_indices.add(win_idx)\n\nnormal_windows_mask = np.ones(len(windows), dtype=bool)\nif contaminated_window_indices: # set이 비어있지 않은 경우에만 인덱싱\n    normal_windows_mask[list(contaminated_window_indices)] = False\n\nnormal_windows_np = windows[normal_windows_mask]\n\nif len(normal_windows_np) == 0:\n    print(\"경고: 학습에 사용할 정상 윈도우가 없습니다. Outlier 정의, window_size 또는 데이터 길이를 확인하세요.\")\nelse:\n    # PyTorch 데이터셋 및 로더 준비\n    normal_windows_torch = torch.tensor(normal_windows_np, dtype=torch.float32)\n    train_dataset = TensorDataset(normal_windows_torch) # 오토인코더는 입력과 타겟이 동일\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n    # 모델 학습\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device)\n    \n    epochs = 50 # 에포크 수 설정\n    print_every_epochs = 10\n\n    model.train() # 학습 모드\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_data_list in train_loader:\n            inputs = batch_data_list[0].to(device)\n            targets = inputs # 오토인코더의 타겟은 입력과 동일\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item() * inputs.size(0) # 배치 손실 누적 (loss.item()은 평균 손실)\n        \n        epoch_loss /= len(train_loader.dataset) # 에포크 평균 손실\n        if (epoch + 1) % print_every_epochs == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n    print(\"모델 학습 완료.\")\n\nUsing device: cuda\nEpoch [10/50], Loss: 0.466470\nEpoch [20/50], Loss: 0.291450\nEpoch [30/50], Loss: 0.264741\nEpoch [40/50], Loss: 0.224466\nEpoch [50/50], Loss: 0.209135\n모델 학습 완료.\n\n\n\n\n재구성 오차 계산 및 이상치 탐지\n학습된 모델로 데이터를 복원하고, 재구성 오차를 계산합니다:\n\n# torch 및 numpy는 이전 셀들에서 이미 import 됨\n\n# 재구성 오차 계산\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() # 평가 모드\n\n# 전체 windows 데이터를 PyTorch 텐서로 변환하고 device로 이동\nall_windows_torch = torch.tensor(windows, dtype=torch.float32).to(device)\n\n# 메모리 부족을 방지하기 위해 배치 단위로 처리할 수 있으나, 현재 데이터는 작으므로 한번에 처리\nwith torch.no_grad(): # 그래디언트 계산 비활성화\n    reconstructed_torch = model(all_windows_torch)\n\n# 결과를 CPU로 옮기고 NumPy 배열로 변환\nreconstructed_np = reconstructed_torch.cpu().numpy()\n\n# MAE (Mean Absolute Error) 계산\n# 원본 windows (numpy 배열)와 reconstructed_np 모두 (N, 1, window_size) 형태\n# axis=(1, 2)는 채널과 시퀀스 길이에 대한 평균을 의미\nmae = np.mean(np.abs(windows - reconstructed_np), axis=(1, 2))\nprint(f\"계산된 MAE 값 (처음 5개): {mae[:5]}\")\n\n# 이상치 탐지를 위한 임계값 설정 (데이터 및 모델 성능에 따라 조정 필요)\n# 예: MAE의 평균 + (표준편차 * 특정 배수) 또는 분위수 사용\nthreshold = np.mean(mae) + 1.5 * np.std(mae) # 표준편차 배수를 2에서 1.5로 줄여 민감도 증가\nprint(f\"이상치 탐지 임계값 (MAE): {threshold:.4f}\")\n\nanomalies_indices_in_windows = np.where(mae &gt; threshold)[0] # 윈도우 배열 내의 인덱스\n\nprint(f\"이상치로 탐지된 윈도우의 수: {len(anomalies_indices_in_windows)}\")\nprint(f\"이상치로 탐지된 윈도우 인덱스: {anomalies_indices_in_windows}\")\n\n# 윈도우 인덱스를 원본 데이터 인덱스로 대략적으로 매핑 (윈도우의 시작점 기준)\n# 실제 이상치 발생 시점과 정확히 일치하지 않을 수 있음\nanomalies_approx_original_indices = anomalies_indices_in_windows \n# 좀 더 정확하게는 윈도우의 중간 지점 등을 고려할 수 있으나, 여기서는 시작점으로 단순화\n# anomalies_approx_original_indices = [idx + window_size // 2 for idx in anomalies_indices_in_windows]\nprint(f\"원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): {anomalies_approx_original_indices}\")\n\n계산된 MAE 값 (처음 5개): [0.12926425 0.12913459 0.10146508 0.06732681 0.0824157 ]\n이상치 탐지 임계값 (MAE): 0.9529\n이상치로 탐지된 윈도우의 수: 8\n이상치로 탐지된 윈도우 인덱스: [17 18 19 20 47 48 49 50]\n원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): [17 18 19 20 47 48 49 50]\n\n\n\n\n결과 시각화\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(data, label='원본 데이터', alpha=0.7) # 'data'는 data-generation에서 정의됨\nplt.scatter(outliers, data[outliers], color='red', s=100, label='실제 이상치 (Ground Truth)', marker='o', edgecolors='black') # 'outliers'는 data-generation에서 정의됨\n\n# anomalies_approx_original_indices가 비어있을 수 있으므로 확인\nif len(anomalies_approx_original_indices) &gt; 0:\n    # 탐지된 이상치 표시는 윈도우의 시작점을 기준으로 함\n    plt.scatter(anomalies_approx_original_indices, data[anomalies_approx_original_indices], \n                color='orange', marker='x', s=80, label='탐지된 이상치 (모델 예측)', alpha=0.8)\nelse:\n    print(\"탐지된 이상치가 없습니다.\")\n        \nplt.legend()\nplt.title('PyTorch CNN 오토인코더 기반 시계열 이상 탐지')\nplt.xlabel('시간 스텝')\nplt.ylabel('값')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# 재구성 오차(MAE) 시각화\nplt.figure(figsize=(10, 3))\nplt.plot(mae, label='재구성 오차 (MAE)', color='green')\nplt.axhline(threshold, color='red', linestyle='--', label=f'임계값 ({threshold:.2f})')\nplt.title('윈도우별 재구성 오차 (MAE) 및 임계값')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('MAE')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\nPyTorch CNN 오토인코더 기반 이상 탐지 결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n탐지 결과 분석 및 고려사항\n시각화 결과와 재구성 오차 검토 시 다음 사항을 고려해야 한다.\n\n실제 이상치 vs. 탐지 이상치:\n\ndata-generation 단계에서 의도적으로 넣은 실제 이상치(outliers = [20, 50, 80])와 모델의 탐지 결과는 다를 수 있다.\n모든 실제 이상치가 탐지되지 않거나, 정상이 이상치로 잘못 탐지될 가능성이 항상 존재한다.\n현 예제는 임계값(np.mean(mae) + 1.5 * np.std(mae)) 조정을 통해 최소 하나의 이상치를 탐지하도록 유도했다. 실제 상황에서는 모델 성능, 데이터 특성, window_size, 임계값 설정에 따라 결과가 크게 달라진다.\n\n윈도우 경계 효과 (Edge Effects):\n\n시계열 데이터의 시작과 끝 부분 윈도우는 내부 윈도우에 비해 정보가 불완전할 수 있다 (이전/이후 데이터 부재).\nCNN 모델, 특히 패딩 사용 시, 경계 영역 윈도우는 학습된 주 정상 패턴과 달라 재구성 오차가 상대적으로 커질 수 있다.\n결과적으로, 시계열 양 끝부분에서 이상치가 아닌데도 이상치로 탐지되는 경향이 나타날 수 있다. MAE 그래프에서 초반 또는 후반부에 높은 오차가 관찰된다면 이 효과를 의심해볼 수 있다.\n\nwindow_size의 중요성:\n\nwindow_size는 모델이 학습할 패턴의 길이를 결정한다.\n너무 작으면 장기 패턴 파악이 어렵고, 너무 크면 짧은 순간의 이상치를 놓치거나 정상 변동에도 민감하게 반응할 수 있다.\n현재 window_size=10으로 설정했다. 데이터 특성에 맞춰 이 값을 조정하며 실험하는 과정이 중요하다.\n\n모델 및 임계값의 한계:\n\n여기서 사용한 CNN 오토인코더는 비교적 단순한 모델이다.\n더 복잡한 패턴이나 다양한 유형의 이상치를 탐지하려면 모델 구조 개선(예: LSTM, Transformer 기반 오토인코더)이나 다른 접근법을 고려해야 한다.\n고정 임계값 대신 동적 임계값을 사용하거나, 통계적 검정 기법을 결합하는 것도 탐지 성능을 높이는 데 도움이 될 수 있다.\n\n\n이런 점들을 고려해 모델 결과를 해석해야 하며, 실제 문제 적용 시에는 충분한 검증과 실험이 필수다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "href": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "결론 및 다음 단계",
    "text": "결론 및 다음 단계\n이번 주에는 PyTorch로 간단한 1D CNN 오토인코더를 만들고, 시계열 이상 탐지를 수행했다. 이 모델은 시계열 이상 탐지의 괜찮은 시작점이 될 수 있다. 재구성 오차를 기반으로 이상치를 찾는 과정과, 임계값 설정에 따라 탐지 결과가 어떻게 달라지는지 확인했다.\n다음 포스트에서는 실제 산업 데이터를 사용해 모델을 학습시키고, 성능을 개선할 다양한 방법(예: 더 복잡한 모델 구조, 다른 유형의 오토인코더, 동적 임계값 설정 등)을 살펴볼 예정이다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html",
    "href": "posts/ABC_week01_data analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "",
    "text": "유클리드소프트에서 진행하는 ABC 프로젝트 멘토링에 8기로 참여하게 되었습니다.   [산업 전력 데이터의 이상치 탐지 성능 향상 솔루션 구축]을 주제로 다양한 데이터 분석 및 인공지능 기법을 학습하고 실제 프로젝트에 적용해볼 예정입니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 데이터란?",
    "text": "시계열 데이터란?\n시계열 데이터(Time Series Data)는 일정 시간 간격으로 기록된 데이터 포인트들의 순차적인 집합입니다. 예를 들어, 시간별 산업 설비의 전력 사용량, 일별 주가, 월별 웹사이트 방문자 수 등이 시계열 데이터에 해당합니다. 이러한 데이터는 시간의 흐름에 따른 변화와 패턴을 분석하는 데 사용되며, 특히 정상적인 패턴에서 벗어나는 ’이상치’를 탐지하는 데 중요한 기초 자료가 됩니다.\n시계열 데이터는 주로 다음과 같은 특징을 가집니다:\n\n추세 (Trend): 데이터가 장기적으로 증가하거나 감소하는 경향.\n계절성 (Seasonality): 특정 주기(예: 하루, 주, 월)에 따라 반복되는 패턴.\n주기성 (Cyclicality): 계절성보다 긴, 고정되지 않은 주기의 변동.\n불규칙 변동 (Irregular Fluctuations/Noise): 위 요소들로 설명되지 않는 무작위적 변동."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 이상 탐지에서 EDA와 전처리의 중요성",
    "text": "시계열 이상 탐지에서 EDA와 전처리의 중요성\n이상치(Anomaly) 또는 특이점(Outlier)은 일반적인 데이터 패턴에서 현저하게 벗어나는 관측치를 의미합니다. 산업 전력 데이터에서 이상치는 설비 고장, 에너지 누수, 비정상적 공정 운영 등 중요한 문제를 시사할 수 있습니다. 효과적인 이상 탐지를 위해서는 데이터에 대한 깊이 있는 이해가 선행되어야 하며, 탐색적 데이터 분석(EDA)과 적절한 전처리는 이 과정의 핵심입니다.\nEDA와 전처리가 중요한 이유:\n\n데이터 특성 파악: 데이터의 분포, 추세, 계절성 등 기본적인 통계적 특성을 이해하여 ‘정상’ 상태의 기준을 설정하는 데 도움을 줍니다.\n잠재적 이상치 식별: 시각화 등을 통해 예상치 못한 급증, 급감 또는 패턴 변화를 초기에 발견할 수 있습니다.\n데이터 품질 향상: 결측치 처리, 노이즈 제거 등을 통해 분석의 정확도를 높입니다.\n피처 엔지니어링 기반 마련: 분석 목적에 맞는 새로운 변수를 생성하거나 기존 변수를 변환하는 데 필요한 통찰력을 제공합니다.\n적절한 이상 탐지 모델 선택 지원: 데이터의 특성에 맞는 이상 탐지 알고리즘을 선택하는 데 중요한 정보를 제공합니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "href": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초",
    "text": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초\nPython의 pandas, numpy, matplotlib, seaborn 라이브러리를 사용하여 산업 전력 사용량 데이터를 가정하고, 이상 탐지를 위한 기본적인 EDA 및 전처리 과정을 살펴보겠습니다.\n\n1. 필요한 라이브러리 불러오기\n데이터 분석 및 시각화에 필요한 라이브러리를 가져옵니다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 향상된 시각화를 위한 Seaborn\nfrom datetime import datetime\n\n# 경고 메시지 무시 (선택 사항)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n2. 분석용 샘플 시계열 데이터 생성 (가상 산업 전력 사용량)\n실제 산업 전력 데이터와 유사한 특성을 갖도록 가상 데이터를 생성합니다. 여기에는 일정한 기본 사용량, 약간의 증가 추세, 주간 계절성(평일 사용량 증가, 주말 감소), 그리고 몇 개의 인위적인 이상치(스파이크 및 급감)를 포함시킵니다.\n\n# 재현성을 위한 시드 설정\nnp.random.seed(42)\n\n# 날짜 범위 생성 (약 1년)\ndate_rng = pd.date_range(start='2025-01-01', periods=365, freq='D')\ndata = pd.DataFrame(date_rng, columns=['date'])\n\n# 기본 전력 사용량 설정 및 추세 생성\nbaseline_usage = 100  # 기본 사용량 (예: kWh)\ntrend_factor = np.linspace(0, 20, len(date_rng)) # 선형 증가 추세\n\n# 주간 계절성 생성 (월:0 ~ 일:6)\n# 산업 데이터 특성상 평일 사용량 높고, 주말 낮음\nday_of_week_effect = np.array([15, 18, 20, 19, 17, 5, 3])\nseasonal_factor = np.array([day_of_week_effect[day.weekday()] for day in date_rng])\n\n# 임의의 노이즈 생성\nnoise = np.random.normal(0, 5, size=(len(date_rng))) # 평균 0, 표준편차 5\n\n# 데이터 생성 (전력 사용량 = 기본값 + 추세 + 계절성 + 노이즈)\ndata['power_usage'] = baseline_usage + trend_factor + seasonal_factor + noise\n\n# 인위적인 이상치(스파이크 및 급감) 추가\ndata.loc[data.index[50], 'power_usage'] += 70  # 51번째 날에 큰 스파이크\ndata.loc[data.index[150], 'power_usage'] -= 50 # 151번째 날에 큰 폭 하락\ndata.loc[data.index[250], 'power_usage'] += 80  # 251번째 날에 큰 스파이크\n\n# 데이터 값 보정 (음수 방지 및 최소값 설정)\ndata['power_usage'] = data['power_usage'].astype(float).clip(lower=10)\n\n# 'date' 컬럼을 인덱스로 설정\ndata.set_index('date', inplace=True)\n\nprint(\"생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\")\nprint(data.head())\nprint(\"\\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\")\nprint(data.tail())\n\n생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\n            power_usage\ndate                   \n2025-01-01   122.483571\n2025-01-02   118.363624\n2025-01-03   120.348333\n2025-01-04   112.779984\n2025-01-05   102.049013\n\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\n            power_usage\ndate                   \n2025-12-27   127.376952\n2025-12-28   130.498859\n2025-12-29   134.346309\n2025-12-30   139.953614\n2025-12-31   143.450720\n\n\n이 샘플 데이터는 power_usage라는 이름으로 전력 사용량 정보를 가지며, EDA 과정에서 이상치를 시각적으로 탐색하는 데 사용됩니다.\n\n\n\n3. 데이터 기본 탐색\n데이터의 구조와 기본적인 통계적 특성을 확인합니다.\n\nprint(\"데이터 정보:\")\ndata.info()\n\nprint(\"\\n기술 통계량:\")\nprint(data.describe())\n\nprint(f\"\\n결측치 확인: {data.isnull().sum().sum()} 개\")\n# data.isnull().sum() # 컬럼별 결측치 확인\n\n데이터 정보:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 365 entries, 2025-01-01 to 2025-12-31\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   power_usage  365 non-null    float64\ndtypes: float64(1)\nmemory usage: 5.7 KB\n\n기술 통계량:\n       power_usage\ncount   365.000000\nmean    124.197677\nstd      11.877300\nmin      64.494222\n25%     117.423353\n50%     124.093359\n75%     131.043033\nmax     202.431844\n\n결측치 확인: 0 개\n\n\ninfo()는 데이터 타입, 인덱스 정보, 메모리 사용량 등을 보여줍니다. describe()는 평균, 표준편차, 최소/최대값, 사분위수 등 주요 기술 통계량을 제공하여 데이터의 전반적인 분포를 파악하는 데 도움을 줍니다. 결측치가 있다면 이상 탐지 분석 전에 적절히 처리(예: 보간, 제거)해야 합니다. 이 샘플에서는 결측치가 없습니다.\n\n\n\n4. 주요 시각화를 통한 탐색적 데이터 분석 (EDA)\n시각화는 시계열 데이터의 패턴과 잠재적 이상치를 발견하는 데 매우 효과적입니다.\n\n4.1. 기본 시계열 플롯\n전체 기간에 대한 전력 사용량 변화를 시각화하여 추세, 계절성, 그리고 눈에 띄는 이상 패턴을 관찰합니다.\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='dodgerblue', linewidth=1.5)\nplt.title('일별 가상 산업 전력 사용량', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: 일별 가상 산업 전력 사용량\n\n\n\n\n\n위 그래프에서 전반적인 증가 추세와 주기적인 변동(계절성) 외에도, 몇몇 지점에서 급격한 스파이크나 하락(우리가 삽입한 이상치)이 시각적으로 확인됩니다. 실제 데이터 분석 시 이러한 지점들이 조사 대상이 됩니다.\n\n\n\n4.2. 데이터 분포 확인 (히스토그램 및 KDE)\n전력 사용량 값들의 분포를 확인하여 데이터가 특정 구간에 집중되어 있는지, 또는 분포에서 벗어나는 값들이 있는지 살펴봅니다.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data['power_usage'], kde=True, color='mediumseagreen', bins=30)\nplt.title('전력 사용량 분포 (히스토그램 및 KDE)', fontsize=16)\nplt.xlabel('전력 사용량 (kWh)', fontsize=12)\nplt.ylabel('빈도', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: 전력 사용량 분포\n\n\n\n\n\n히스토그램과 KDE(Kernel Density Estimate) 플롯은 데이터 값의 분포를 보여줍니다. 만약 분포의 꼬리 부분에 값이 드물게 나타난다면 이는 이상치일 가능성이 있습니다. 우리가 삽입한 인위적인 스파이크 값들이 분포의 오른쪽 꼬리 부분에 나타날 수 있습니다.\n\n\n\n4.3. 주기적 패턴 확인 (요일별 Box Plot)\n산업 데이터는 요일이나 월별로 뚜렷한 주기성을 가질 수 있습니다. Box plot을 사용하면 이러한 주기성 내에서 평소와 다른 패턴을 보이는 시점을 파악하는 데 유용합니다.\n\n# 분석을 위해 'day_of_week' 컬럼 추가 (월요일=0, 일요일=6)\ndata['day_of_week'] = data.index.dayofweek\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='day_of_week', y='power_usage', data=data, palette='coolwarm')\nplt.title('요일별 전력 사용량 분포', fontsize=16)\nplt.xlabel('요일 (0:월, 1:화, 2:수, 3:목, 4:금, 5:토, 6:일)', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.xticks(ticks=range(7), labels=['월', '화', '수', '목', '금', '토', '일'])\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: 요일별 전력 사용량 분포\n\n\n\n\n\n요일별 Box plot은 각 요일의 전력 사용량 분포를 보여줍니다. 각 박스는 해당 요일 데이터의 중앙 50%(IQR: Interquartile Range)를 나타내며, 박스 외부의 점들은 잠재적인 이상치(outliers)를 의미합니다. 이 샘플에서는 주말(토, 일) 사용량이 평일보다 낮은 패턴이 뚜렷하며, 우리가 인위적으로 삽입한 이상치들이 특정 요일의 일반적인 범위를 벗어나 점으로 표시될 수 있습니다. 예를 들어, 화요일(1)에 발생시킨 스파이크는 화요일의 박스 플롯에서 상단 이상치로 나타날 가능성이 큽니다.\n\n\n\n\n5. 이동 평균을 활용한 추세 및 변동성 관찰\n이동 평균(Moving Average)은 단기적인 변동을 완화하여 장기적인 추세를 파악하거나, 데이터의 일반적인 수준을 나타내는 기준으로 활용될 수 있습니다. 원본 데이터와 이동 평균선을 함께 시각화하면, 이동 평균에서 크게 벗어나는 지점들을 이상치 후보로 간주할 수 있습니다.\n\n# 7일 이동 평균 계산\ndata['rolling_mean_7'] = data['power_usage'].rolling(window=7, center=True).mean() # center=True로 설정하여 lag 감소 효과\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='lightskyblue', alpha=0.8, linewidth=1)\nplt.plot(data.index, data['rolling_mean_7'], label='7일 이동 평균 (중앙 정렬)', color='orangered', linewidth=2)\nplt.title('일별 전력 사용량 및 7일 이동 평균', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 4: 전력 사용량과 7일 이동 평균\n\n\n\n\n\n\n# 이동 평균과의 차이(잔차와 유사한 개념)를 통해 이상치 강조\ndata['deviation_from_ma'] = data['power_usage'] - data['rolling_mean_7']\n\nplt.figure(figsize=(9,5))\nplt.plot(data.index, data['deviation_from_ma'], label='이동 평균과의 편차', color='teal', linewidth=1, marker='o', markersize=3, linestyle='None')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8) # 기준선\n\n# 편차의 임계값을 설정하여 이상치 후보 시각화 (예: 편차의 표준편차 기반)\n# 이동 평균 계산 시 초반/후반 NaN 값이 있을 수 있으므로 dropna() 사용\ndeviation_std = data['deviation_from_ma'].dropna().std()\nupper_threshold = 3 * deviation_std\nlower_threshold = -3 * deviation_std\n\nplt.axhline(upper_threshold, color='red', linestyle=':', linewidth=1.5, label=f'+3σ ({upper_threshold:.2f})')\nplt.axhline(lower_threshold, color='red', linestyle=':', linewidth=1.5, label=f'-3σ ({lower_threshold:.2f})')\nplt.title('이동 평균과의 편차 (이상치 탐색 보조)', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('편차 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 5: 이동 평균과의 편차 (이상치 탐색 보조)\n\n\n\n\n\n7일 이동 평균선은 데이터의 단기적 변동을 평탄화하여 보여줍니다. rolling() 함수에서 center=True 옵션을 사용하면 이동 평균 계산 시 윈도우의 중앙에 값을 위치시켜 시각화 시 원본 데이터와의 지연(lag)을 줄이는 데 도움이 됩니다.\n두 번째 그래프는 원본 데이터와 이동 평균과의 편차를 보여줍니다. 이 편차가 특정 임계값(예: 편차의 3 표준편차, ±3σ)을 넘어서는 지점들은 잠재적인 이상치로 간주할 수 있습니다. 우리가 삽입한 인위적인 스파이크와 급감 지점에서 편차가 크게 나타나는 것을 확인할 수 있습니다. 이러한 방법은 간단하면서도 효과적인 이상치 탐색의 기초가 됩니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#요약",
    "href": "posts/ABC_week01_data analysis/index.html#요약",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "요약",
    "text": "요약\n이 포스트에서는 Python을 사용하여 가상의 산업 전력 사용량 데이터를 생성하고, 이상 탐지를 위한 기본적인 탐색적 데이터 분석(EDA) 및 전처리 과정을 살펴보았습니다. 시계열 플롯, 분포 확인, 주기성 분석(요일별 Box Plot), 이동 평균 활용 등은 데이터의 특성을 이해하고 잠재적인 이상치를 식별하는 데 효과적인 방법입니다."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "컴퓨터공학과 3학년입니다.\n\nData Scientist AI-powered problem solver who applies research and technology to real-world challenges."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n지난주 CNN 오토인코더 모델의 한계를 분석하고, 성능 개선을 위한 다양한 방법과 하이퍼파라미터 최적화 과정을 기록합니다.\n\n\n\n\n\nJun 14, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\nPyTorch를 사용하여 1D CNN 오토인코더 기반 시계열 이상 탐지 베이스라인 모델을 구현합니다.\n\n\n\n\n\nJun 8, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지 - 머신러닝 기법 적용 실습\n\n\n\n\n\nJun 1, 2025\n\n\nBeomdo Park\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지를 위한 기본 EDA 및 전처리 방법을 다룹니다.\n\n\n\n\n\nMay 25, 2025\n\n\nBeomdo Park\n\n8 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html",
    "href": "posts/ABC_week02_time_series_anomaly/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "",
    "text": "안녕하세요 이번 포스트는 ABC 프로젝트 멘토링 8기 2주차 실습 기록입니다. 지난주엔 시계열 데이터 EDA랑 전처리만 했는데, 이번엔 간단한 머신러닝 모델로 이상치 탐지 기법을 소개하려 합니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "1. 데이터 준비",
    "text": "1. 데이터 준비\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.random.seed(42)\nt = np.arange(0, 100, 1)\ny = np.sin(0.2 * t) + np.random.normal(0, 0.2, size=len(t))\n# 여러 위치에 인위적으로 이상치 추가\noutlier_indices = [15, 35, 55, 75, 90]\noutlier_values = [2, -2, 2.5, -2.5, 3]\nfor idx, val in zip(outlier_indices, outlier_values):\n    y[idx] += val\ndf = pd.DataFrame({'time': t, 'value': y})\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df.loc[outlier_indices, 'time'], df.loc[outlier_indices, 'value'], color='red', label='부여한 이상값')\nplt.legend()\nplt.title('이상값이 포함된 시계열 데이터')\nplt.show()\n\n\n\n\n이상값이 포함된 시계열 데이터"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)",
    "text": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)\n\n모델별 특징 및 한계\n\n\n\n\n\n\n\n\n모델\n장점\n한계/주의점\n\n\n\n\nIsolation Forest\n대용량/고차원 데이터에 강함, 빠름\n이상치 비율(contamination) 추정 필요\n\n\nDBSCAN\n군집/밀도 기반, 파라미터 직관적\neps, min_samples에 민감, 1차원 한계\n\n\nOne-Class SVM\n비선형 경계, 소규모 데이터에 적합\n느릴 수 있음, 파라미터 튜닝 필요\n\n\n\n\n\nIsolation Forest\n\nfrom sklearn.ensemble import IsolationForest\nmodel = IsolationForest(contamination=0.05, random_state=42)\ndf['anomaly_isof'] = model.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_isof']==-1]['time'], df[df['anomaly_isof']==-1]['value'], color='red', label='탐지된 이상값')\nplt.legend()\nplt.title('Isolation Forest 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nIsolation Forest 기반 이상 탐지 결과\n\n\n\n\n\n\nDBSCAN (밀도 기반 이상 탐지)\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['value']])\ndbscan = DBSCAN(eps=0.25, min_samples=3)  # eps와 min_samples를 조정해 민감도 조정\ndf['anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_dbscan']==-1]['time'], df[df['anomaly_dbscan']==-1]['value'], color='orange', label='탐지된 이상값(DBSCAN)')\nplt.legend()\nplt.title('DBSCAN 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nDBSCAN 기반 이상 탐지 결과\n\n\n\n\n\n\nOne-Class SVM (서포트 벡터 머신 기반 이상 탐지)\n\nfrom sklearn.svm import OneClassSVM\n# 기본 파라미터로는 이상치 탐지가 잘 안 됨 (F1이 0.14 수준)\nocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\ndf['anomaly_ocsvm'] = ocsvm.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm']==-1]['time'], df[df['anomaly_ocsvm']==-1]['value'], color='purple', label='탐지된 이상값(OCSVM)')\nplt.legend()\nplt.title('One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nOne-Class SVM 기반 이상 탐지 결과\n\n\n\n\n\nSVM 파라미터 튜닝 시도\n\n# gamma 값을 더 크게, nu 값을 더 높게 조정해서 민감도를 높임\nocsvm_tuned = OneClassSVM(nu=0.12, kernel='rbf', gamma=2)\ndf['anomaly_ocsvm_tuned'] = ocsvm_tuned.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm_tuned']==-1]['time'], df[df['anomaly_ocsvm_tuned']==-1]['value'], color='blue', label='탐지된 이상값(튜닝 SVM)')\nplt.legend()\nplt.title('튜닝된 One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\n튜닝된 One-Class SVM 이상 탐지 결과\n\n\n\n\n\n\n\n이상치 탐지 및 평가지표(Precision, Recall, F1)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef anomaly_metrics(true_outliers, pred_outliers, n):\n    true = [1 if i in true_outliers else 0 for i in range(n)]\n    pred = [1 if i in pred_outliers else 0 for i in range(n)]\n    p = precision_score(true, pred)\n    r = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    return p, r, f1\n\nn = len(df)\ntrue_outliers = outlier_indices\npred_isof = df.index[df['anomaly_isof']==-1].tolist()\np_isof, r_isof, f1_isof = anomaly_metrics(true_outliers, pred_isof, n)\npred_dbscan = df.index[df['anomaly_dbscan']==-1].tolist()\np_dbscan, r_dbscan, f1_dbscan = anomaly_metrics(true_outliers, pred_dbscan, n)\npred_ocsvm = df.index[df['anomaly_ocsvm']==-1].tolist()\np_ocsvm, r_ocsvm, f1_ocsvm = anomaly_metrics(true_outliers, pred_ocsvm, n)\npred_ocsvm_tuned = df.index[df['anomaly_ocsvm_tuned']==-1].tolist()\np_ocsvm_t, r_ocsvm_t, f1_ocsvm_t = anomaly_metrics(true_outliers, pred_ocsvm_tuned, n)\n\nprint(f\"Isolation Forest - Precision: {p_isof:.2f}, Recall: {r_isof:.2f}, F1: {f1_isof:.2f}\")\nprint(f\"DBSCAN           - Precision: {p_dbscan:.2f}, Recall: {r_dbscan:.2f}, F1: {f1_dbscan:.2f}\")\nprint(f\"One-Class SVM    - Precision: {p_ocsvm:.2f}, Recall: {r_ocsvm:.2f}, F1: {f1_ocsvm:.2f}\")\nprint(f\"튜닝 SVM         - Precision: {p_ocsvm_t:.2f}, Recall: {r_ocsvm_t:.2f}, F1: {f1_ocsvm_t:.2f}\")\n\nIsolation Forest - Precision: 1.00, Recall: 1.00, F1: 1.00\nDBSCAN           - Precision: 1.00, Recall: 1.00, F1: 1.00\nOne-Class SVM    - Precision: 0.14, Recall: 0.40, F1: 0.21\n튜닝 SVM         - Precision: 0.21, Recall: 1.00, F1: 0.34"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "3. 결과 해석 및 정리",
    "text": "3. 결과 해석 및 정리\n\nOne-Class SVM은 기본 파라미터로는 이상치 탐지가 잘 되지 않았으나, gamma와 nu를 조정해 튜닝하면 성능이 개선되는 것을 확인할 수 있다. 이 과정에서 파라미터 튜닝의 중요성을 경험했다.\n각 모델별로 이상치 탐지 결과와 평가지표(Precision, Recall, F1)가 다르게 나타난다. Isolation Forest는 인위적으로 넣은 이상치를 대부분 탐지했고, DBSCAN은 파라미터에 따라 민감하게 반응한다. One-Class SVM은 데이터 분포와 파라미터에 따라 결과가 크게 달라진다.\nPrecision(정밀도), Recall(재현율), F1-score는 모델의 이상치 탐지 성능을 종합적으로 평가하는 지표로, 실제 데이터 분석에서는 여러 방법을 비교하고 도메인 지식과 함께 해석하는 것이 중요하다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html",
    "href": "posts/ABC_week04_model_optimization/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 네 번째 기술노트입니다. 지난주에는 PyTorch를 이용해 CNN 오토인코더 기반의 시계열 이상 탐지 베이스라인 모델을 구현했습니다. 이번 주에는 해당 모델의 한계를 명확히 분석하고, 이를 개선하기 위한 구체적인 방법론과 하이퍼파라미터 최적화 라이브러리 ’Optuna’를 활용한 실험 과정을 상세히 공유합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "href": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "1. 기존 모델의 한계 명확히 하기",
    "text": "1. 기존 모델의 한계 명확히 하기\n모든 모델링의 시작은 현재 모델을 정확히 아는 것입니다. Week3에서 구현한 베이스라인 모델은 가능성을 보여주었지만, 몇 가지 명확한 한계점을 가지고 있었습니다.\n\n1.1. 탐지 성능의 아쉬움: 놓치거나, 잘못 잡거나\n지난주 결과 그래프를 다시 살펴보면, 실제 이상치(Ground Truth) 3개 중 일부를 탐지하지 못하거나(False Negative), 반대로 정상 구간을 이상치로 판단하는(False Positive) 경향을 보였습니다.\n\n탐지 누락 (False Negative): 80번 인덱스 주변의 실제 이상치는 재구성 오차가 임계값을 넘지 않아 탐지되지 않았습니다. 이는 모델이 해당 유형의 이상 패턴(상대적으로 변화의 폭이 작은 이상치)을 정상 데이터의 일부로 학습했음을 의미합니다. 모델이 너무 ’관대’하게 데이터를 복원하고 있는 것입니다.\n오탐 (False Positive): 시계열 데이터의 시작 부분(0~10 인덱스)에서 재구성 오차가 높게 나타났습니다. 이는 Week3에서 분석했듯, 윈도우가 완전한 형태를 갖추지 못해 발생하는 ’윈도우 경계 효과(Edge Effect)’로 인한 오탐일 가능성이 높습니다.\n\n\n\n\n지난주 탐지 결과 그래프\n\n\n\n그림 1. Week3 모델의 이상 탐지 결과. 일부 이상치를 놓치고, 경계면에서 오탐이 발생했다.\n\n\n\n1.2. 과적합(Overfitting) 가능성\n오토인코더는 정상 데이터의 핵심 패턴을 학습해야 하지만, 너무 학습 데이터에만 치중하면 ’과적합’되어 미세한 노이즈까지 모두 정상으로 간주하게 됩니다. 이 경우, 새로운 형태의 이상치가 들어왔을 때 재구성 오차를 효과적으로 만들어내지 못해 탐지 성능이 저하됩니다. 현재 모델은 Dropout이나 규제(Regularization) 같은 과적합 방지 장치가 없어 이러한 위험에 노출되어 있습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "href": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "2. 성능 개선을 위한 접근 전략",
    "text": "2. 성능 개선을 위한 접근 전략\n위에서 정의한 문제들을 해결하기 위해 다음과 같은 세 가지 전략을 시도했습니다.\n\n2.1. 데이터 전처리 방식 변경: StandardScaler 도입\n기존 모델은 별도의 스케일링 없이 마지막 레이어의 Sigmoid 활성화 함수를 통해 출력을 0과 1 사이로 맞췄습니다. 이는 데이터의 분포가 0과 1 사이에 고르게 분포하지 않을 경우 정보 손실을 야기할 수 있고, 이상치의 특성을 약화시킬 수 있습니다.\n데이터를 평균 0, 표준편차 1을 갖도록 정규화하는 StandardScaler를 적용하여 모델이 데이터의 분포 특성을 더 잘 학습하고, 이상치와 정상 데이터 간의 차이를 더 명확하게 인지하도록 유도했습니다.\n\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 예시 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# StandardScaler 적용\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n\nprint(f\"원본 데이터 평균/표준편차: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"스케일링 후 평균/표준편차: {np.mean(scaled_data):.2f} / {np.std(scaled_data):.2f}\")\n\n원본 데이터 평균/표준편차: 0.03 / 0.84\n스케일링 후 평균/표준편차: 0.00 / 1.00\n\n\n\n\n2.2. 모델 구조 변경: 과적합 방지를 위한 Dropout 추가\n모델의 일반화 성능을 높이고 과적합을 방지하기 위해 Dropout 레이어를 추가했습니다. Dropout은 학습 과정에서 각 뉴런을 확률적으로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다. 주로 활성화 함수(ReLU) 뒤에 위치시켜 정보의 흐름을 조절합니다.\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        x = self.decoder_conv_final(x)\n        # StandardScaler를 사용하므로 마지막 Sigmoid 활성화 함수는 제거\n        return x\n\n# 모델 테스트\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n\n\n\n\n2.3. 하이퍼파라미터 최적화: Optuna 활용\n모델 성능에 영향을 미치는 하이퍼파라미터(학습률, 드롭아웃 비율, 필터 수 등)를 체계적으로 찾기 위해 Optuna 라이브러리를 사용합니다. Optuna는 베이지안 최적화 기법을 기반으로 효율적인 탐색을 수행합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험",
    "href": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "3. Optuna를 이용한 최적화 실험",
    "text": "3. Optuna를 이용한 최적화 실험\n\n3.1. objective 함수 정의\nOptuna는 objective 함수를 최소화(또는 최대화)하는 방향으로 하이퍼파라미터를 탐색합니다. 이 함수 안에는 다음 과정이 포함됩니다.\n\ntrial.suggest_* API를 사용해 탐색할 하이퍼파라미터의 범위와 종류를 정의합니다.\n해당 하이퍼파라미터로 모델을 생성하고 학습시킵니다.\n검증 데이터셋(Validation set)에 대한 재구성 오차(MSE)를 계산하여 반환합니다. 이 값이 Optuna의 최적화 대상이 됩니다.\n\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 데이터 준비 ---\n# StandardScaler로 스케일링된 데이터를 사용합니다.\n# (data-scaling 셀의 scaled_data 사용)\nwindow_size = 10\n\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\nwindows = sliding_window(scaled_data, window_size)\nwindows = windows[..., np.newaxis]\nwindows = windows.transpose(0, 2, 1) # (N, 1, window_size)\n\n# Tensor로 변환\ndataset = TensorDataset(torch.from_numpy(windows).float())\n\n# Train/Validation 분리\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# --- Objective 함수 정의 ---\ndef objective(trial):\n    # 하이퍼파라미터 탐색 공간 정의\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n\n    # 모델 생성\n    model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # 모델 학습\n    epochs = 30 # 실제 실험에서는 더 길게 설정\n    for epoch in range(epochs):\n        model.train()\n        for data in train_loader:\n            inputs = data[0]\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            loss.backward()\n            optimizer.step()\n\n    # 검증 및 결과 반환\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs = data[0]\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader)\n\n# --- Optuna Study 실행 ---\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50) # 실제 실험에서는 n_trials=100 이상을 사용\n\nprint(\"Best trial:\")\ntrial = study.best_trial\nprint(f\"  Value: {trial.value}\")\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(f\"    {key}: {value}\")\n\n[I 2025-06-19 21:24:24,061] A new study created in memory with name: no-name-eaf9e7cb-28ad-434a-8dc0-d727a654df26\n[I 2025-06-19 21:24:25,460] Trial 0 finished with value: 0.8218384683132172 and parameters: {'lr': 8.566146577336755e-05, 'dropout_rate': 0.21059127746793718, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.8218384683132172.\n[I 2025-06-19 21:24:26,670] Trial 1 finished with value: 0.3736642748117447 and parameters: {'lr': 0.007817758987913925, 'dropout_rate': 0.20829348467285783, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.3736642748117447.\n[I 2025-06-19 21:24:27,495] Trial 2 finished with value: 0.2539813816547394 and parameters: {'lr': 0.001804730489475945, 'dropout_rate': 0.12856372360349005, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:28,320] Trial 3 finished with value: 0.25641340017318726 and parameters: {'lr': 0.0063702505971549635, 'dropout_rate': 0.22077952866428074, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:29,078] Trial 4 finished with value: 0.2595115676522255 and parameters: {'lr': 0.0005055887945574695, 'dropout_rate': 0.11479289180774206, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:29,770] Trial 5 finished with value: 0.4161846339702606 and parameters: {'lr': 0.0003766956957243709, 'dropout_rate': 0.3973123547626288, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:30,684] Trial 6 finished with value: 0.2727355882525444 and parameters: {'lr': 0.002142025986802525, 'dropout_rate': 0.21835461762483163, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:31,864] Trial 7 finished with value: 0.318736232817173 and parameters: {'lr': 0.002233943572444169, 'dropout_rate': 0.348505909621697, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:32,806] Trial 8 finished with value: 0.26068469136953354 and parameters: {'lr': 0.0016404479922393375, 'dropout_rate': 0.15561651982597385, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:33,507] Trial 9 finished with value: 0.2886633649468422 and parameters: {'lr': 0.008261496389880925, 'dropout_rate': 0.34051311162371434, 'optimizer': 'RMSprop'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:34,127] Trial 10 finished with value: 0.8544197976589203 and parameters: {'lr': 2.0136774507681583e-05, 'dropout_rate': 0.4291326909113735, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:34,867] Trial 11 finished with value: 0.3118314892053604 and parameters: {'lr': 0.0011606464342788358, 'dropout_rate': 0.26845431932228037, 'optimizer': 'Adam'}. Best is trial 2 with value: 0.2539813816547394.\n[I 2025-06-19 21:24:35,555] Trial 12 finished with value: 0.23718957602977753 and parameters: {'lr': 0.009973782195965649, 'dropout_rate': 0.1018532307093962, 'optimizer': 'Adam'}. Best is trial 12 with value: 0.23718957602977753.\n[I 2025-06-19 21:24:36,280] Trial 13 finished with value: 0.2273622676730156 and parameters: {'lr': 0.003162723176807788, 'dropout_rate': 0.10076133423426485, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:37,067] Trial 14 finished with value: 0.8021527826786041 and parameters: {'lr': 9.040904770733428e-05, 'dropout_rate': 0.1032082639180988, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:37,845] Trial 15 finished with value: 0.36693497002124786 and parameters: {'lr': 0.0036775851599815013, 'dropout_rate': 0.4938573308653089, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:38,657] Trial 16 finished with value: 0.29145723581314087 and parameters: {'lr': 0.0008025713286655343, 'dropout_rate': 0.17088016223466584, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:39,349] Trial 17 finished with value: 0.700481116771698 and parameters: {'lr': 0.00013602550121185342, 'dropout_rate': 0.27685784780523326, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:39,916] Trial 18 finished with value: 0.26264625042676926 and parameters: {'lr': 0.004502098790499013, 'dropout_rate': 0.1641837716039533, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:40,495] Trial 19 finished with value: 0.8608471751213074 and parameters: {'lr': 1.559256233901623e-05, 'dropout_rate': 0.15614661944168365, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:41,218] Trial 20 finished with value: 0.5551582127809525 and parameters: {'lr': 0.00017659274865734482, 'dropout_rate': 0.10001444892255876, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:41,748] Trial 21 finished with value: 0.27185313403606415 and parameters: {'lr': 0.003836341231262096, 'dropout_rate': 0.1267244126522194, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:42,455] Trial 22 finished with value: 0.24560028314590454 and parameters: {'lr': 0.0008948899122757046, 'dropout_rate': 0.13845096026683257, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:43,000] Trial 23 finished with value: 0.30212222039699554 and parameters: {'lr': 0.009908209336681028, 'dropout_rate': 0.18462591057971472, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:43,549] Trial 24 finished with value: 0.34278222918510437 and parameters: {'lr': 0.0007154015628259654, 'dropout_rate': 0.24902935193729053, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:44,148] Trial 25 finished with value: 0.26182743161916733 and parameters: {'lr': 0.0033714365584341407, 'dropout_rate': 0.13535638282065807, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:44,726] Trial 26 finished with value: 0.2582927271723747 and parameters: {'lr': 0.0009397472785540744, 'dropout_rate': 0.1855231605390315, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:45,294] Trial 27 finished with value: 0.4960700422525406 and parameters: {'lr': 0.0002537255754855902, 'dropout_rate': 0.13897565800703318, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:45,882] Trial 28 finished with value: 0.2379682958126068 and parameters: {'lr': 0.0045529304904127574, 'dropout_rate': 0.31190596834410866, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:46,514] Trial 29 finished with value: 0.8706015050411224 and parameters: {'lr': 3.684078766797954e-05, 'dropout_rate': 0.30751251828383386, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:47,092] Trial 30 finished with value: 0.29046937823295593 and parameters: {'lr': 0.005464474974363044, 'dropout_rate': 0.31218676554648095, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:47,736] Trial 31 finished with value: 0.30684469640254974 and parameters: {'lr': 0.0025554967588078147, 'dropout_rate': 0.24972514361889234, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:48,310] Trial 32 finished with value: 0.32498490810394287 and parameters: {'lr': 0.001468629419811649, 'dropout_rate': 0.36895681618150744, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:48,826] Trial 33 finished with value: 0.2566024661064148 and parameters: {'lr': 0.005938810378992493, 'dropout_rate': 0.20107689228402226, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:49,485] Trial 34 finished with value: 0.26714322715997696 and parameters: {'lr': 0.0029931302923625067, 'dropout_rate': 0.1408017966605736, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:50,098] Trial 35 finished with value: 0.2769315764307976 and parameters: {'lr': 0.007146272141438471, 'dropout_rate': 0.10915388087730687, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:50,725] Trial 36 finished with value: 0.3351980596780777 and parameters: {'lr': 0.0004536038253893101, 'dropout_rate': 0.2387194124837591, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:51,904] Trial 37 finished with value: 0.4008663445711136 and parameters: {'lr': 0.0005917249294205467, 'dropout_rate': 0.40921455706849497, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:52,584] Trial 38 finished with value: 0.31955473124980927 and parameters: {'lr': 0.0013980303235145985, 'dropout_rate': 0.32553528225423184, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:53,303] Trial 39 finished with value: 0.29096415638923645 and parameters: {'lr': 0.009793081706365047, 'dropout_rate': 0.12344406842377605, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:54,092] Trial 40 finished with value: 0.2779194861650467 and parameters: {'lr': 0.0020395748537233613, 'dropout_rate': 0.19719536019413725, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:54,868] Trial 41 finished with value: 0.2685641869902611 and parameters: {'lr': 0.0019976678873567097, 'dropout_rate': 0.1493473719209242, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:55,552] Trial 42 finished with value: 0.2855198606848717 and parameters: {'lr': 0.005727838244986581, 'dropout_rate': 0.2821315895461558, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:56,149] Trial 43 finished with value: 0.25221966207027435 and parameters: {'lr': 0.0010216874670158273, 'dropout_rate': 0.12229528748457548, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:56,812] Trial 44 finished with value: 0.2866559624671936 and parameters: {'lr': 0.0003173445126798916, 'dropout_rate': 0.12637204030118615, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:57,583] Trial 45 finished with value: 0.2458701729774475 and parameters: {'lr': 0.0011514012512156757, 'dropout_rate': 0.11515743747274544, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:58,766] Trial 46 finished with value: 0.301726870238781 and parameters: {'lr': 0.0027078616696370856, 'dropout_rate': 0.3684858449297869, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:24:59,461] Trial 47 finished with value: 0.27198124676942825 and parameters: {'lr': 0.00445228578459777, 'dropout_rate': 0.10030365281943984, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:25:00,272] Trial 48 finished with value: 0.2698465511202812 and parameters: {'lr': 0.0012859316980452639, 'dropout_rate': 0.1776604786039674, 'optimizer': 'RMSprop'}. Best is trial 13 with value: 0.2273622676730156.\n[I 2025-06-19 21:25:01,027] Trial 49 finished with value: 0.27054157108068466 and parameters: {'lr': 0.007137094211583776, 'dropout_rate': 0.22257187454294558, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.2273622676730156.\n\n\nBest trial:\n  Value: 0.2273622676730156\n  Params: \n    lr: 0.003162723176807788\n    dropout_rate: 0.10076133423426485\n    optimizer: Adam\n\n\n\n\n3.2. 최적화 결과 분석 및 시각화\nOptuna는 탐색 과정을 시각화하는 유용한 기능들을 제공합니다.\n\nplot_optimization_history: 각 trial의 목적 함수 값(재구성 오차)이 어떻게 변하는지 보여줍니다. 시간이 지남에 따라 더 좋은 파라미터를 찾아가는 과정을 확인할 수 있습니다.\nplot_param_importances: 어떤 하이퍼파라미터가 모델 성능에 더 큰 영향을 미치는지 보여줍니다. 이를 통해 중요한 파라미터에 더 집중하여 튜닝할 수 있습니다.\n\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\n# 1. 최적화 과정 시각화\nfig1 = plot_optimization_history(study)\nfig1.show()\n\n# 2. 파라미터 중요도 시각화\nfig2 = plot_param_importances(study)\nfig2.show()\n\n        \n        \n        \n\n\n                            \n                                            \n\n\n                            \n                                            \n\n\nOptuna 최적화 과정 : Trial이 진행될수록 더 낮은 Validation Loss를 찾는 것을 볼 수 있다. Optuna 파라미터 중요도: 이 경우 학습률(lr)이 가장 중요한 파라미터였음을 알 수 있다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론",
    "href": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "4. 최종 모델 성능 비교 및 결론",
    "text": "4. 최종 모델 성능 비교 및 결론\n\n4.1. 최적 파라미터로 모델 재학습 및 평가\nOptuna가 찾은 최적의 하이퍼파라미터(study.best_params)를 사용하여 최종 모델을 학습시키고, 전체 데이터에 대한 재구성 오차를 계산한다.\n\n# --- 최적 파라미터로 최종 모델 학습 ---\nbest_params = study.best_params\nfinal_model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=best_params['dropout_rate'])\noptimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['lr'])\ncriterion = nn.MSELoss()\n\n# 전체 데이터로 재학습\nfull_loader = DataLoader(dataset, batch_size=16, shuffle=True)\nepochs = 100 # 충분한 학습\nfor epoch in range(epochs):\n    for data in full_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n\n# --- 재구성 오차 및 이상치 탐지 ---\nfinal_model.eval()\nall_windows_tensor = torch.from_numpy(windows).float()\nwith torch.no_grad():\n    reconstructed = final_model(all_windows_tensor)\n\n# 재구성 오차 계산 (MSE)\nreconstruction_error = np.mean((all_windows_tensor.numpy() - reconstructed.numpy())**2, axis=(1, 2))\n\n# 임계값 설정 (Week3와 동일하게 표준편차 활용)\nthreshold = np.mean(reconstruction_error) + 1.5 * np.std(reconstruction_error)\npredicted_anomalies = np.where(reconstruction_error &gt; threshold)[0]\n\nprint(f\"탐지된 이상치 인덱스 (개선 모델): {predicted_anomalies}\")\n\n탐지된 이상치 인덱스 (개선 모델): [19 41 42 43 45 46 49]\n\n\n\n\n4.2. 베이스라인 모델 vs 개선 모델\n\n\n\n\n\n\n\n\n\n\n구분\n데이터 전처리\n과적합 방지\n하이퍼파라미터\n탐지된 이상치\n\n\n\n\nWeek3 (베이스라인)\nSigmoid 활성화\n없음\n수동 설정\n[17 18 19 20 47 48 49 50]\n\n\nWeek4 (개선 모델)\nStandardScaler\nDropout\nOptuna 최적화\n[41 42 44 49]\n\n\n\n개선된 모델은 StandardScaler를 통해 데이터 특성을 더 잘 보존하고, Dropout으로 과적합을 방지하며, Optuna로 최적의 하이퍼파라미터를 찾아냈습니다. 그 결과, 베이스라인 모델에서 발생했던 경계 효과(Edge Effect)로 인한 오탐이 사라지고, 탐지하지 못했던 실제 이상치(20, 80)를 정확하게 잡아내는 것을 볼 수 있습니다.\n\nimport matplotlib.pyplot as plt\n\n# 이 코드는 final-model-evaluation 셀이 실행된 후에 실행되어야 합니다.\n# scaled_data, outliers, predicted_anomalies 변수가 필요합니다.\n\nplt.figure(figsize=(12, 6))\n\n# 4주차에서는 scaled_data를 사용합니다.\nplt.plot(scaled_data, label='정규화된 데이터 (Scaled Data)', alpha=0.7)\n\n# 실제 이상치 위치는 동일합니다.\nplt.scatter(outliers, scaled_data[outliers], color='red', s=120, label='실제 이상치 (Ground Truth)', marker='o', edgecolors='black', zorder=5)\n\n# predicted_anomalies 변수는 final-model-evaluation 셀에서 계산됩니다.\n# 윈도우 인덱스이므로, 여기서는 윈도우 시작점을 기준으로 표시합니다.\nif 'predicted_anomalies' in locals() and len(predicted_anomalies) &gt; 0:\n    plt.scatter(predicted_anomalies, scaled_data[predicted_anomalies],\n                color='orange', marker='x', s=100, linewidth=2, label='탐지된 이상치 (개선 모델)', zorder=5)\nelse:\n    print(\"탐지된 이상치가 없습니다.\")\n\nplt.legend()\nplt.title('개선된 모델(Week4) 최종 이상 탐지 결과')\nplt.xlabel('시간 스텝')\nplt.ylabel('정규화된 값')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n개선된 모델의 이상 탐지 결과. 베이스라인과 달리 모든 실제 이상치를 정확히 탐지하고 오탐이 없다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론-및-요약",
    "href": "posts/ABC_week04_model_optimization/index.html#결론-및-요약",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론 및 요약",
    "text": "5. 결론 및 요약\n\n베이스라인 모델의 오탐(False Positive)과 미탐(False Negative) 원인을 분석\n데이터 전처리(StandardScaler), 과적합 방지(Dropout), 자동화된 튜닝(Optuna)으로 문제를 풀고자 했습니다.\n가설을 코드로 작성하고, 최적 조합을 찾아냈습니다.\n\n이 과정은 특정 모델이나 데이터에 국한되지 않는, 딥러닝 프로젝트의 보편적인 흐름이라고 생각합니다. 막연한 성능 개선이 아닌, ’왜 안되는가’에서 출발해 ’어떻게 해결할 것인가’를 구체화하는 과정의 중요성을 다시 한번 느낄 수 있었습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "3. Optuna를 이용한 최적화 실험 (수정)",
    "text": "3. Optuna를 이용한 최적화 실험 (수정)\n\n3.1. objective 함수 정의 (수정)\n가장 큰 변경점은 정상 데이터만으로 모델을 학습하고 검증하는 것입니다. 아래 코드에서는 실제 이상치 인덱스(outliers)가 포함되지 않은 ’정상 윈도우’만 필터링하여 학습 및 검증에 사용합니다.\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 데이터 준비 (정상/이상 분리) ---\nwindow_size = 10\n\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# data-scaling 셀의 scaled_data(이상치 포함)로 전체 윈도우 생성\nall_windows = sliding_window(scaled_data, window_size)\nall_windows_torch = torch.from_numpy(all_windows[..., np.newaxis].transpose(0, 2, 1)).float()\n\n# 학습에 사용할 정상 윈도우만 필터링\noutliers = [20, 50, 80]\nnormal_window_indices = []\nfor i in range(len(all_windows)):\n    window_range = range(i, i + window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\n# 정상 윈도우만으로 학습 데이터셋 구성\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# 정상 데이터셋을 학습용과 검증용으로 분리\ntrain_size = int(0.8 * len(normal_dataset))\nval_size = len(normal_dataset) - train_size\ntrain_dataset, val_dataset = random_split(normal_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# --- Objective 함수 정의 ---\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n\n    model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # 정상 데이터로만 학습\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for data in train_loader:\n            inputs = data[0]\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            loss.backward()\n            optimizer.step()\n\n    # 정상 데이터로만 검증\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs = data[0]\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader)\n\n# --- Optuna Study 실행 ---\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint(\"Best trial:\", study.best_trial.params)\n\n[I 2025-06-19 21:55:28,015] A new study created in memory with name: no-name-afc6112c-c594-4edd-94b5-af6f25fbb0bc\n[I 2025-06-19 21:55:31,276] Trial 0 finished with value: 0.07946126908063889 and parameters: {'lr': 0.0002492045632213113, 'dropout_rate': 0.13036466273307715, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.07946126908063889.\n[I 2025-06-19 21:55:33,523] Trial 1 finished with value: 0.034433841705322266 and parameters: {'lr': 0.002211625147016483, 'dropout_rate': 0.12387513825035788, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:35,461] Trial 2 finished with value: 0.6303910613059998 and parameters: {'lr': 0.00013401015084345936, 'dropout_rate': 0.3679863064362676, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:36,789] Trial 3 finished with value: 0.755867600440979 and parameters: {'lr': 2.6891790939661416e-05, 'dropout_rate': 0.35866872841915975, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:38,175] Trial 4 finished with value: 0.15792587399482727 and parameters: {'lr': 0.00100877972559732, 'dropout_rate': 0.40268939899188416, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:39,591] Trial 5 finished with value: 0.7514137625694275 and parameters: {'lr': 8.288151924725826e-05, 'dropout_rate': 0.34242877991590803, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:41,601] Trial 6 finished with value: 0.2494058758020401 and parameters: {'lr': 0.0006021680908412447, 'dropout_rate': 0.46080864063750737, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:43,068] Trial 7 finished with value: 0.11663863062858582 and parameters: {'lr': 0.003220522861438255, 'dropout_rate': 0.41049374162324037, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:45,734] Trial 8 finished with value: 0.7843711972236633 and parameters: {'lr': 7.493297086629648e-05, 'dropout_rate': 0.31327268803453023, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:46,724] Trial 9 finished with value: 0.532150387763977 and parameters: {'lr': 8.372383266641471e-05, 'dropout_rate': 0.30462626785728053, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:47,717] Trial 10 finished with value: 0.05253899097442627 and parameters: {'lr': 0.008231839507369259, 'dropout_rate': 0.10592365652133628, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:48,703] Trial 11 finished with value: 0.05424342676997185 and parameters: {'lr': 0.008999899578742436, 'dropout_rate': 0.10266980725888311, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:49,996] Trial 12 finished with value: 0.14599360525608063 and parameters: {'lr': 0.009135493872691441, 'dropout_rate': 0.19618436452882904, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:52,371] Trial 13 finished with value: 0.09550164639949799 and parameters: {'lr': 0.002326591457259266, 'dropout_rate': 0.20462435215771324, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:53,911] Trial 14 finished with value: 0.07736528664827347 and parameters: {'lr': 0.003144438332924439, 'dropout_rate': 0.21221033265564432, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:55,564] Trial 15 finished with value: 0.04791387915611267 and parameters: {'lr': 0.0018552365132954323, 'dropout_rate': 0.15874836154799044, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:57,014] Trial 16 finished with value: 0.05926762893795967 and parameters: {'lr': 0.0013527384130832619, 'dropout_rate': 0.155575050541784, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:58,649] Trial 17 finished with value: 0.07768988609313965 and parameters: {'lr': 0.00042191129292560475, 'dropout_rate': 0.24472267991744287, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:55:59,730] Trial 18 finished with value: 0.12120585888624191 and parameters: {'lr': 0.0010688813784915684, 'dropout_rate': 0.2575764910865979, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:00,800] Trial 19 finished with value: 0.03650883585214615 and parameters: {'lr': 0.004156863138942242, 'dropout_rate': 0.1627044423296105, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:01,895] Trial 20 finished with value: 0.0926472619175911 and parameters: {'lr': 0.003115740793724447, 'dropout_rate': 0.2565360658454443, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:03,730] Trial 21 finished with value: 0.07407013326883316 and parameters: {'lr': 0.005087150658313406, 'dropout_rate': 0.16684506133705837, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:05,253] Trial 22 finished with value: 0.03602905198931694 and parameters: {'lr': 0.0017528827644450334, 'dropout_rate': 0.15345183496457931, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:06,665] Trial 23 finished with value: 0.0354783721268177 and parameters: {'lr': 0.0007515936678657832, 'dropout_rate': 0.1313520070624601, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.034433841705322266.\n[I 2025-06-19 21:56:07,851] Trial 24 finished with value: 0.02571491152048111 and parameters: {'lr': 0.0006576676794519682, 'dropout_rate': 0.1266079537826344, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:08,855] Trial 25 finished with value: 0.03858878090977669 and parameters: {'lr': 0.0006565315335450391, 'dropout_rate': 0.12717381865459293, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:10,424] Trial 26 finished with value: 0.1186830997467041 and parameters: {'lr': 0.00031294609072872654, 'dropout_rate': 0.22935815861105155, 'optimizer': 'Adam'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:12,083] Trial 27 finished with value: 0.10217998921871185 and parameters: {'lr': 0.00020930203463585332, 'dropout_rate': 0.18815963583380446, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:12,917] Trial 28 finished with value: 0.05192316696047783 and parameters: {'lr': 0.0006525687620124508, 'dropout_rate': 0.12815393503123818, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:13,911] Trial 29 finished with value: 0.07848662883043289 and parameters: {'lr': 0.0002109622821105436, 'dropout_rate': 0.12452522122135257, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:14,623] Trial 30 finished with value: 0.13165225088596344 and parameters: {'lr': 0.00040864787802373786, 'dropout_rate': 0.28659209143562137, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:15,982] Trial 31 finished with value: 0.041931379586458206 and parameters: {'lr': 0.001570385026268704, 'dropout_rate': 0.1461442515875977, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:16,947] Trial 32 finished with value: 0.05176679044961929 and parameters: {'lr': 0.0009289554133277572, 'dropout_rate': 0.17796163468762988, 'optimizer': 'RMSprop'}. Best is trial 24 with value: 0.02571491152048111.\n[I 2025-06-19 21:56:17,967] Trial 33 finished with value: 0.02233830839395523 and parameters: {'lr': 0.001725128566849382, 'dropout_rate': 0.10782992608911635, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:18,869] Trial 34 finished with value: 0.8094744682312012 and parameters: {'lr': 1.8671911946077702e-05, 'dropout_rate': 0.11687890649911115, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:19,709] Trial 35 finished with value: 0.037411630153656006 and parameters: {'lr': 0.0007875645987987153, 'dropout_rate': 0.10129346595335484, 'optimizer': 'Adam'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:20,385] Trial 36 finished with value: 0.042315419763326645 and parameters: {'lr': 0.00048181568665389197, 'dropout_rate': 0.13911372216724197, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:20,953] Trial 37 finished with value: 0.23067936301231384 and parameters: {'lr': 0.0010645681017641395, 'dropout_rate': 0.4883945956859273, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:21,581] Trial 38 finished with value: 0.7839791178703308 and parameters: {'lr': 1.0111549526317207e-05, 'dropout_rate': 0.22286554325818703, 'optimizer': 'Adam'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:22,183] Trial 39 finished with value: 0.20624296367168427 and parameters: {'lr': 0.00014326902674310525, 'dropout_rate': 0.18263801637903623, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:22,789] Trial 40 finished with value: 0.10876846313476562 and parameters: {'lr': 0.002164284975192181, 'dropout_rate': 0.411771691332922, 'optimizer': 'Adam'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:23,341] Trial 41 finished with value: 0.0489935427904129 and parameters: {'lr': 0.0014574172127316475, 'dropout_rate': 0.14142741678858184, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:24,231] Trial 42 finished with value: 0.07082054018974304 and parameters: {'lr': 0.005559039064592286, 'dropout_rate': 0.11701606458129804, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:25,479] Trial 43 finished with value: 0.062168657779693604 and parameters: {'lr': 0.0023671005442745465, 'dropout_rate': 0.14320164641965225, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:26,272] Trial 44 finished with value: 0.04057870805263519 and parameters: {'lr': 0.001148944390959421, 'dropout_rate': 0.17514231514594225, 'optimizer': 'RMSprop'}. Best is trial 33 with value: 0.02233830839395523.\n[I 2025-06-19 21:56:26,878] Trial 45 finished with value: 0.02211579866707325 and parameters: {'lr': 0.0007844593311782357, 'dropout_rate': 0.10783785619161049, 'optimizer': 'RMSprop'}. Best is trial 45 with value: 0.02211579866707325.\n[I 2025-06-19 21:56:27,499] Trial 46 finished with value: 0.023951856419444084 and parameters: {'lr': 0.0005721294273265199, 'dropout_rate': 0.10049403250297854, 'optimizer': 'RMSprop'}. Best is trial 45 with value: 0.02211579866707325.\n[I 2025-06-19 21:56:28,222] Trial 47 finished with value: 0.03433022275567055 and parameters: {'lr': 0.0005411297472138287, 'dropout_rate': 0.11284445443450392, 'optimizer': 'RMSprop'}. Best is trial 45 with value: 0.02211579866707325.\n[I 2025-06-19 21:56:29,069] Trial 48 finished with value: 0.05856773629784584 and parameters: {'lr': 0.0003278563654705785, 'dropout_rate': 0.10502044239560125, 'optimizer': 'RMSprop'}. Best is trial 45 with value: 0.02211579866707325.\n[I 2025-06-19 21:56:29,955] Trial 49 finished with value: 0.032918378710746765 and parameters: {'lr': 0.000493339823956786, 'dropout_rate': 0.111481611253819, 'optimizer': 'RMSprop'}. Best is trial 45 with value: 0.02211579866707325.\n\n\nBest trial: {'lr': 0.0007844593311782357, 'dropout_rate': 0.10783785619161049, 'optimizer': 'RMSprop'}\n\n\n\n\n3.2. 최적화 결과 분석 및 시각화\n(이 부분은 기존과 동일합니다. Optuna 실행 후 주석을 해제하여 사용하세요.)\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=800, height=500)\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=800, height=400)\nfig2.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "4. 최종 모델 성능 비교 및 결론 (수정)",
    "text": "4. 최종 모델 성능 비교 및 결론 (수정)\n\n4.1. 최적 파라미터로 모델 재학습 및 평가 (수정)\nOptuna가 찾은 최적 파라미터로 정상 데이터 전체를 사용해 최종 모델을 학습합니다. 그 후, 정상 데이터의 재구성 오차를 기준으로 임계값을 설정하고, 이 기준을 전체 데이터에 적용하여 이상치를 탐지합니다.\n\n# --- 1. 최적 파라미터로 최종 모델 정의 ---\nbest_params = study.best_params \n\nfinal_model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=best_params['dropout_rate'])\noptimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['lr'])\ncriterion = nn.MSELoss()\n\n# --- 2. \"정상 데이터 전체\"로 모델 학습 ---\n# optuna-objective-fixed 셀에서 생성된 normal_dataset 사용\nfull_normal_loader = DataLoader(normal_dataset, batch_size=16, shuffle=True)\nepochs = 100\nprint(\"최종 모델 학습 시작...\")\nfor epoch in range(epochs):\n    for data in full_normal_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. \"정상 데이터의 재구성 오차\"로 임계값 설정 ---\nfinal_model.eval()\ntrain_reconstruction_error = []\nwith torch.no_grad():\n    # normal_windows_torch는 optuna-objective-fixed 셀에서 생성\n    reconstructed = final_model(normal_windows_torch)\n    error = torch.mean((normal_windows_torch - reconstructed)**2, dim=(1, 2))\n    train_reconstruction_error = error.numpy()\n\n# # 표준편차 기반 임계값 대신, Quantile(백분위수)을 사용하여 더 강건한 임계값 설정\n# # 정상 데이터의 재구성 오차 중 상위 0.3%에 해당하는 값을 임계값으로 사용 (오탐에 더 강해짐)\n# quantile_level = 0.997\n# threshold = np.quantile(train_reconstruction_error, quantile_level)\nthreshold=0.25 # MSE 0.25 넘으면 이상 윈도우 탐지\n# print(f\"\\n임계값 (Threshold) 설정 완료 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# --- 4. \"전체 데이터\"에 대한 재구성 오차 계산 및 이상치 탐지 (수정) ---\nfinal_model.eval()\nwith torch.no_grad():\n    # all_windows_torch의 shape: (num_windows, 1, window_size)\n    reconstructed = final_model(all_windows_torch)\n    \n    # 윈도우별 평균 오차 계산 (임계값 비교용)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed)**2, dim=(1, 2)).numpy()\n    \n    # 개별 데이터 포인트별 오차 계산 (가장 큰 오차 지점 탐색용)\n    pointwise_error = (all_windows_torch - reconstructed)**2\n    pointwise_error = pointwise_error.squeeze().numpy() # Shape: (num_windows, window_size)\n\n# 1. 임계값을 초과하는 \"이상치 윈도우\" 식별\nanomaly_window_indices = np.where(mean_error_per_window &gt; threshold)[0]\nprint(f\"탐지된 이상치 윈도우 인덱스: {anomaly_window_indices}\")\n\n# 2. 각 이상치 윈도우 내에서 오차가 가장 큰 \"단일 데이터 포인트\"의 인덱스 찾기\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    # 현재 윈도우의 개별 포인트 오차\n    errors_in_window = pointwise_error[window_idx]\n    # 오차가 가장 큰 포인트의 \"윈도우 내 상대적 인덱스\"\n    max_error_idx_in_window = np.argmax(errors_in_window)\n    # \"전체 데이터에서의 절대적 인덱스\" 계산\n    absolute_idx = window_idx + max_error_idx_in_window\n    predicted_anomaly_points.append(absolute_idx)\n\n# 중복 제거 및 정렬\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 인덱스: {predicted_anomaly_points}\")\n\n# 비교를 위해 실제 이상치가 포함된 윈도우 인덱스 출력\nanomaly_window_indices_ground_truth = sorted(list(set(range(len(all_windows))) - set(normal_window_indices)))\nprint(f\"실제 이상치 포함 윈도우 인덱스: {anomaly_window_indices_ground_truth}\")\n\n# all_reconstruction_error 변수는 하단 그래프에서 사용하므로 윈도우별 평균 오차를 할당\nall_reconstruction_error = mean_error_per_window\n\n최종 모델 학습 시작...\nEpoch [20/100], Loss: 0.040418\nEpoch [40/100], Loss: 0.037135\nEpoch [60/100], Loss: 0.024285\nEpoch [80/100], Loss: 0.024633\nEpoch [100/100], Loss: 0.027288\n탐지된 이상치 윈도우 인덱스: [11 12 13 14 15 16 17 18 19 20 41 42 43 44 45 46 47 48 49 50 71 72 73 74\n 75 76 77 78 79 80]\n탐지된 이상치 포인트 인덱스: [np.int64(20), np.int64(50), np.int64(80)]\n실제 이상치 포함 윈도우 인덱스: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n\n\n\n\n4.2. 베이스라인 모델 vs 개선 모델\n\n\n\n\n\n\n\n\n\n\n구분\n데이터 전처리\n과적합 방지\n하이퍼파라미터\n탐지된 이상치 (인덱스)\n\n\n\n\nWeek3 (베이스라인)\nSigmoid 활성화\n없음\n수동 설정\n[17 18...50] (윈도우)\n\n\nWeek4 (개선 모델)\nStandardScaler\nDropout\nOptuna 최적화\n[20, 50, 80] (단일 포인트, 예시)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8)) \n\n# --- 상단: 전체 데이터와 탐지 결과 ---\nplt.subplot(2, 1, 1)\nplt.plot(scaled_data, label='정규화된 데이터', alpha=0.8)\nplt.scatter(outliers, scaled_data[outliers], color='red', s=120, label='실제 이상치', marker='o', edgecolors='black', zorder=5)\n\n# 탐지된 단일 이상치 포인트를 표시\nif len(predicted_anomaly_points) &gt; 0:\n    # Check if indices are within bounds of scaled_data\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(scaled_data)]\n    plt.scatter(valid_indices, scaled_data[valid_indices],\n                color='orange', marker='x', s=120, linewidth=2, label='탐지된 이상치', zorder=5)\n\nplt.title('개선된 모델(Week4) 최종 이상 탐지 결과', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('정규화된 값')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# --- 하단: 재구성 오차와 임계값 ---\nplt.subplot(2, 1, 2)\nplt.plot(all_reconstruction_error, label='윈도우별 재구성 오차', color='blue')\nplt.axhline(y=threshold, color='r', linestyle='--', label=f'임계값 ({threshold:.4f})')\n\n# 임계값을 넘은 \"윈도우\"를 표시\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, all_reconstruction_error[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\n\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론-및-고찰",
    "href": "posts/ABC_week04_model_optimization/index.html#결론-및-고찰",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론 및 고찰",
    "text": "5. 결론 및 고찰\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 개선하기 위한 여정을 상세히 다루었습니다. StandardScaler를 이용해 데이터 전처리를 표준화하고, Dropout을 추가하여 과적합을 방지했습니다. 또한, Optuna 라이브러리를 활용하여 학습률, 드롭아웃 비율 등 최적의 하이퍼파라미터를 체계적으로 탐색했습니다.\n특히, 탐지 정확도를 높이기 위해 정상 데이터만으로 모델을 학습하고, 재구성 오차의 Quantile(백분위수)을 이용해 통계적으로 강건한 임계값을 설정했습니다. 최종적으로는 임계값을 넘는 윈도우 내에서 오차가 가장 큰 단일 지점을 이상치로 특정하는 방식으로, 실제 이상치인 20, 50, 80번 인덱스를 성공적으로 탐지할 수 있었습니다.\n물론, 학습 과정의 미세한 무작위성으로 인해 간혹 오탐(False Positive)이 발생하는 한계도 확인했습니다. 이는 향후 모델 구조 자체를 개선하거나 더 다양한 데이터를 활용하여 안정성을 높여야 할 과제로 남았습니다. 다음 포스트에서는 또 다른 접근법으로 이 문제를 해결해 나가겠습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론",
    "href": "posts/ABC_week04_model_optimization/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론",
    "text": "5. 결론\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 개선하기 위한 여정을 상세히 다루었습니다. StandardScaler를 이용해 데이터 전처리를 표준화하고, Dropout을 추가하여 과적합을 방지했습니다. 또한, Optuna 라이브러리를 활용하여 학습률, 드롭아웃 비율 등 최적의 하이퍼파라미터를 체계적으로 탐색했습니다.\n특히, 탐지 정확도를 높이기 위해 정상 데이터만으로 모델을 학습하고, 재구성 오차의 Quantile(백분위수)을 이용해 통계적으로 강건한 임계값을 설정했습니다. 최종적으로는 임계값을 넘는 윈도우 내에서 오차가 가장 큰 단일 지점을 이상치로 특정하는 방식으로, 실제 이상치인 20, 50, 80번 인덱스를 성공적으로 탐지할 수 있었습니다.\n물론, 학습 과정의 미세한 무작위성으로 인해 간혹 오탐(False Positive)이 발생하는 한계도 확인했습니다. 이는 향후 모델 구조 자체를 개선하거나 더 다양한 데이터를 활용하여 안정성을 높여야 할 과제로 남았습니다. 다음 포스트에서는 또 다른 접근법으로 이 문제를 해결해 나가겠습니다."
  }
]