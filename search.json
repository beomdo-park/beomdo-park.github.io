[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "I am an undergraduate researcher at Eco AI Lab.\n\n\nI aim to become a Data Scientist an AI-powered problem solver applying research to real-world challenges.\n\n\n3rd-year Computer Engineering Student, Hanbat National University (HBNU).\n\n\nğŸ“‚ You can check out my portfolios:Â  Blog |Â  Repository\n\n\n    í™ˆí˜ì´ì§€\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n   \n\n\n\n\n\n     \n\n\n\n\n\n\n\n\nê¸°ê°„\ní™œë™ëª…\në‚´ìš©\nê¸°ê´€(ì¥ì†Œ)\n\n\n\n\n2026.01.20 ~ 01.27\nAAAI-26 Student Abstract and Poster Program (Accept)\nMulti-Stage Reinforcement Learning for Robust Charging of Quantum Batteries\nAAAI-26 (Singapore EXPO, Singapore)\n\n\n2025.11.20\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ í•™ë¶€ìƒ ìº¡ìŠ¤í†¤ ê²½ì§„ëŒ€íšŒ\n(ğŸ†ìš°ìˆ˜ìƒ ìˆ˜ìƒ) ì–‘ìë°°í„°ë¦¬ ì´ˆí¡ìˆ˜ ì´ë“ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ê·¸ë˜í”„ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì¶©ì „ ì œì–´ [ë°•ì¤€ì„±, ë°•ë²”ë„, ì¥í˜„ì„]\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.19 ~ 11.21\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ ìš°ìˆ˜ë…¼ë¬¸ìƒ(í•™ë¶€ìƒ)\n(ğŸ†ì¥ë ¤ìƒ ìˆ˜ìƒ) ë‹¨ê³„ì  ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ê°•ê±´í•œ ì–‘ì ë°°í„°ë¦¬ ì¶©ì „(2ì €ì)\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.01\nğŸªª TOPCIT ì •ê¸°í‰ê°€ (ìˆ˜ì¤€ 3 ë‹¬ì„±)\nìˆ˜ì¤€3: ê¸°ìˆ  ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­ì— ëŒ€í•œ ì§€ì‹ê³¼ ìŠ¤í‚¬ì„ ì ìš©í•˜ì—¬ ê³¼ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€\nì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)\n\n\n2025.10.31\níŠ¹í—ˆ ì¶œì›\nì»¤ë¦¬í˜ëŸ¼ ê°•í™”í•™ìŠµì„ ì´ìš©í•œ ì–‘ì ë°°í„°ë¦¬ì˜ ê°•ê±´í•œ ì¶©ì „ ì‹œìŠ¤í…œ / ê¸°ì—¬ë„ 15% (ì¶œì›/ì‹¬ì‚¬ì¤‘)\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì‚°í•™í˜‘ë ¥ë‹¨\n\n\n2025.10.30\nì»´í“¨í„°ê³µí•™ê³¼ í¬íŠ¸í´ë¦¬ì˜¤ ê²½ì§„ëŒ€íšŒ\nğŸ†ìš°ìˆ˜ìƒ\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.07.14\nA Comparative Study of Customized Algorithms for Anomaly Detection in Industry-Specific Power Data(SCIE)\nDBA K-Means ëª¨ë¸ ë¶€ë¶„(ê³µì €ì)\nMDPI journal energies\n\n\n2025.06.30 ~ 07.11\nê¸°ì—… ì¸í„´ì‹­\nììœ¨ì£¼í–‰ ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë° ê²½ë¡œ ìµœì í™” (Gazebo ì´ìš©)\n(ì£¼) ì•„êµ°\n\n\n2025.06.24 ~ 06.27\nëŒ€í•œì „ìê³µí•™íšŒ í•˜ê³„ ì¢…í•©í•™ìˆ ëŒ€íšŒ\nê³ ì† í‘¸ë¦¬ì— ë³€í™˜(FFT) ê¸°ë°˜ ì£¼ê¸° ì¶”ì¶œ ë° ìœˆë„ìš° êµ¬ì„±ì„ í™œìš©í•œ GELU CNN-GRU AE ëª¨ë¸ì˜ ì‚°ì—… ì „ë ¥ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\në¡¯ë°í˜¸í…” ì œì£¼(ì¤‘ë¬¸)\n\n\n2025.05.20 ~ 06.30\nABC í”„ë¡œì íŠ¸ ë©˜í† ë§\nì‚°ì—… ì „ë ¥ ì†Œë¹„ëŸ‰ ì´ìƒì¹˜ ê²€ì¶œ ë° ìµœì í™”\nìœ í´ë¦¬ë“œ ì†Œí”„íŠ¸\n\n\n2025.06.13\nğŸªª ë°ì´í„°ë¶„ì„ ì¤€ì „ë¬¸ê°€(ADsP) ìê²©ì¦ ì·¨ë“\në°ì´í„° ë¶„ì„ ê¸°íš ë° ìˆ˜í–‰ ì‹¤ë¬´ì ìê²©\ní•œêµ­ë°ì´í„°ì‚°ì—…ì§„í¥ì›\n\n\n2025.05.01 ~\nEco AI Lab ë©ì¥\nì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì§€ì›, ì¼ì • ì¡°ìœ¨, í–‰ì • ì—…ë¬´ ë“±\nêµ­ë¦½í•œë°­ëŒ€í•™êµ EcoAI Lab\n\n\n2025.04.07 ~\nì‚°í•™í˜‘ë ¥ í”„ë¡œì íŠ¸\nììœ¨ì£¼í–‰ ì£¼ì°¨ë¡œë´‡ ìš´ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.24 ~\nì†Œì¤‘í•œ JUMP-UP Labs\nAI(ì¸ê³µì§€ëŠ¥)ë¥¼ í™œìš©í•œ ì‚°ì—…ì²´ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.04 ~ 06.13\në°ì´í„°ì‚¬ì´ì–¸ìŠ¤(ì´ìƒê¸ˆ êµìˆ˜ë‹˜) ì‹¤ìŠµì¡°êµ 25-1í•™ê¸°\nì‹¤ìŠµìë£Œ ì œì‘ ë° ì§ˆì˜ì‘ë‹µ ê´€ë¦¬\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.02.05 ~ 02.07\ní•œêµ­í†µì‹ í•™íšŒ ë™ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ\nDBA K-Means êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í™”ê³µì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nê°•ì›ë„ ìš©í‰ë¦¬ì¡°íŠ¸\n\n\n2025.01.16 ~ 01.21\nIROC 2024/25 ì„¸ê³„ëŒ€íšŒ\nëŒ€íšŒ ì§„í–‰ìš”ì›\në¶€ì‚° ë²¡ìŠ¤ì½”"
  },
  {
    "objectID": "about.html#activities",
    "href": "about.html#activities",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "ê¸°ê°„\ní™œë™ëª…\në‚´ìš©\nê¸°ê´€(ì¥ì†Œ)\n\n\n\n\n2026.01.20 ~ 01.27\nAAAI-26 Student Abstract and Poster Program (Accept)\nMulti-Stage Reinforcement Learning for Robust Charging of Quantum Batteries\nAAAI-26 (Singapore EXPO, Singapore)\n\n\n2025.11.20\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ í•™ë¶€ìƒ ìº¡ìŠ¤í†¤ ê²½ì§„ëŒ€íšŒ\n(ğŸ†ìš°ìˆ˜ìƒ ìˆ˜ìƒ) ì–‘ìë°°í„°ë¦¬ ì´ˆí¡ìˆ˜ ì´ë“ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ê·¸ë˜í”„ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì¶©ì „ ì œì–´ [ë°•ì¤€ì„±, ë°•ë²”ë„, ì¥í˜„ì„]\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.19 ~ 11.21\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ ìš°ìˆ˜ë…¼ë¬¸ìƒ(í•™ë¶€ìƒ)\n(ğŸ†ì¥ë ¤ìƒ ìˆ˜ìƒ) ë‹¨ê³„ì  ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ê°•ê±´í•œ ì–‘ì ë°°í„°ë¦¬ ì¶©ì „(2ì €ì)\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.01\nğŸªª TOPCIT ì •ê¸°í‰ê°€ (ìˆ˜ì¤€ 3 ë‹¬ì„±)\nìˆ˜ì¤€3: ê¸°ìˆ  ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­ì— ëŒ€í•œ ì§€ì‹ê³¼ ìŠ¤í‚¬ì„ ì ìš©í•˜ì—¬ ê³¼ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€\nì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)\n\n\n2025.10.31\níŠ¹í—ˆ ì¶œì›\nì»¤ë¦¬í˜ëŸ¼ ê°•í™”í•™ìŠµì„ ì´ìš©í•œ ì–‘ì ë°°í„°ë¦¬ì˜ ê°•ê±´í•œ ì¶©ì „ ì‹œìŠ¤í…œ / ê¸°ì—¬ë„ 15% (ì¶œì›/ì‹¬ì‚¬ì¤‘)\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì‚°í•™í˜‘ë ¥ë‹¨\n\n\n2025.10.30\nì»´í“¨í„°ê³µí•™ê³¼ í¬íŠ¸í´ë¦¬ì˜¤ ê²½ì§„ëŒ€íšŒ\nğŸ†ìš°ìˆ˜ìƒ\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.07.14\nA Comparative Study of Customized Algorithms for Anomaly Detection in Industry-Specific Power Data(SCIE)\nDBA K-Means ëª¨ë¸ ë¶€ë¶„(ê³µì €ì)\nMDPI journal energies\n\n\n2025.06.30 ~ 07.11\nê¸°ì—… ì¸í„´ì‹­\nììœ¨ì£¼í–‰ ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë° ê²½ë¡œ ìµœì í™” (Gazebo ì´ìš©)\n(ì£¼) ì•„êµ°\n\n\n2025.06.24 ~ 06.27\nëŒ€í•œì „ìê³µí•™íšŒ í•˜ê³„ ì¢…í•©í•™ìˆ ëŒ€íšŒ\nê³ ì† í‘¸ë¦¬ì— ë³€í™˜(FFT) ê¸°ë°˜ ì£¼ê¸° ì¶”ì¶œ ë° ìœˆë„ìš° êµ¬ì„±ì„ í™œìš©í•œ GELU CNN-GRU AE ëª¨ë¸ì˜ ì‚°ì—… ì „ë ¥ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\në¡¯ë°í˜¸í…” ì œì£¼(ì¤‘ë¬¸)\n\n\n2025.05.20 ~ 06.30\nABC í”„ë¡œì íŠ¸ ë©˜í† ë§\nì‚°ì—… ì „ë ¥ ì†Œë¹„ëŸ‰ ì´ìƒì¹˜ ê²€ì¶œ ë° ìµœì í™”\nìœ í´ë¦¬ë“œ ì†Œí”„íŠ¸\n\n\n2025.06.13\nğŸªª ë°ì´í„°ë¶„ì„ ì¤€ì „ë¬¸ê°€(ADsP) ìê²©ì¦ ì·¨ë“\në°ì´í„° ë¶„ì„ ê¸°íš ë° ìˆ˜í–‰ ì‹¤ë¬´ì ìê²©\ní•œêµ­ë°ì´í„°ì‚°ì—…ì§„í¥ì›\n\n\n2025.05.01 ~\nEco AI Lab ë©ì¥\nì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì§€ì›, ì¼ì • ì¡°ìœ¨, í–‰ì • ì—…ë¬´ ë“±\nêµ­ë¦½í•œë°­ëŒ€í•™êµ EcoAI Lab\n\n\n2025.04.07 ~\nì‚°í•™í˜‘ë ¥ í”„ë¡œì íŠ¸\nììœ¨ì£¼í–‰ ì£¼ì°¨ë¡œë´‡ ìš´ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.24 ~\nì†Œì¤‘í•œ JUMP-UP Labs\nAI(ì¸ê³µì§€ëŠ¥)ë¥¼ í™œìš©í•œ ì‚°ì—…ì²´ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.04 ~ 06.13\në°ì´í„°ì‚¬ì´ì–¸ìŠ¤(ì´ìƒê¸ˆ êµìˆ˜ë‹˜) ì‹¤ìŠµì¡°êµ 25-1í•™ê¸°\nì‹¤ìŠµìë£Œ ì œì‘ ë° ì§ˆì˜ì‘ë‹µ ê´€ë¦¬\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.02.05 ~ 02.07\ní•œêµ­í†µì‹ í•™íšŒ ë™ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ\nDBA K-Means êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í™”ê³µì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nê°•ì›ë„ ìš©í‰ë¦¬ì¡°íŠ¸\n\n\n2025.01.16 ~ 01.21\nIROC 2024/25 ì„¸ê³„ëŒ€íšŒ\nëŒ€íšŒ ì§„í–‰ìš”ì›\në¶€ì‚° ë²¡ìŠ¤ì½”"
  },
  {
    "objectID": "posts/project-abc-05-real-data-analysis/index.html",
    "href": "posts/project-abc-05-real-data-analysis/index.html",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸° ë‹¤ì„¯ ë²ˆì§¸ ê¸°ìˆ ë…¸íŠ¸ì…ë‹ˆë‹¤. Week04ì—ì„œ ê°œë°œí•œ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì„ ì‹¤ì œ Kaggle ê³µê°œ ë°ì´í„°ì…‹(ì£¼íƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ 3ë…„ì¹˜)ì— ì ìš©í•´, ì‹¤ì „ í™˜ê²½ì—ì„œì˜ ì´ìƒ íƒì§€ ì„±ëŠ¥ê³¼ í•œê³„ë¥¼ ì ê²€í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ í†µí•´, ì´ë¡ ì  ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„°ì—ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì‹¤ë¬´ì—ì„œ ë§ˆì£¼ì¹  ìˆ˜ ìˆëŠ” ë¬¸ì œì™€ í•´ê²°ì±…ì„ íƒêµ¬í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-05-real-data-analysis/index.html#ë°ì´í„°-ì†Œê°œ-ë°-ì¤€ë¹„",
    "href": "posts/project-abc-05-real-data-analysis/index.html#ë°ì´í„°-ì†Œê°œ-ë°-ì¤€ë¹„",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€",
    "section": "1. ë°ì´í„° ì†Œê°œ ë° ì¤€ë¹„",
    "text": "1. ë°ì´í„° ì†Œê°œ ë° ì¤€ë¹„\nì´ë²ˆ ì£¼ì— ì‚¬ìš©í•  ë°ì´í„°ëŠ” Kaggleì— ê³µê°œëœ â€˜Residential Power Usage 3-Years Dataâ€™ì…ë‹ˆë‹¤. í•œ ê°€ì •ì˜ 3ë…„ê°„ ì „ë ¥ ì‚¬ìš©ëŸ‰ì´ ë¶„ ë‹¨ìœ„ë¡œ ê¸°ë¡ëœ ì‹œê³„ì—´ ë°ì´í„°ë¡œ, ì‹¤ì œ í™˜ê²½ì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ íŒ¨í„´ê³¼ ì´ìƒ í˜„ìƒì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.\në¨¼ì €, GitHub Raw URLì„ í†µí•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ ë‚ ì§œ ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•œ ë’¤, ì „ë ¥ ì‚¬ìš©ëŸ‰ ì»¬ëŸ¼ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# GitHub Raw URLì„ í†µí•´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\nurl = 'https://raw.githubusercontent.com/beomdo-park/ml-dl-by-dataset/main/datasets/power_usage_2016_to_2020.csv'\n\nprint(\"ë°ì´í„° ë¡œë”© ì‹œì‘...\")\ndf = pd.read_csv(url)\nprint(\"ë°ì´í„° ë¡œë”© ì™„ë£Œ.\")\n\ndf.info()\n\në°ì´í„° ë¡œë”© ì‹œì‘...\në°ì´í„° ë¡œë”© ì™„ë£Œ.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35952 entries, 0 to 35951\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   StartDate    35952 non-null  object \n 1   Value (kWh)  35952 non-null  float64\n 2   day_of_week  35952 non-null  int64  \n 3   notes        35952 non-null  object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\në°ì´í„°ëŠ” â€˜StartDateâ€™, â€˜Value (kWh)â€™, â€˜day_of_weekâ€™, â€˜notesâ€™ ë“±ì˜ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì‹œê³„ì—´ ë¶„ì„ì„ ìœ„í•´ â€˜StartDateâ€™ë¥¼ datetime í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•œ ë’¤, â€™Value (kWh)â€™ ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n\nprint(\"ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\")\n# 'StartDate'ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ë±ìŠ¤ë¡œ ì„¤ì •\ndf[\"StartDate\"] = pd.to_datetime(df[\"StartDate\"])\ndf = df.set_index(\"StartDate\")\n\n# ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬\ndf.sort_index(inplace=True)\nprint(\"ë°ì´í„° ì‹œê°„ìˆœ ì •ë ¬ ì™„ë£Œ.\")\n\n# 'Value (kWh)' ì»¬ëŸ¼ë§Œ ì„ íƒ\nvalue_col = \"Value (kWh)\"\ndf_value = df[[value_col]].copy()\n\n# ì‹œê°„ ë‹¨ìœ„ë¥¼ 'H'ë¡œ ì¬ìƒ˜í”Œë§í•˜ê³  ëˆ„ë½ëœ ê°’ì€ ì„ í˜• ë³´ê°„\nprint(\"ë°ì´í„°ë¥¼ ì‹œê°„ ë‹¨ìœ„ë¡œ ì¬ìƒ˜í”Œë§í•˜ê³  ëˆ„ë½ëœ ê°’ì„ ë³´ê°„í•©ë‹ˆë‹¤...\")\ndf_value = df_value.resample(\"H\").mean()\ndf_value[value_col] = df_value[value_col].interpolate(method=\"linear\")\nprint(\"ì¬ìƒ˜í”Œë§ ë° ë³´ê°„ ì™„ë£Œ.\")\n\n# [ìˆ˜ì •] ë¶„ì„ íš¨ìœ¨ì„±ì„ ìœ„í•´ 2019ë…„ ë°ì´í„°ë§Œ ì‚¬ìš©\nprint(\"ë¶„ì„ íš¨ìœ¨ì„±ì„ ìœ„í•´ 2019ë…„ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤...\")\ndf_value = df_value[df_value.index.year == 2019].copy()\nprint(\"ë°ì´í„° ìŠ¬ë¼ì´ì‹± ì™„ë£Œ.\")\n\n\n# ë°ì´í„° ì‹œê°í™” (2019ë…„)\nplt.figure(figsize=(10, 5))\nplt.plot(df_value.index, df_value[value_col], label=\"ì „ë ¥ ì‚¬ìš©ëŸ‰ (2019ë…„)\")\nplt.title(\"ì‹œê°„ì— ë”°ë¥¸ ì „ë ¥ ì‚¬ìš©ëŸ‰ (2019ë…„)\")\nplt.xlabel(\"ë‚ ì§œ\")\nplt.ylabel(\"ì‚¬ìš©ëŸ‰ (kWh)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"2019ë…„ ë°ì´í„° í¬ê¸°: {df_value.shape}\")\n\në°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...\në°ì´í„° ì‹œê°„ìˆœ ì •ë ¬ ì™„ë£Œ.\në°ì´í„°ë¥¼ ì‹œê°„ ë‹¨ìœ„ë¡œ ì¬ìƒ˜í”Œë§í•˜ê³  ëˆ„ë½ëœ ê°’ì„ ë³´ê°„í•©ë‹ˆë‹¤...\nì¬ìƒ˜í”Œë§ ë° ë³´ê°„ ì™„ë£Œ.\në¶„ì„ íš¨ìœ¨ì„±ì„ ìœ„í•´ 2019ë…„ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤...\në°ì´í„° ìŠ¬ë¼ì´ì‹± ì™„ë£Œ.\n\n\nC:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_8580\\3802000988.py:16: FutureWarning:\n\n'H' is deprecated and will be removed in a future version, please use 'h' instead.\n\n\n\n\n\n\n\n\n\n\n2019ë…„ ë°ì´í„° í¬ê¸°: (8760, 1)"
  },
  {
    "objectID": "posts/project-abc-05-real-data-analysis/index.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ê³„ì ˆì„±-ì œê±°",
    "href": "posts/project-abc-05-real-data-analysis/index.html#ë°ì´í„°-ì „ì²˜ë¦¬-ë°-ê³„ì ˆì„±-ì œê±°",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€",
    "section": "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ê³„ì ˆì„± ì œê±°",
    "text": "2. ë°ì´í„° ì „ì²˜ë¦¬ ë° ê³„ì ˆì„± ì œê±°\nì‹¤ì œ ì „ë ¥ ë°ì´í„°ëŠ” ê°•í•œ ê³„ì ˆì„±ê³¼ ì¼ê°„ íŒ¨í„´ì„ ë³´ì…ë‹ˆë‹¤. íš¨ê³¼ì ì¸ ì´ìƒ íƒì§€ë¥¼ ìœ„í•´ ë‹¤ë‹¨ê³„ ê³„ì ˆì„± ì œê±°ë¥¼ ì ìš©í•œ í›„ ìœˆë„ìš° ìƒì„±ê³¼ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\nfrom statsmodels.tsa.seasonal import STL\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# --- Helper Functions ---\ndef create_sliding_windows(data, window_size):\n    \"\"\"ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i : i + window_size])\n    return np.array(windows)\n\n\ndef normalize_windows(windows):\n    \"\"\"ê° ìœˆë„ìš°ë³„ë¡œ ê°œë³„ ì •ê·œí™”\"\"\"\n    normalized_windows = []\n    scalers = []\n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    return np.array(normalized_windows), scalers\n\n\ndef seasonal_decomposition_approach(data, index):\n    \"\"\"STL ë¶„í•´ë¥¼ í†µí•œ ê³„ì ˆì„± ì œê±°\"\"\"\n    # ë°ì´í„°ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì£¼ê¸°ë¥¼ ì¡°ì •\n    period = 8760  # ì—°ê°„ ì£¼ê¸° (ì‹œê°„ ë‹¨ìœ„)\n    if len(data) &lt; 2 * period:\n        period = 24 * 365  # ê·¼ì‚¬ì¹˜\n        if len(data) &lt; 2 * period:\n            period = 24 * 30  # ì›”ê°„ ì£¼ê¸°\n            if len(data) &lt; 2 * period:\n                period = 24 * 7  # ì£¼ê°„ ì£¼ê¸°\n\n    # STLì˜ seasonal íŒŒë¼ë¯¸í„°ëŠ” ë°˜ë“œì‹œ í™€ìˆ˜ì—¬ì•¼ í•¨\n    if period % 2 == 0:\n        period += 1\n\n    ts = pd.Series(data, index=index[: len(data)])\n    stl = STL(ts, seasonal=period, robust=True)\n    result = stl.fit()\n    deseasonalized = ts - result.seasonal\n    return deseasonalized.values, result\n\n\ndef comprehensive_deseasonalize(data, index, window_size):\n    \"\"\"ë‹¤ë‹¨ê³„ ê³„ì ˆì„± ì œê±° ë° ìœˆë„ìš° ìƒì„±\"\"\"\n    print(\"1ë‹¨ê³„: STL ë¶„í•´ë¡œ ì£¼ìš” ê³„ì ˆì„± ì œê±°...\")\n    deseason_data, decomp_result = seasonal_decomposition_approach(data, index)\n\n    print(\"2ë‹¨ê³„: ì¼ê°„ íŒ¨í„´ ì œê±° (24ì‹œê°„ ì´ë™í‰ê· )...\")\n    daily_smooth = pd.Series(deseason_data).rolling(24, center=True).mean()\n    deseason_data = deseason_data - daily_smooth.fillna(0).values\n\n    print(\"3ë‹¨ê³„: ìœˆë„ìš° ìƒì„± ë° ì •ê·œí™”...\")\n    windows = create_sliding_windows(deseason_data, window_size)\n    normalized_windows, scalers = normalize_windows(windows)\n\n    return normalized_windows, scalers, decomp_result\n\n\n# --- Data Extraction ---\nraw_data = df_value[value_col].values\nprint(f\"ì›ë³¸ ë°ì´í„° í¬ê¸°: {len(raw_data)}\")\n\n# --- 1. ê³„ì ˆì„± ë¶„í•´ ì‹œê°í™” ---\n_, decomp_result_for_plot = seasonal_decomposition_approach(raw_data, df_value.index)\n\nplt.figure(figsize=(10, 8)) # ë„ˆë¹„ 10ìœ¼ë¡œ ìˆ˜ì •\nplt.subplot(4, 1, 1)\nplt.plot(decomp_result_for_plot.observed)\nplt.title('ì›ë³¸ ë°ì´í„° (2019ë…„)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 2)\nplt.plot(decomp_result_for_plot.trend)\nplt.title('íŠ¸ë Œë“œ (2019ë…„)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 3)\nplt.plot(decomp_result_for_plot.seasonal)\nplt.title('ê³„ì ˆì„± (2019ë…„)')\nplt.ylabel(\"kWh\")\n\nplt.subplot(4, 1, 4)\n# ê³„ì ˆì„± ì œê±°ëœ ë°ì´í„° ê³„ì‚°\ndesasonalized_for_plot = (\n    decomp_result_for_plot.observed - decomp_result_for_plot.seasonal\n)\nplt.plot(desasonalized_for_plot)\nplt.title('ê³„ì ˆì„± ì œê±° í›„ (2019ë…„)')\nplt.ylabel(\"kWh\")\nplt.xlabel(\"ì‹œê°„\")\n\nplt.tight_layout()\nplt.show()\n\n# --- 2. ìµœì¢… ë°ì´í„° ì²˜ë¦¬ ë° ìœˆë„ìš° ìƒì„± ---\nprint(\"\\në‹¤ë‹¨ê³„ ê³„ì ˆì„± ì œê±° ë° ìœˆë„ìš° ìƒì„± ì‹œì‘...\")\nwindow_size = 10 # Week4 Optuna ìµœì ê°’\nprocessed_windows, window_scalers, decomp_result = comprehensive_deseasonalize(\n    raw_data, df_value.index, window_size\n)\n\nprint(f\"ê³„ì ˆì„± ì œê±° í›„ ìƒì„±ëœ ìœˆë„ìš° ìˆ˜: {len(processed_windows)}\")\nprint(f\"ê° ìœˆë„ìš° í¬ê¸°: {processed_windows.shape[1]}\")\n\n# PyTorch í…ì„œë¡œ ë³€í™˜\nall_windows_torch = torch.from_numpy(processed_windows).unsqueeze(1).float()\nprint(f\"í…ì„œ í˜•íƒœ: {all_windows_torch.shape}\")\n\n# --- 3. ì²˜ë¦¬ ì „í›„ ë¹„êµ ì‹œê°í™” ---\nplt.figure(figsize=(10, 6))\nplt.subplot(2, 1, 1)\nplt.plot(raw_data, alpha=0.8, label='ì›ë³¸ ë°ì´í„° (2019ë…„)')\nplt.title('ì›ë³¸ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° (2019ë…„)')\nplt.ylabel('kWh')\nplt.legend()\n\nplt.subplot(2, 1, 2)\ndeseason_full = raw_data - decomp_result.seasonal.values\nplt.plot(deseason_full, alpha=0.8, label='ê³„ì ˆì„± ì œê±° í›„', color=\"orange\")\nplt.title('ê³„ì ˆì„± ì œê±° í›„ ë°ì´í„° (2019ë…„)')\nplt.ylabel('kWh')\nplt.xlabel('ì‹œê°„')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nì›ë³¸ ë°ì´í„° í¬ê¸°: 8760\n\n\n\n\n\n\n\n\n\n\në‹¤ë‹¨ê³„ ê³„ì ˆì„± ì œê±° ë° ìœˆë„ìš° ìƒì„± ì‹œì‘...\n1ë‹¨ê³„: STL ë¶„í•´ë¡œ ì£¼ìš” ê³„ì ˆì„± ì œê±°...\n2ë‹¨ê³„: ì¼ê°„ íŒ¨í„´ ì œê±° (24ì‹œê°„ ì´ë™í‰ê· )...\n3ë‹¨ê³„: ìœˆë„ìš° ìƒì„± ë° ì •ê·œí™”...\nê³„ì ˆì„± ì œê±° í›„ ìƒì„±ëœ ìœˆë„ìš° ìˆ˜: 8751\nê° ìœˆë„ìš° í¬ê¸°: 10\ní…ì„œ í˜•íƒœ: torch.Size([8751, 1, 10])"
  },
  {
    "objectID": "posts/project-abc-05-real-data-analysis/index.html#ëª¨ë¸-í•™ìŠµ-ë°-ì´ìƒì¹˜-íƒì§€",
    "href": "posts/project-abc-05-real-data-analysis/index.html#ëª¨ë¸-í•™ìŠµ-ë°-ì´ìƒì¹˜-íƒì§€",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€",
    "section": "3. ëª¨ë¸ í•™ìŠµ ë° ì´ìƒì¹˜ íƒì§€",
    "text": "3. ëª¨ë¸ í•™ìŠµ ë° ì´ìƒì¹˜ íƒì§€\nWeek4ì—ì„œ ìµœì í™”í•œ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì „ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì´ìƒì¹˜ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. í•™ìŠµ, í‰ê°€, ì‹œê°í™” ê³¼ì •ì„ í•˜ë‚˜ì˜ ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ í†µí•©í•˜ì—¬ ì„¤ëª…í•©ë‹ˆë‹¤.\n\n3.1. ëª¨ë¸ ì •ì˜\nWeek4ì—ì„œ Optunaë¥¼ í†µí•´ ìµœì í™”í•œ ëª¨ë¸ êµ¬ì¡°ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ Dropoutì„ í¬í•¨í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ , ConvTranspose1d ëŒ€ì‹  Upsampleê³¼ AdaptiveAvgPool1dë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ìœˆë„ìš° í¬ê¸°ì— ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆë„ë¡ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n\n\n3.2. í•™ìŠµ ë° í‰ê°€\nì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” íŒ¨í„´ì´ ì•ˆì •í™”ëœ êµ¬ê°„ì„ â€™ì •ìƒâ€™ìœ¼ë¡œ ê°„ì£¼í•˜ê³  í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì˜ 20% ~ 80% êµ¬ê°„ì„ ì •ìƒ ë°ì´í„°ë¡œ ì •ì˜í•˜ê³ , ì´ ë°ì´í„°ë¡œë§Œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ê·¸ í›„, í•™ìŠµëœ ëª¨ë¸ì„ ì „ì²´ ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.\n\n# --- 1. í•™ìŠµ ë°ì´í„° ë¶„í•  ---\ntotal_windows = len(all_windows_torch)\ntrain_start_idx = int(total_windows * 0.2)\ntrain_end_idx = int(total_windows * 0.8)\ntrain_windows_torch = all_windows_torch[train_start_idx:train_end_idx]\n\nprint(f\"ì „ì²´ ìœˆë„ìš° ìˆ˜: {total_windows}\")\nprint(f\"í•™ìŠµ êµ¬ê°„: {train_start_idx} ~ {train_end_idx}\")\nprint(f\"í•™ìŠµì— ì‚¬ìš©í•  ìœˆë„ìš° ìˆ˜: {len(train_windows_torch)}\")\n\n# --- 2. ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ ---\n# Week4ì—ì„œ ì°¾ì€ ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.137)\noptimizer = optim.RMSprop(model.parameters(), lr=0.000981)\ncriterion = nn.MSELoss()\n\ndataset = TensorDataset(train_windows_torch)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\nprint(\"\\nëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\nepochs = 15\nfor epoch in range(epochs):\n    model.train()\n    for data_batch in data_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. ì„ê³„ê°’ ì„¤ì • ë° ì´ìƒì¹˜ íƒì§€ ---\nmodel.eval()\n\n# í•™ìŠµ ë°ì´í„°ì˜ ì¬êµ¬ì„± ì˜¤ì°¨ë¡œ ì„ê³„ê°’ ì„¤ì •\nwith torch.no_grad():\n    reconstructed_train = model(train_windows_torch)\n    error_train = torch.mean(\n        (train_windows_torch - reconstructed_train) ** 2, dim=(1, 2)\n    )\n    train_reconstruction_error = error_train.numpy()\n\nquantile_level = 0.99\nthreshold = np.quantile(train_reconstruction_error, quantile_level)\nprint(f\"\\nì„ê³„ê°’ ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\nwith torch.no_grad():\n    reconstructed_all = model(all_windows_torch)\n    reconstruction_error = torch.mean(\n        (all_windows_torch - reconstructed_all) ** 2, dim=(1, 2)\n    ).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all) ** 2).squeeze().numpy()\n\n# ì´ìƒì¹˜ íƒì§€\nanomaly_window_indices = np.where(reconstruction_error &gt; threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx &lt; len(pointwise_error):\n        max_error_idx_in_window = np.argmax(pointwise_error[window_idx])\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\npredicted_anomaly_points = [\n    idx for idx in predicted_anomaly_points if idx &lt; len(raw_data)\n]\nprint(f\"íƒì§€ëœ ì´ìƒì¹˜ í¬ì¸íŠ¸ ìˆ˜: {len(predicted_anomaly_points)}\")\n\nì „ì²´ ìœˆë„ìš° ìˆ˜: 8751\ní•™ìŠµ êµ¬ê°„: 1750 ~ 7000\ní•™ìŠµì— ì‚¬ìš©í•  ìœˆë„ìš° ìˆ˜: 5250\n\nëª¨ë¸ í•™ìŠµ ì‹œì‘...\nEpoch [5/15], Loss: 0.020301\nEpoch [10/15], Loss: 0.027192\nEpoch [15/15], Loss: 0.044883\n\nì„ê³„ê°’ (99.0% Quantile): 0.123072\níƒì§€ëœ ì´ìƒì¹˜ í¬ì¸íŠ¸ ìˆ˜: 79\n\n\n\n\n3.3. ê²°ê³¼ ì‹œê°í™”\níƒì§€ëœ ì´ìƒì¹˜ë¥¼ ì›ë³¸ ë°ì´í„°ì™€ í•¨ê»˜ ì‹œê°í™”í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤. ì „ì²´ ê¸°ê°„ê³¼ ìµœê·¼ 3ê°œì›” êµ¬ê°„ì„ ë‚˜ëˆ„ì–´ ìƒì„¸íˆ ë¶„ì„í•©ë‹ˆë‹¤.\n\nplt.figure(figsize=(10, 9))\n\n# ìƒë‹¨: ì „ì²´ ë°ì´í„°ì™€ íƒì§€ ê²°ê³¼\nplt.subplot(3, 1, 1)\nplt.plot(raw_data, label='ì›ë³¸ ì „ë ¥ ì‚¬ìš©ëŸ‰ (2019ë…„)', alpha=0.7, color='blue', linewidth=0.8)\nif predicted_anomaly_points:\n    plt.scatter(predicted_anomaly_points, raw_data[predicted_anomaly_points],\n                color='red', marker='x', s=80, linewidth=2, label=f'íƒì§€ëœ ì´ìƒì¹˜ ({len(predicted_anomaly_points)}ê°œ)', zorder=5)\nplt.title('ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° ì´ìƒ íƒì§€ ê²°ê³¼ (2019ë…„)', fontsize=14, fontweight='bold')\nplt.ylabel('ì‚¬ìš©ëŸ‰ (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# ì¤‘ê°„: ì¬êµ¬ì„± ì˜¤ì°¨ì™€ ì„ê³„ê°’\nplt.subplot(3, 1, 2)\nplt.plot(reconstruction_error, label='ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨', color='steelblue', linewidth=1)\nplt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, label=f'ì„ê³„ê°’ ({threshold:.4f})')\nif anomaly_window_indices.any():\n    plt.scatter(anomaly_window_indices, reconstruction_error[anomaly_window_indices], \n               c='red', s=60, alpha=0.8, label=f'ì´ìƒ ìœˆë„ìš° ({len(anomaly_window_indices)}ê°œ)', zorder=5)\nplt.title('ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨ ë¶„í¬ (2019ë…„)', fontsize=14, fontweight='bold')\nplt.xlabel('ìœˆë„ìš° ì¸ë±ìŠ¤')\nplt.ylabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# í•˜ë‹¨: ì„¸ë¶„í™”ëœ êµ¬ê°„ ì‹œê°í™” (ë§¨ ë’¤ 3ê°œì›”ì¹˜ ë°ì´í„°)\nplt.subplot(3, 1, 3)\nmonth_hours = 90 * 24\nstart_idx_viz = max(0, len(raw_data) - month_hours)\nend_idx_viz = len(raw_data)\n\nplt.plot(range(start_idx_viz, end_idx_viz), raw_data[start_idx_viz:end_idx_viz], \n         label=f'ì „ë ¥ ì‚¬ìš©ëŸ‰ (2019ë…„, ìµœê·¼ 3ê°œì›”)', alpha=0.8, color='navy', linewidth=1)\n\nmonth_anomalies = [i for i in predicted_anomaly_points if start_idx_viz &lt;= i &lt; end_idx_viz]\nif month_anomalies:\n    plt.scatter(month_anomalies, raw_data[month_anomalies],\n                color='red', marker='o', s=100, alpha=0.8, \n                label=f'ìµœê·¼ 3ê°œì›” ì´ìƒì¹˜ ({len(month_anomalies)}ê°œ)', zorder=5)\n\nplt.title('ì„¸ë¶„í™”ëœ ì´ìƒ íƒì§€ ê²°ê³¼ (2019ë…„, ìµœê·¼ 3ê°œì›”)', fontsize=14, fontweight='bold')\nplt.xlabel('ì‹œê°„ ìŠ¤í…')\nplt.ylabel('ì‚¬ìš©ëŸ‰ (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# íƒì§€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥\nprint(f\"\\n=== ì´ìƒ íƒì§€ ê²°ê³¼ ìš”ì•½ (2019ë…„) ===\")\nprint(f\"ë¶„ì„ ë°ì´í„° ê¸¸ì´: {len(raw_data):,} ì‹œê°„\")\nprint(f\"ì „ì²´ íƒì§€ëœ ì´ìƒì¹˜: {len(predicted_anomaly_points)}ê°œ\")\nprint(f\"ìµœê·¼ 3ê°œì›” êµ¬ê°„ ì´ìƒì¹˜: {len(month_anomalies)}ê°œ\")\nprint(f\"ì´ìƒì¹˜ ë¹„ìœ¨: {len(predicted_anomaly_points) / len(raw_data) * 100:.3f}%\")\n\n\n\n\n\n\n\n\n\n=== ì´ìƒ íƒì§€ ê²°ê³¼ ìš”ì•½ (2019ë…„) ===\në¶„ì„ ë°ì´í„° ê¸¸ì´: 8,760 ì‹œê°„\nì „ì²´ íƒì§€ëœ ì´ìƒì¹˜: 79ê°œ\nìµœê·¼ 3ê°œì›” êµ¬ê°„ ì´ìƒì¹˜: 26ê°œ\nì´ìƒì¹˜ ë¹„ìœ¨: 0.902%"
  },
  {
    "objectID": "posts/project-abc-05-real-data-analysis/index.html#ê²°ë¡ ",
    "href": "posts/project-abc-05-real-data-analysis/index.html#ê²°ë¡ ",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€",
    "section": "4. ê²°ë¡ ",
    "text": "4. ê²°ë¡ \nì´ë²ˆ 5ì£¼ì°¨ì—ì„œëŠ” ì§€ë‚œ 4ì£¼ê°„ ê°œë°œí•´ì˜¨ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì„ ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ì— ì ìš©í•´ë´¤ìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ëª¨ë¸ì„ ëŒë ¤ë³´ëŠ” ë°ì„œ ê·¸ì¹˜ì§€ ì•Šê³ , ì‹¤ì œ ë°ì´í„°ê°€ ê°€ì§„ ë³µì¡í•¨ê³¼ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥ì„±ì„ ì§ì ‘ ë§ˆì£¼í•˜ë©°, ì´ë¡ ê³¼ ì‹¤ì „ì˜ ê°„ê·¹ì„ ì¢íˆëŠ” ê³¼ì •ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤.\n\nì§„í–‰ ê³¼ì •\n\nì²´ê³„ì ì¸ ê°œì„ \nWeek 4ì—ì„œ ì •ë¦½í•œ â€™ìœˆë„ìš°ë³„ ì •ê·œí™” â†’ Dropoutìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ â†’ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” â†’ Quantile ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì •â€™ì´ë¼ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤ì œ ë°ì´í„°ì—ì„œë„ íš¨ê³¼ì ì´ì—ˆìŠµë‹ˆë‹¤. ê°ì— ì˜ì¡´í•˜ì§€ ì•Šê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì ‘ê·¼í•˜ëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\në°ì´í„°ë¥¼ ì¤‘ì‹¬ì— ë‘ê¸°\nì‹¤ì œ ë°ì´í„°ëŠ” ë…¸ì´ì¦ˆì™€ ê³„ì ˆì„±ì´ ê°•í•˜ê²Œ ì„ì—¬ ìˆìŠµë‹ˆë‹¤. STL ë¶„í•´ì™€ ì´ë™í‰ê· ì„ ê²°í•©í•œ ë‹¤ë‹¨ê³„ ì „ì²˜ë¦¬ ë•ë¶„ì—, ëª¨ë¸ì´ ë³¸ì§ˆì ì¸ íŒ¨í„´ì— ì§‘ì¤‘í•  ìˆ˜ ìˆì—ˆê³ , ì´ìƒ ì‹ í˜¸ë„ ë” ëª…í™•í•˜ê²Œ ì¡ì•„ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê²°êµ­ ëª¨ë¸ë§ì˜ í•µì‹¬ì€ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì´í•´í•˜ê³  ë‹¤ë£¨ëŠëƒì— ë‹¬ë ¤ ìˆë‹¤ëŠ” ì ì„ ì‹¤ê°í–ˆìŠµë‹ˆë‹¤.\nâ€™ì •ìƒâ€™ì˜ ê¸°ì¤€ ê³ ë¯¼í•˜ê¸°\nì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” â€™ì •ìƒâ€™ì´ ë¬´ì—‡ì¸ì§€ ì •ì˜í•˜ëŠ” ê²ƒë¶€í„° ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ì˜ 20%~80% êµ¬ê°„ì„ ì •ìƒìœ¼ë¡œ ê°„ì£¼í•´ í•™ìŠµì— ì‚¬ìš©í–ˆê³ , ë•ë¶„ì— ëª¨ë¸ì´ ì „ì²´ ë³€ë™ì„±ì— íœ˜ë‘˜ë¦¬ì§€ ì•Šê³  ì§„ì§œ ì´ìƒ ì‹ í˜¸ì— ì§‘ì¤‘í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ìƒ íƒì§€ì—ì„œëŠ” ë„ë©”ì¸ ì§€ì‹ê³¼ í•©ë¦¬ì ì¸ ê°€ì •ì´ í•„ìˆ˜ë¼ëŠ” ì ë„ ë‹¤ì‹œ ëŠê¼ˆìŠµë‹ˆë‹¤.\n\nWeek 1ì˜ ë°ì´í„° íƒìƒ‰ë¶€í„° Week 5ì˜ ì‹¤ì œ ë°ì´í„° ì ìš©ê¹Œì§€, ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¼ëŠ” ëª©í‘œë¥¼ í–¥í•´ í•œ ë‹¨ê³„ì”© ë‚˜ì•„ê°”ìŠµë‹ˆë‹¤. ë‹¨ìˆœí•œ ë² ì´ìŠ¤ë¼ì¸ì—ì„œ ì¶œë°œí•´ ì ì§„ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ê°œì„ í•˜ê³ , ì‹¤ì œ ë°ì´í„°ì˜ ë³µì¡ì„±ê¹Œì§€ ë‹¤ë£° ìˆ˜ ìˆê²Œ ëœ ì´ë²ˆ ê³¼ì •ì€, ëª¨ë¸ë§ì´ ë‹¨ìˆœíˆ ì½”ë“œë¥¼ ì§œëŠ” ì¼ì´ ì•„ë‹ˆë¼ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³ , ê°€ì„¤ì„ ì„¸ìš°ê³ , ì‹¤í—˜í•˜ê³ , ê²€ì¦í•˜ëŠ” ì¼ë ¨ì˜ íƒêµ¬ë¼ëŠ” ì‚¬ì‹¤ì„ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í–ˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-02-time-series-anomaly/index.html",
    "href": "posts/project-abc-02-time-series-anomaly/index.html",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 2ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì ìš©",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš” ì´ë²ˆ í¬ìŠ¤íŠ¸ëŠ” ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸° 2ì£¼ì°¨ ì‹¤ìŠµ ê¸°ë¡ì…ë‹ˆë‹¤. ì§€ë‚œì£¼ì—” ì‹œê³„ì—´ ë°ì´í„° EDAë‘ ì „ì²˜ë¦¬ë§Œ í–ˆëŠ”ë°, ì´ë²ˆì—” ê°„ë‹¨í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ì´ìƒì¹˜ íƒì§€ ê¸°ë²•ì„ ì†Œê°œí•˜ë ¤ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-02-time-series-anomaly/index.html#ë°ì´í„°-ì¤€ë¹„",
    "href": "posts/project-abc-02-time-series-anomaly/index.html#ë°ì´í„°-ì¤€ë¹„",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 2ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì ìš©",
    "section": "1. ë°ì´í„° ì¤€ë¹„",
    "text": "1. ë°ì´í„° ì¤€ë¹„\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.random.seed(42)\nt = np.arange(0, 100, 1)\ny = np.sin(0.2 * t) + np.random.normal(0, 0.2, size=len(t))\n# ì—¬ëŸ¬ ìœ„ì¹˜ì— ì¸ìœ„ì ìœ¼ë¡œ ì´ìƒì¹˜ ì¶”ê°€\noutlier_indices = [15, 35, 55, 75, 90]\noutlier_values = [2, -2, 2.5, -2.5, 3]\nfor idx, val in zip(outlier_indices, outlier_values):\n    y[idx] += val\ndf = pd.DataFrame({'time': t, 'value': y})\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='ì‹œê³„ì—´ ë°ì´í„°')\nplt.scatter(df.loc[outlier_indices, 'time'], df.loc[outlier_indices, 'value'], color='red', label='ë¶€ì—¬í•œ ì´ìƒê°’')\nplt.legend()\nplt.title('ì´ìƒê°’ì´ í¬í•¨ëœ ì‹œê³„ì—´ ë°ì´í„°')\nplt.show()\n\n\n\n\nì´ìƒê°’ì´ í¬í•¨ëœ ì‹œê³„ì—´ ë°ì´í„°"
  },
  {
    "objectID": "posts/project-abc-02-time-series-anomaly/index.html#ë¨¸ì‹ ëŸ¬ë‹-ê¸°ë°˜-ì´ìƒ-íƒì§€-isolation-forest-dbscan-one-class-svm",
    "href": "posts/project-abc-02-time-series-anomaly/index.html#ë¨¸ì‹ ëŸ¬ë‹-ê¸°ë°˜-ì´ìƒ-íƒì§€-isolation-forest-dbscan-one-class-svm",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 2ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì ìš©",
    "section": "2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ (Isolation Forest, DBSCAN, One-Class SVM)",
    "text": "2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ (Isolation Forest, DBSCAN, One-Class SVM)\n\nëª¨ë¸ë³„ íŠ¹ì§• ë° í•œê³„\n\n\n\n\n\n\n\n\nëª¨ë¸\nì¥ì \ní•œê³„/ì£¼ì˜ì \n\n\n\n\nIsolation Forest\nëŒ€ìš©ëŸ‰/ê³ ì°¨ì› ë°ì´í„°ì— ê°•í•¨, ë¹ ë¦„\nì´ìƒì¹˜ ë¹„ìœ¨(contamination) ì¶”ì • í•„ìš”\n\n\nDBSCAN\nêµ°ì§‘/ë°€ë„ ê¸°ë°˜, íŒŒë¼ë¯¸í„° ì§ê´€ì \neps, min_samplesì— ë¯¼ê°, 1ì°¨ì› í•œê³„\n\n\nOne-Class SVM\në¹„ì„ í˜• ê²½ê³„, ì†Œê·œëª¨ ë°ì´í„°ì— ì í•©\nëŠë¦´ ìˆ˜ ìˆìŒ, íŒŒë¼ë¯¸í„° íŠœë‹ í•„ìš”\n\n\n\n\n\nIsolation Forest\n\nfrom sklearn.ensemble import IsolationForest\nmodel = IsolationForest(contamination=0.05, random_state=42)\ndf['anomaly_isof'] = model.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='ì‹œê³„ì—´ ë°ì´í„°')\nplt.scatter(df[df['anomaly_isof']==-1]['time'], df[df['anomaly_isof']==-1]['value'], color='red', label='íƒì§€ëœ ì´ìƒê°’')\nplt.legend()\nplt.title('Isolation Forest ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼')\nplt.show()\n\n\n\n\nIsolation Forest ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n\n\n\n\n\nDBSCAN (ë°€ë„ ê¸°ë°˜ ì´ìƒ íƒì§€)\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['value']])\ndbscan = DBSCAN(eps=0.25, min_samples=3)  # epsì™€ min_samplesë¥¼ ì¡°ì •í•´ ë¯¼ê°ë„ ì¡°ì •\ndf['anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='ì‹œê³„ì—´ ë°ì´í„°')\nplt.scatter(df[df['anomaly_dbscan']==-1]['time'], df[df['anomaly_dbscan']==-1]['value'], color='orange', label='íƒì§€ëœ ì´ìƒê°’(DBSCAN)')\nplt.legend()\nplt.title('DBSCAN ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼')\nplt.show()\n\n\n\n\nDBSCAN ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n\n\n\n\n\nOne-Class SVM (ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ê¸°ë°˜ ì´ìƒ íƒì§€)\n\nfrom sklearn.svm import OneClassSVM\n# ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œëŠ” ì´ìƒì¹˜ íƒì§€ê°€ ì˜ ì•ˆ ë¨ (F1ì´ 0.14 ìˆ˜ì¤€)\nocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\ndf['anomaly_ocsvm'] = ocsvm.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='ì‹œê³„ì—´ ë°ì´í„°')\nplt.scatter(df[df['anomaly_ocsvm']==-1]['time'], df[df['anomaly_ocsvm']==-1]['value'], color='purple', label='íƒì§€ëœ ì´ìƒê°’(OCSVM)')\nplt.legend()\nplt.title('One-Class SVM ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼')\nplt.show()\n\n\n\n\nOne-Class SVM ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n\n\n\n\nSVM íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œë„\n\n# gamma ê°’ì„ ë” í¬ê²Œ, nu ê°’ì„ ë” ë†’ê²Œ ì¡°ì •í•´ì„œ ë¯¼ê°ë„ë¥¼ ë†’ì„\nocsvm_tuned = OneClassSVM(nu=0.12, kernel='rbf', gamma=2)\ndf['anomaly_ocsvm_tuned'] = ocsvm_tuned.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='ì‹œê³„ì—´ ë°ì´í„°')\nplt.scatter(df[df['anomaly_ocsvm_tuned']==-1]['time'], df[df['anomaly_ocsvm_tuned']==-1]['value'], color='blue', label='íƒì§€ëœ ì´ìƒê°’(íŠœë‹ SVM)')\nplt.legend()\nplt.title('íŠœë‹ëœ One-Class SVM ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼')\nplt.show()\n\n\n\n\níŠœë‹ëœ One-Class SVM ì´ìƒ íƒì§€ ê²°ê³¼\n\n\n\n\n\n\n\nì´ìƒì¹˜ íƒì§€ ë° í‰ê°€ì§€í‘œ(Precision, Recall, F1)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef anomaly_metrics(true_outliers, pred_outliers, n):\n    true = [1 if i in true_outliers else 0 for i in range(n)]\n    pred = [1 if i in pred_outliers else 0 for i in range(n)]\n    p = precision_score(true, pred)\n    r = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    return p, r, f1\n\nn = len(df)\ntrue_outliers = outlier_indices\npred_isof = df.index[df['anomaly_isof']==-1].tolist()\np_isof, r_isof, f1_isof = anomaly_metrics(true_outliers, pred_isof, n)\npred_dbscan = df.index[df['anomaly_dbscan']==-1].tolist()\np_dbscan, r_dbscan, f1_dbscan = anomaly_metrics(true_outliers, pred_dbscan, n)\npred_ocsvm = df.index[df['anomaly_ocsvm']==-1].tolist()\np_ocsvm, r_ocsvm, f1_ocsvm = anomaly_metrics(true_outliers, pred_ocsvm, n)\npred_ocsvm_tuned = df.index[df['anomaly_ocsvm_tuned']==-1].tolist()\np_ocsvm_t, r_ocsvm_t, f1_ocsvm_t = anomaly_metrics(true_outliers, pred_ocsvm_tuned, n)\n\nprint(f\"Isolation Forest - Precision: {p_isof:.2f}, Recall: {r_isof:.2f}, F1: {f1_isof:.2f}\")\nprint(f\"DBSCAN           - Precision: {p_dbscan:.2f}, Recall: {r_dbscan:.2f}, F1: {f1_dbscan:.2f}\")\nprint(f\"One-Class SVM    - Precision: {p_ocsvm:.2f}, Recall: {r_ocsvm:.2f}, F1: {f1_ocsvm:.2f}\")\nprint(f\"íŠœë‹ SVM         - Precision: {p_ocsvm_t:.2f}, Recall: {r_ocsvm_t:.2f}, F1: {f1_ocsvm_t:.2f}\")\n\nIsolation Forest - Precision: 1.00, Recall: 1.00, F1: 1.00\nDBSCAN           - Precision: 1.00, Recall: 1.00, F1: 1.00\nOne-Class SVM    - Precision: 0.14, Recall: 0.40, F1: 0.21\níŠœë‹ SVM         - Precision: 0.21, Recall: 1.00, F1: 0.34"
  },
  {
    "objectID": "posts/project-abc-02-time-series-anomaly/index.html#ê²°ê³¼-í•´ì„-ë°-ì •ë¦¬",
    "href": "posts/project-abc-02-time-series-anomaly/index.html#ê²°ê³¼-í•´ì„-ë°-ì •ë¦¬",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 2ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì ìš©",
    "section": "3. ê²°ê³¼ í•´ì„ ë° ì •ë¦¬",
    "text": "3. ê²°ê³¼ í•´ì„ ë° ì •ë¦¬\n\nOne-Class SVMì€ ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¡œëŠ” ì´ìƒì¹˜ íƒì§€ê°€ ì˜ ë˜ì§€ ì•Šì•˜ìœ¼ë‚˜, gammaì™€ nuë¥¼ ì¡°ì •í•´ íŠœë‹í•˜ë©´ ì„±ëŠ¥ì´ ê°œì„ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ ê³¼ì •ì—ì„œ íŒŒë¼ë¯¸í„° íŠœë‹ì˜ ì¤‘ìš”ì„±ì„ ê²½í—˜í–ˆë‹¤.\nê° ëª¨ë¸ë³„ë¡œ ì´ìƒì¹˜ íƒì§€ ê²°ê³¼ì™€ í‰ê°€ì§€í‘œ(Precision, Recall, F1)ê°€ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚œë‹¤. Isolation ForestëŠ” ì¸ìœ„ì ìœ¼ë¡œ ë„£ì€ ì´ìƒì¹˜ë¥¼ ëŒ€ë¶€ë¶„ íƒì§€í–ˆê³ , DBSCANì€ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤. One-Class SVMì€ ë°ì´í„° ë¶„í¬ì™€ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§„ë‹¤.\nPrecision(ì •ë°€ë„), Recall(ì¬í˜„ìœ¨), F1-scoreëŠ” ëª¨ë¸ì˜ ì´ìƒì¹˜ íƒì§€ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì§€í‘œë¡œ, ì‹¤ì œ ë°ì´í„° ë¶„ì„ì—ì„œëŠ” ì—¬ëŸ¬ ë°©ë²•ì„ ë¹„êµí•˜ê³  ë„ë©”ì¸ ì§€ì‹ê³¼ í•¨ê»˜ í•´ì„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤."
  },
  {
    "objectID": "posts/project-abc-01-data-analysis/index.html",
    "href": "posts/project-abc-01-data-analysis/index.html",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬",
    "section": "",
    "text": "ìœ í´ë¦¬ë“œì†Œí”„íŠ¸ì—ì„œ ì§„í–‰í•˜ëŠ” ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ì— 8ê¸°ë¡œ ì°¸ì—¬í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. Â  [ì‚°ì—… ì „ë ¥ ë°ì´í„°ì˜ ì´ìƒì¹˜ íƒì§€ ì„±ëŠ¥ í–¥ìƒ ì†”ë£¨ì…˜ êµ¬ì¶•]ì„ ì£¼ì œë¡œ ë‹¤ì–‘í•œ ë°ì´í„° ë¶„ì„ ë° ì¸ê³µì§€ëŠ¥ ê¸°ë²•ì„ í•™ìŠµí•˜ê³  ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì ìš©í•´ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-01-data-analysis/index.html#ì‹œê³„ì—´-ë°ì´í„°ë€",
    "href": "posts/project-abc-01-data-analysis/index.html#ì‹œê³„ì—´-ë°ì´í„°ë€",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬",
    "section": "ì‹œê³„ì—´ ë°ì´í„°ë€?",
    "text": "ì‹œê³„ì—´ ë°ì´í„°ë€?\nì‹œê³„ì—´ ë°ì´í„°(Time Series Data)ëŠ” ì¼ì • ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ê¸°ë¡ëœ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ ìˆœì°¨ì ì¸ ì§‘í•©ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹œê°„ë³„ ì‚°ì—… ì„¤ë¹„ì˜ ì „ë ¥ ì‚¬ìš©ëŸ‰, ì¼ë³„ ì£¼ê°€, ì›”ë³„ ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ì ìˆ˜ ë“±ì´ ì‹œê³„ì—´ ë°ì´í„°ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ëŠ” ì‹œê°„ì˜ íë¦„ì— ë”°ë¥¸ ë³€í™”ì™€ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, íŠ¹íˆ ì •ìƒì ì¸ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚˜ëŠ” â€™ì´ìƒì¹˜â€™ë¥¼ íƒì§€í•˜ëŠ” ë° ì¤‘ìš”í•œ ê¸°ì´ˆ ìë£Œê°€ ë©ë‹ˆë‹¤.\nì‹œê³„ì—´ ë°ì´í„°ëŠ” ì£¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§‘ë‹ˆë‹¤:\n\nì¶”ì„¸ (Trend): ë°ì´í„°ê°€ ì¥ê¸°ì ìœ¼ë¡œ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê²½í–¥.\nê³„ì ˆì„± (Seasonality): íŠ¹ì • ì£¼ê¸°(ì˜ˆ: í•˜ë£¨, ì£¼, ì›”)ì— ë”°ë¼ ë°˜ë³µë˜ëŠ” íŒ¨í„´.\nì£¼ê¸°ì„± (Cyclicality): ê³„ì ˆì„±ë³´ë‹¤ ê¸´, ê³ ì •ë˜ì§€ ì•Šì€ ì£¼ê¸°ì˜ ë³€ë™.\në¶ˆê·œì¹™ ë³€ë™ (Irregular Fluctuations/Noise): ìœ„ ìš”ì†Œë“¤ë¡œ ì„¤ëª…ë˜ì§€ ì•ŠëŠ” ë¬´ì‘ìœ„ì  ë³€ë™."
  },
  {
    "objectID": "posts/project-abc-01-data-analysis/index.html#ì‹œê³„ì—´-ì´ìƒ-íƒì§€ì—ì„œ-edaì™€-ì „ì²˜ë¦¬ì˜-ì¤‘ìš”ì„±",
    "href": "posts/project-abc-01-data-analysis/index.html#ì‹œê³„ì—´-ì´ìƒ-íƒì§€ì—ì„œ-edaì™€-ì „ì²˜ë¦¬ì˜-ì¤‘ìš”ì„±",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬",
    "section": "ì‹œê³„ì—´ ì´ìƒ íƒì§€ì—ì„œ EDAì™€ ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±",
    "text": "ì‹œê³„ì—´ ì´ìƒ íƒì§€ì—ì„œ EDAì™€ ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±\nì´ìƒì¹˜(Anomaly) ë˜ëŠ” íŠ¹ì´ì (Outlier)ì€ ì¼ë°˜ì ì¸ ë°ì´í„° íŒ¨í„´ì—ì„œ í˜„ì €í•˜ê²Œ ë²—ì–´ë‚˜ëŠ” ê´€ì¸¡ì¹˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì‚°ì—… ì „ë ¥ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ëŠ” ì„¤ë¹„ ê³ ì¥, ì—ë„ˆì§€ ëˆ„ìˆ˜, ë¹„ì •ìƒì  ê³µì • ìš´ì˜ ë“± ì¤‘ìš”í•œ ë¬¸ì œë¥¼ ì‹œì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íš¨ê³¼ì ì¸ ì´ìƒ íƒì§€ë¥¼ ìœ„í•´ì„œëŠ” ë°ì´í„°ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì´í•´ê°€ ì„ í–‰ë˜ì–´ì•¼ í•˜ë©°, íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)ê³¼ ì ì ˆí•œ ì „ì²˜ë¦¬ëŠ” ì´ ê³¼ì •ì˜ í•µì‹¬ì…ë‹ˆë‹¤.\nEDAì™€ ì „ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìœ :\n\në°ì´í„° íŠ¹ì„± íŒŒì•…: ë°ì´í„°ì˜ ë¶„í¬, ì¶”ì„¸, ê³„ì ˆì„± ë“± ê¸°ë³¸ì ì¸ í†µê³„ì  íŠ¹ì„±ì„ ì´í•´í•˜ì—¬ â€˜ì •ìƒâ€™ ìƒíƒœì˜ ê¸°ì¤€ì„ ì„¤ì •í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\nì ì¬ì  ì´ìƒì¹˜ ì‹ë³„: ì‹œê°í™” ë“±ì„ í†µí•´ ì˜ˆìƒì¹˜ ëª»í•œ ê¸‰ì¦, ê¸‰ê° ë˜ëŠ” íŒ¨í„´ ë³€í™”ë¥¼ ì´ˆê¸°ì— ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në°ì´í„° í’ˆì§ˆ í–¥ìƒ: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ë…¸ì´ì¦ˆ ì œê±° ë“±ì„ í†µí•´ ë¶„ì„ì˜ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\ní”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ê¸°ë°˜ ë§ˆë ¨: ë¶„ì„ ëª©ì ì— ë§ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ë³€ìˆ˜ë¥¼ ë³€í™˜í•˜ëŠ” ë° í•„ìš”í•œ í†µì°°ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\nì ì ˆí•œ ì´ìƒ íƒì§€ ëª¨ë¸ ì„ íƒ ì§€ì›: ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ëŠ” ì´ìƒ íƒì§€ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒí•˜ëŠ” ë° ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-01-data-analysis/index.html#pythonì„-ì´ìš©í•œ-ì‹œê³„ì—´-ë°ì´í„°-eda-ë°-ì „ì²˜ë¦¬-ê¸°ì´ˆ",
    "href": "posts/project-abc-01-data-analysis/index.html#pythonì„-ì´ìš©í•œ-ì‹œê³„ì—´-ë°ì´í„°-eda-ë°-ì „ì²˜ë¦¬-ê¸°ì´ˆ",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬",
    "section": "Pythonì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° EDA ë° ì „ì²˜ë¦¬ ê¸°ì´ˆ",
    "text": "Pythonì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° EDA ë° ì „ì²˜ë¦¬ ê¸°ì´ˆ\nPythonì˜ pandas, numpy, matplotlib, seaborn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ê°€ì •í•˜ê³ , ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ì ì¸ EDA ë° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n\n1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°\në°ì´í„° ë¶„ì„ ë° ì‹œê°í™”ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # í–¥ìƒëœ ì‹œê°í™”ë¥¼ ìœ„í•œ Seaborn\nfrom datetime import datetime\n\n# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ (ì„ íƒ ì‚¬í•­)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n2. ë¶„ì„ìš© ìƒ˜í”Œ ì‹œê³„ì—´ ë°ì´í„° ìƒì„± (ê°€ìƒ ì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰)\nì‹¤ì œ ì‚°ì—… ì „ë ¥ ë°ì´í„°ì™€ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ê°–ë„ë¡ ê°€ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ëŠ” ì¼ì •í•œ ê¸°ë³¸ ì‚¬ìš©ëŸ‰, ì•½ê°„ì˜ ì¦ê°€ ì¶”ì„¸, ì£¼ê°„ ê³„ì ˆì„±(í‰ì¼ ì‚¬ìš©ëŸ‰ ì¦ê°€, ì£¼ë§ ê°ì†Œ), ê·¸ë¦¬ê³  ëª‡ ê°œì˜ ì¸ìœ„ì ì¸ ì´ìƒì¹˜(ìŠ¤íŒŒì´í¬ ë° ê¸‰ê°)ë¥¼ í¬í•¨ì‹œí‚µë‹ˆë‹¤.\n\n# ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\nnp.random.seed(42)\n\n# ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì•½ 1ë…„)\ndate_rng = pd.date_range(start='2025-01-01', periods=365, freq='D')\ndata = pd.DataFrame(date_rng, columns=['date'])\n\n# ê¸°ë³¸ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì„¤ì • ë° ì¶”ì„¸ ìƒì„±\nbaseline_usage = 100  # ê¸°ë³¸ ì‚¬ìš©ëŸ‰ (ì˜ˆ: kWh)\ntrend_factor = np.linspace(0, 20, len(date_rng)) # ì„ í˜• ì¦ê°€ ì¶”ì„¸\n\n# ì£¼ê°„ ê³„ì ˆì„± ìƒì„± (ì›”:0 ~ ì¼:6)\n# ì‚°ì—… ë°ì´í„° íŠ¹ì„±ìƒ í‰ì¼ ì‚¬ìš©ëŸ‰ ë†’ê³ , ì£¼ë§ ë‚®ìŒ\nday_of_week_effect = np.array([15, 18, 20, 19, 17, 5, 3])\nseasonal_factor = np.array([day_of_week_effect[day.weekday()] for day in date_rng])\n\n# ì„ì˜ì˜ ë…¸ì´ì¦ˆ ìƒì„±\nnoise = np.random.normal(0, 5, size=(len(date_rng))) # í‰ê·  0, í‘œì¤€í¸ì°¨ 5\n\n# ë°ì´í„° ìƒì„± (ì „ë ¥ ì‚¬ìš©ëŸ‰ = ê¸°ë³¸ê°’ + ì¶”ì„¸ + ê³„ì ˆì„± + ë…¸ì´ì¦ˆ)\ndata['power_usage'] = baseline_usage + trend_factor + seasonal_factor + noise\n\n# ì¸ìœ„ì ì¸ ì´ìƒì¹˜(ìŠ¤íŒŒì´í¬ ë° ê¸‰ê°) ì¶”ê°€\ndata.loc[data.index[50], 'power_usage'] += 70  # 51ë²ˆì§¸ ë‚ ì— í° ìŠ¤íŒŒì´í¬\ndata.loc[data.index[150], 'power_usage'] -= 50 # 151ë²ˆì§¸ ë‚ ì— í° í­ í•˜ë½\ndata.loc[data.index[250], 'power_usage'] += 80  # 251ë²ˆì§¸ ë‚ ì— í° ìŠ¤íŒŒì´í¬\n\n# ë°ì´í„° ê°’ ë³´ì • (ìŒìˆ˜ ë°©ì§€ ë° ìµœì†Œê°’ ì„¤ì •)\ndata['power_usage'] = data['power_usage'].astype(float).clip(lower=10)\n\n# 'date' ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •\ndata.set_index('date', inplace=True)\n\nprint(\"ìƒì„±ëœ ê°€ìƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\")\nprint(data.head())\nprint(\"\\nìƒì„±ëœ ê°€ìƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œ (í•˜ìœ„ 5ê°œ):\")\nprint(data.tail())\n\nìƒì„±ëœ ê°€ìƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œ (ìƒìœ„ 5ê°œ):\n            power_usage\ndate                   \n2025-01-01   122.483571\n2025-01-02   118.363624\n2025-01-03   120.348333\n2025-01-04   112.779984\n2025-01-05   102.049013\n\nìƒì„±ëœ ê°€ìƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œ (í•˜ìœ„ 5ê°œ):\n            power_usage\ndate                   \n2025-12-27   127.376952\n2025-12-28   130.498859\n2025-12-29   134.346309\n2025-12-30   139.953614\n2025-12-31   143.450720\n\n\nì´ ìƒ˜í”Œ ë°ì´í„°ëŠ” power_usageë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ê°€ì§€ë©°, EDA ê³¼ì •ì—ì„œ ì´ìƒì¹˜ë¥¼ ì‹œê°ì ìœ¼ë¡œ íƒìƒ‰í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n\n\n3. ë°ì´í„° ê¸°ë³¸ íƒìƒ‰\në°ì´í„°ì˜ êµ¬ì¡°ì™€ ê¸°ë³¸ì ì¸ í†µê³„ì  íŠ¹ì„±ì„ í™•ì¸í•©ë‹ˆë‹¤.\n\nprint(\"ë°ì´í„° ì •ë³´:\")\ndata.info()\n\nprint(\"\\nê¸°ìˆ  í†µê³„ëŸ‰:\")\nprint(data.describe())\n\nprint(f\"\\nê²°ì¸¡ì¹˜ í™•ì¸: {data.isnull().sum().sum()} ê°œ\")\n# data.isnull().sum() # ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ í™•ì¸\n\në°ì´í„° ì •ë³´:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 365 entries, 2025-01-01 to 2025-12-31\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   power_usage  365 non-null    float64\ndtypes: float64(1)\nmemory usage: 5.7 KB\n\nê¸°ìˆ  í†µê³„ëŸ‰:\n       power_usage\ncount   365.000000\nmean    124.197677\nstd      11.877300\nmin      64.494222\n25%     117.423353\n50%     124.093359\n75%     131.043033\nmax     202.431844\n\nê²°ì¸¡ì¹˜ í™•ì¸: 0 ê°œ\n\n\ninfo()ëŠ” ë°ì´í„° íƒ€ì…, ì¸ë±ìŠ¤ ì •ë³´, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë“±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. describe()ëŠ” í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€ê°’, ì‚¬ë¶„ìœ„ìˆ˜ ë“± ì£¼ìš” ê¸°ìˆ  í†µê³„ëŸ‰ì„ ì œê³µí•˜ì—¬ ë°ì´í„°ì˜ ì „ë°˜ì ì¸ ë¶„í¬ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ê°€ ìˆë‹¤ë©´ ì´ìƒ íƒì§€ ë¶„ì„ ì „ì— ì ì ˆíˆ ì²˜ë¦¬(ì˜ˆ: ë³´ê°„, ì œê±°)í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ìƒ˜í”Œì—ì„œëŠ” ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n\n\n4. ì£¼ìš” ì‹œê°í™”ë¥¼ í†µí•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ (EDA)\nì‹œê°í™”ëŠ” ì‹œê³„ì—´ ë°ì´í„°ì˜ íŒ¨í„´ê³¼ ì ì¬ì  ì´ìƒì¹˜ë¥¼ ë°œê²¬í•˜ëŠ” ë° ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.\n\n4.1. ê¸°ë³¸ ì‹œê³„ì—´ í”Œë¡¯\nì „ì²´ ê¸°ê°„ì— ëŒ€í•œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë³€í™”ë¥¼ ì‹œê°í™”í•˜ì—¬ ì¶”ì„¸, ê³„ì ˆì„±, ê·¸ë¦¬ê³  ëˆˆì— ë„ëŠ” ì´ìƒ íŒ¨í„´ì„ ê´€ì°°í•©ë‹ˆë‹¤.\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='ì¼ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰', color='dodgerblue', linewidth=1.5)\nplt.title('ì¼ë³„ ê°€ìƒ ì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰', fontsize=16)\nplt.xlabel('ë‚ ì§œ', fontsize=12)\nplt.ylabel('ì „ë ¥ ì‚¬ìš©ëŸ‰ (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 1: ì¼ë³„ ê°€ìƒ ì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰\n\n\n\n\n\nìœ„ ê·¸ë˜í”„ì—ì„œ ì „ë°˜ì ì¸ ì¦ê°€ ì¶”ì„¸ì™€ ì£¼ê¸°ì ì¸ ë³€ë™(ê³„ì ˆì„±) ì™¸ì—ë„, ëª‡ëª‡ ì§€ì ì—ì„œ ê¸‰ê²©í•œ ìŠ¤íŒŒì´í¬ë‚˜ í•˜ë½(ìš°ë¦¬ê°€ ì‚½ì…í•œ ì´ìƒì¹˜)ì´ ì‹œê°ì ìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„° ë¶„ì„ ì‹œ ì´ëŸ¬í•œ ì§€ì ë“¤ì´ ì¡°ì‚¬ ëŒ€ìƒì´ ë©ë‹ˆë‹¤.\n\n\n\n4.2. ë°ì´í„° ë¶„í¬ í™•ì¸ (íˆìŠ¤í† ê·¸ë¨ ë° KDE)\nì „ë ¥ ì‚¬ìš©ëŸ‰ ê°’ë“¤ì˜ ë¶„í¬ë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„°ê°€ íŠ¹ì • êµ¬ê°„ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ”ì§€, ë˜ëŠ” ë¶„í¬ì—ì„œ ë²—ì–´ë‚˜ëŠ” ê°’ë“¤ì´ ìˆëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data['power_usage'], kde=True, color='mediumseagreen', bins=30)\nplt.title('ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨ ë° KDE)', fontsize=16)\nplt.xlabel('ì „ë ¥ ì‚¬ìš©ëŸ‰ (kWh)', fontsize=12)\nplt.ylabel('ë¹ˆë„', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 2: ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬\n\n\n\n\n\níˆìŠ¤í† ê·¸ë¨ê³¼ KDE(Kernel Density Estimate) í”Œë¡¯ì€ ë°ì´í„° ê°’ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ë§Œì•½ ë¶„í¬ì˜ ê¼¬ë¦¬ ë¶€ë¶„ì— ê°’ì´ ë“œë¬¼ê²Œ ë‚˜íƒ€ë‚œë‹¤ë©´ ì´ëŠ” ì´ìƒì¹˜ì¼ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚½ì…í•œ ì¸ìœ„ì ì¸ ìŠ¤íŒŒì´í¬ ê°’ë“¤ì´ ë¶„í¬ì˜ ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ ë¶€ë¶„ì— ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n4.3. ì£¼ê¸°ì  íŒ¨í„´ í™•ì¸ (ìš”ì¼ë³„ Box Plot)\nì‚°ì—… ë°ì´í„°ëŠ” ìš”ì¼ì´ë‚˜ ì›”ë³„ë¡œ ëšœë ·í•œ ì£¼ê¸°ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Box plotì„ ì‚¬ìš©í•˜ë©´ ì´ëŸ¬í•œ ì£¼ê¸°ì„± ë‚´ì—ì„œ í‰ì†Œì™€ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì´ëŠ” ì‹œì ì„ íŒŒì•…í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n\n# ë¶„ì„ì„ ìœ„í•´ 'day_of_week' ì»¬ëŸ¼ ì¶”ê°€ (ì›”ìš”ì¼=0, ì¼ìš”ì¼=6)\ndata['day_of_week'] = data.index.dayofweek\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='day_of_week', y='power_usage', data=data, palette='coolwarm')\nplt.title('ìš”ì¼ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬', fontsize=16)\nplt.xlabel('ìš”ì¼ (0:ì›”, 1:í™”, 2:ìˆ˜, 3:ëª©, 4:ê¸ˆ, 5:í† , 6:ì¼)', fontsize=12)\nplt.ylabel('ì „ë ¥ ì‚¬ìš©ëŸ‰ (kWh)', fontsize=12)\nplt.xticks(ticks=range(7), labels=['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼'])\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 3: ìš”ì¼ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬\n\n\n\n\n\nìš”ì¼ë³„ Box plotì€ ê° ìš”ì¼ì˜ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë¶„í¬ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ë°•ìŠ¤ëŠ” í•´ë‹¹ ìš”ì¼ ë°ì´í„°ì˜ ì¤‘ì•™ 50%(IQR: Interquartile Range)ë¥¼ ë‚˜íƒ€ë‚´ë©°, ë°•ìŠ¤ ì™¸ë¶€ì˜ ì ë“¤ì€ ì ì¬ì ì¸ ì´ìƒì¹˜(outliers)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ ìƒ˜í”Œì—ì„œëŠ” ì£¼ë§(í† , ì¼) ì‚¬ìš©ëŸ‰ì´ í‰ì¼ë³´ë‹¤ ë‚®ì€ íŒ¨í„´ì´ ëšœë ·í•˜ë©°, ìš°ë¦¬ê°€ ì¸ìœ„ì ìœ¼ë¡œ ì‚½ì…í•œ ì´ìƒì¹˜ë“¤ì´ íŠ¹ì • ìš”ì¼ì˜ ì¼ë°˜ì ì¸ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ ì ìœ¼ë¡œ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í™”ìš”ì¼(1)ì— ë°œìƒì‹œí‚¨ ìŠ¤íŒŒì´í¬ëŠ” í™”ìš”ì¼ì˜ ë°•ìŠ¤ í”Œë¡¯ì—ì„œ ìƒë‹¨ ì´ìƒì¹˜ë¡œ ë‚˜íƒ€ë‚  ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤.\n\n\n\n\n5. ì´ë™ í‰ê· ì„ í™œìš©í•œ ì¶”ì„¸ ë° ë³€ë™ì„± ê´€ì°°\nì´ë™ í‰ê· (Moving Average)ì€ ë‹¨ê¸°ì ì¸ ë³€ë™ì„ ì™„í™”í•˜ì—¬ ì¥ê¸°ì ì¸ ì¶”ì„¸ë¥¼ íŒŒì•…í•˜ê±°ë‚˜, ë°ì´í„°ì˜ ì¼ë°˜ì ì¸ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ê¸°ì¤€ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°ì™€ ì´ë™ í‰ê· ì„ ì„ í•¨ê»˜ ì‹œê°í™”í•˜ë©´, ì´ë™ í‰ê· ì—ì„œ í¬ê²Œ ë²—ì–´ë‚˜ëŠ” ì§€ì ë“¤ì„ ì´ìƒì¹˜ í›„ë³´ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n# 7ì¼ ì´ë™ í‰ê·  ê³„ì‚°\ndata['rolling_mean_7'] = data['power_usage'].rolling(window=7, center=True).mean() # center=Trueë¡œ ì„¤ì •í•˜ì—¬ lag ê°ì†Œ íš¨ê³¼\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='ì¼ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰', color='lightskyblue', alpha=0.8, linewidth=1)\nplt.plot(data.index, data['rolling_mean_7'], label='7ì¼ ì´ë™ í‰ê·  (ì¤‘ì•™ ì •ë ¬)', color='orangered', linewidth=2)\nplt.title('ì¼ë³„ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë° 7ì¼ ì´ë™ í‰ê· ', fontsize=16)\nplt.xlabel('ë‚ ì§œ', fontsize=12)\nplt.ylabel('ì „ë ¥ ì‚¬ìš©ëŸ‰ (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 4: ì „ë ¥ ì‚¬ìš©ëŸ‰ê³¼ 7ì¼ ì´ë™ í‰ê· \n\n\n\n\n\n\n# ì´ë™ í‰ê· ê³¼ì˜ ì°¨ì´(ì”ì°¨ì™€ ìœ ì‚¬í•œ ê°œë…)ë¥¼ í†µí•´ ì´ìƒì¹˜ ê°•ì¡°\ndata['deviation_from_ma'] = data['power_usage'] - data['rolling_mean_7']\n\nplt.figure(figsize=(9,5))\nplt.plot(data.index, data['deviation_from_ma'], label='ì´ë™ í‰ê· ê³¼ì˜ í¸ì°¨', color='teal', linewidth=1, marker='o', markersize=3, linestyle='None')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8) # ê¸°ì¤€ì„ \n\n# í¸ì°¨ì˜ ì„ê³„ê°’ì„ ì„¤ì •í•˜ì—¬ ì´ìƒì¹˜ í›„ë³´ ì‹œê°í™” (ì˜ˆ: í¸ì°¨ì˜ í‘œì¤€í¸ì°¨ ê¸°ë°˜)\n# ì´ë™ í‰ê·  ê³„ì‚° ì‹œ ì´ˆë°˜/í›„ë°˜ NaN ê°’ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ dropna() ì‚¬ìš©\ndeviation_std = data['deviation_from_ma'].dropna().std()\nupper_threshold = 3 * deviation_std\nlower_threshold = -3 * deviation_std\n\nplt.axhline(upper_threshold, color='red', linestyle=':', linewidth=1.5, label=f'+3Ïƒ ({upper_threshold:.2f})')\nplt.axhline(lower_threshold, color='red', linestyle=':', linewidth=1.5, label=f'-3Ïƒ ({lower_threshold:.2f})')\nplt.title('ì´ë™ í‰ê· ê³¼ì˜ í¸ì°¨ (ì´ìƒì¹˜ íƒìƒ‰ ë³´ì¡°)', fontsize=16)\nplt.xlabel('ë‚ ì§œ', fontsize=12)\nplt.ylabel('í¸ì°¨ (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 5: ì´ë™ í‰ê· ê³¼ì˜ í¸ì°¨ (ì´ìƒì¹˜ íƒìƒ‰ ë³´ì¡°)\n\n\n\n\n\n7ì¼ ì´ë™ í‰ê· ì„ ì€ ë°ì´í„°ì˜ ë‹¨ê¸°ì  ë³€ë™ì„ í‰íƒ„í™”í•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤. rolling() í•¨ìˆ˜ì—ì„œ center=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì´ë™ í‰ê·  ê³„ì‚° ì‹œ ìœˆë„ìš°ì˜ ì¤‘ì•™ì— ê°’ì„ ìœ„ì¹˜ì‹œì¼œ ì‹œê°í™” ì‹œ ì›ë³¸ ë°ì´í„°ì™€ì˜ ì§€ì—°(lag)ì„ ì¤„ì´ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.\në‘ ë²ˆì§¸ ê·¸ë˜í”„ëŠ” ì›ë³¸ ë°ì´í„°ì™€ ì´ë™ í‰ê· ê³¼ì˜ í¸ì°¨ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ í¸ì°¨ê°€ íŠ¹ì • ì„ê³„ê°’(ì˜ˆ: í¸ì°¨ì˜ 3 í‘œì¤€í¸ì°¨, Â±3Ïƒ)ì„ ë„˜ì–´ì„œëŠ” ì§€ì ë“¤ì€ ì ì¬ì ì¸ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ ì‚½ì…í•œ ì¸ìœ„ì ì¸ ìŠ¤íŒŒì´í¬ì™€ ê¸‰ê° ì§€ì ì—ì„œ í¸ì°¨ê°€ í¬ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ì´ìƒì¹˜ íƒìƒ‰ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-01-data-analysis/index.html#ìš”ì•½",
    "href": "posts/project-abc-01-data-analysis/index.html#ìš”ì•½",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬",
    "section": "ìš”ì•½",
    "text": "ìš”ì•½\nì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Pythonì„ ì‚¬ìš©í•˜ì—¬ ê°€ìƒì˜ ì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ì ì¸ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA) ë° ì „ì²˜ë¦¬ ê³¼ì •ì„ ì‚´í´ë³´ì•˜ìŠµë‹ˆë‹¤. ì‹œê³„ì—´ í”Œë¡¯, ë¶„í¬ í™•ì¸, ì£¼ê¸°ì„± ë¶„ì„(ìš”ì¼ë³„ Box Plot), ì´ë™ í‰ê·  í™œìš© ë“±ì€ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³  ì ì¬ì ì¸ ì´ìƒì¹˜ë¥¼ ì‹ë³„í•˜ëŠ” ë° íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-03-cnn-baseline/index.html",
    "href": "posts/project-abc-03-cnn-baseline/index.html",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸° ì„¸ ë²ˆì§¸ ê¸°ìˆ ë…¸íŠ¸ì…ë‹ˆë‹¤. ì´ë²ˆ ì£¼ëŠ” ì‹œê³„ì—´ ë°ì´í„°ì˜ â€™íŒ¨í„´â€™ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë”¥ëŸ¬ë‹, ê·¸ì¤‘ì—ì„œë„ CNNì„ í™œìš©í•œ ì´ìƒ íƒì§€ì˜ ì²«ê±¸ìŒì„ PyTorchë¡œ êµ¬í˜„í•´ ë³´ê² ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-03-cnn-baseline/index.html#ì‹œê³„ì—´-ë°ì´í„°ë¥¼-cnnì—-ì…ë ¥í•˜ëŠ”-ë°©ë²•-ìœˆë„ì‰windowing",
    "href": "posts/project-abc-03-cnn-baseline/index.html#ì‹œê³„ì—´-ë°ì´í„°ë¥¼-cnnì—-ì…ë ¥í•˜ëŠ”-ë°©ë²•-ìœˆë„ì‰windowing",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)",
    "section": "1. ì‹œê³„ì—´ ë°ì´í„°ë¥¼ CNNì— ì…ë ¥í•˜ëŠ” ë°©ë²•: ìœˆë„ì‰(Windowing)",
    "text": "1. ì‹œê³„ì—´ ë°ì´í„°ë¥¼ CNNì— ì…ë ¥í•˜ëŠ” ë°©ë²•: ìœˆë„ì‰(Windowing)\nì‹œê³„ì—´ ë°ì´í„°ë¥¼ CNN ëª¨ë¸ì— ì…ë ¥í•˜ë ¤ë©´ ì—°ì†ëœ ë°ì´í„°ë¥¼ ì¼ì •í•œ ê¸¸ì´ì˜ ì¡°ê°(window)ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” â€˜ìŠ¬ë¼ì´ë”© ìœˆë„ìš°â€™ ê¸°ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë°ì´í„°ì˜ ì‹œê°„ì  íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.\n\nìŠ¬ë¼ì´ë”© ìœˆë„ìš° êµ¬í˜„\nì•„ë˜ëŠ” numpyë¥¼ ì‚¬ìš©í•´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ êµ¬í˜„í•˜ëŠ” ê°„ë‹¨í•œ Python í•¨ìˆ˜ì…ë‹ˆë‹¤:\n\nimport numpy as np\n\ndef sliding_window(data, window_size, step_size=1):\n    \"\"\"ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ ë³€í™˜\"\"\"\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# ì˜ˆì œ ë°ì´í„°\ndata = np.sin(np.linspace(0, 20, 100))\nwindowed_data = sliding_window(data, window_size=10)\nprint(\"ìœˆë„ìš° í˜•íƒœ:\", windowed_data.shape)\n\nìœˆë„ìš° í˜•íƒœ: (91, 10)"
  },
  {
    "objectID": "posts/project-abc-03-cnn-baseline/index.html#ê¸°ë³¸-ì´ìƒ-íƒì§€-ëª¨ë¸-cnn-ì˜¤í† ì¸ì½”ë”-autoencoder",
    "href": "posts/project-abc-03-cnn-baseline/index.html#ê¸°ë³¸-ì´ìƒ-íƒì§€-ëª¨ë¸-cnn-ì˜¤í† ì¸ì½”ë”-autoencoder",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)",
    "section": "2. ê¸°ë³¸ ì´ìƒ íƒì§€ ëª¨ë¸: CNN ì˜¤í† ì¸ì½”ë” (Autoencoder)",
    "text": "2. ê¸°ë³¸ ì´ìƒ íƒì§€ ëª¨ë¸: CNN ì˜¤í† ì¸ì½”ë” (Autoencoder)\n\nì˜¤í† ì¸ì½”ë”ë€?\nì˜¤í† ì¸ì½”ë”ëŠ” ë°ì´í„°ë¥¼ ì••ì¶•(ì¸ì½”ë”)í–ˆë‹¤ê°€ ë‹¤ì‹œ ë³µì›(ë””ì½”ë”)í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì…ë‹ˆë‹¤. ì •ìƒ ë°ì´í„°ëŠ” ì˜ ë³µì›ë˜ì§€ë§Œ, ì´ìƒ ë°ì´í„°ëŠ” ë³µì›ì´ ì˜ ë˜ì§€ ì•Šì•„ ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ì»¤ì§€ëŠ” íŠ¹ì§•ì„ í™œìš©í•©ë‹ˆë‹¤.\n\n\nëª¨ë¸ êµ¬ì¡°\n\nì¸ì½”ë” (Encoder): Conv1Dì™€ MaxPooling1D ì¸µì„ ì‚¬ìš©í•´ ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ì••ì¶•í•©ë‹ˆë‹¤.\në””ì½”ë” (Decoder): ConvTranspose1D (ë˜ëŠ” Upsample + Conv1D) ì¸µì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë³µì›í•©ë‹ˆë‹¤.\n\n\n\nPyTorch êµ¬í˜„\nì•„ë˜ëŠ” PyTorchë¥¼ ì‚¬ìš©í•œ ê°„ë‹¨í•œ 1D CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ êµ¬í˜„ì…ë‹ˆë‹¤:\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoder(nn.Module):\n    def __init__(self, input_shape): # input_shape: (sequence_length, num_features)\n        super(CNNAutoencoder, self).__init__()\n        # Encoder\n        # input_shape[1]ì€ íŠ¹ì„± ìˆ˜ (in_channelsë¡œ ì‚¬ìš©)\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # ì‹œí€€ìŠ¤ ê¸¸ì´ 1/2ë¡œ ê°ì†Œ\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # ì‹œí€€ìŠ¤ ê¸¸ì´ 1/4ë¡œ ê°ì†Œ\n\n        # Decoder\n        # ì¸ì½”ë”ì—ì„œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 1/4ë¡œ ì¤„ì—ˆìœ¼ë¯€ë¡œ, ë””ì½”ë”ì—ì„œ ì›ë˜ ê¸¸ì´ë¡œ ë³µì›\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1) # ì›ë³¸ íŠ¹ì„± ìˆ˜ë¡œ ë³µì›\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_conv_final(x)\n        decoded = self.sigmoid(x)\n        return decoded\n\n# ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼ì€ data-generation ì…€ ì´í›„ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n# input_shapeë„ window_sizeë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •ë©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-03-cnn-baseline/index.html#ëª¨ë¸-í•™ìŠµ-ë°-ì´ìƒì¹˜-íƒì§€",
    "href": "posts/project-abc-03-cnn-baseline/index.html#ëª¨ë¸-í•™ìŠµ-ë°-ì´ìƒì¹˜-íƒì§€",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)",
    "section": "3. ëª¨ë¸ í•™ìŠµ ë° ì´ìƒì¹˜ íƒì§€",
    "text": "3. ëª¨ë¸ í•™ìŠµ ë° ì´ìƒì¹˜ íƒì§€\n\në°ì´í„° ìƒì„±\nWeek2ì—ì„œ ì‚¬ìš©í•œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ìƒ/ë¹„ì •ìƒ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤:\n\n# numpyëŠ” sliding_window_implementation ì…€ì—ì„œ ì´ë¯¸ import ë¨\n\n# ë°ì´í„° ìƒì„±\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì ìš©\nwindow_size = 10\nwindows = sliding_window(data, window_size) # (N, L) -&gt; (N, window_size)\nwindows = windows[..., np.newaxis]  # (N, L, C) -&gt; (N, window_size, 1)\n# PyTorch Conv1dëŠ” (N, C, L) ì…ë ¥ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì°¨ì› ë³€ê²½\nwindows = windows.transpose(0, 2, 1) # (N, C, L) -&gt; (N, 1, window_size)\nprint(f\"ìœˆë„ìš° ë°ì´í„° í˜•íƒœ (N, C, L): {windows.shape}\")\n\nìœˆë„ìš° ë°ì´í„° í˜•íƒœ (N, C, L): (91, 1, 10)\n\n\n\nimport torch.optim as optim # PyTorch ì˜µí‹°ë§ˆì´ì €\n\n# ëª¨ë¸ ìƒì„±\n# input_shapeì€ (window_size, 1) ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (sequence_length, num_features)\n# data-generation ì…€ì—ì„œ windowsëŠ” (N, 1, window_size) í˜•íƒœë¡œ ì¤€ë¹„ë¨.\n# CNNAutoencoderì˜ __init__ì€ input_shape=(window_size, 1)ì„ ë°›ì•„ input_shape[1]=1ì„ in_channelsë¡œ ì‚¬ìš©.\nmodel_input_shape = (window_size, 1) # (sequence_length, num_features)\nmodel = CNNAutoencoder(model_input_shape) # cnn-autoencoder-definition ì…€ì—ì„œ ì •ì˜ëœ í´ë˜ìŠ¤ ì‚¬ìš©\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss() # í‰ê·  ì œê³± ì˜¤ì°¨ ì†ì‹¤\n\nprint(\"PyTorch ëª¨ë¸ êµ¬ì¡°:\")\nprint(model)\n\nPyTorch ëª¨ë¸ êµ¬ì¡°:\nCNNAutoencoder(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n  (sigmoid): Sigmoid()\n)\n\n\n\n\nëª¨ë¸ í•™ìŠµ\nì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤:\n\n# torchëŠ” cnn-autoencoder-definition ì…€ì—ì„œ ì´ë¯¸ import ë¨\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ\n# 'outliers'ëŠ” ì›ë³¸ 'data' ë°°ì—´ì˜ ì¸ë±ìŠ¤ì…ë‹ˆë‹¤.\n# 'windows' ë°°ì—´ì—ì„œ ì´ìƒì¹˜ê°€ í¬í•¨ëœ ìœˆë„ìš°ë¥¼ ì‹ë³„í•˜ì—¬ ì œì™¸í•©ë‹ˆë‹¤.\ncontaminated_window_indices = set()\n# 'outliers', 'window_size', 'windows' ë³€ìˆ˜ëŠ” ì´ì „ ì…€ë“¤ì—ì„œ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\nfor outlier_data_idx in outliers: \n    start_contaminated_win_idx = max(0, outlier_data_idx - window_size + 1)\n    end_contaminated_win_idx = outlier_data_idx \n    \n    for win_idx in range(start_contaminated_win_idx, end_contaminated_win_idx + 1):\n        if win_idx &lt; len(windows): # ìœˆë„ìš° ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸\n            contaminated_window_indices.add(win_idx)\n\nnormal_windows_mask = np.ones(len(windows), dtype=bool)\nif contaminated_window_indices: # setì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ì¸ë±ì‹±\n    normal_windows_mask[list(contaminated_window_indices)] = False\n\nnormal_windows_np = windows[normal_windows_mask]\n\nif len(normal_windows_np) == 0:\n    print(\"ê²½ê³ : í•™ìŠµì— ì‚¬ìš©í•  ì •ìƒ ìœˆë„ìš°ê°€ ì—†ìŠµë‹ˆë‹¤. Outlier ì •ì˜, window_size ë˜ëŠ” ë°ì´í„° ê¸¸ì´ë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\nelse:\n    # PyTorch ë°ì´í„°ì…‹ ë° ë¡œë” ì¤€ë¹„\n    normal_windows_torch = torch.tensor(normal_windows_np, dtype=torch.float32)\n    train_dataset = TensorDataset(normal_windows_torch) # ì˜¤í† ì¸ì½”ë”ëŠ” ì…ë ¥ê³¼ íƒ€ê²Ÿì´ ë™ì¼\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n    # ëª¨ë¸ í•™ìŠµ\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device)\n    \n    epochs = 50 # ì—í¬í¬ ìˆ˜ ì„¤ì •\n    print_every_epochs = 10\n\n    model.train() # í•™ìŠµ ëª¨ë“œ\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_data_list in train_loader:\n            inputs = batch_data_list[0].to(device)\n            targets = inputs # ì˜¤í† ì¸ì½”ë”ì˜ íƒ€ê²Ÿì€ ì…ë ¥ê³¼ ë™ì¼\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item() * inputs.size(0) # ë°°ì¹˜ ì†ì‹¤ ëˆ„ì  (loss.item()ì€ í‰ê·  ì†ì‹¤)\n        \n        epoch_loss /= len(train_loader.dataset) # ì—í¬í¬ í‰ê·  ì†ì‹¤\n        if (epoch + 1) % print_every_epochs == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n    print(\"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n\nUsing device: cpu\nEpoch [10/50], Loss: 0.443446\nEpoch [20/50], Loss: 0.266177\nEpoch [30/50], Loss: 0.225018\nEpoch [40/50], Loss: 0.210972\nEpoch [50/50], Loss: 0.207535\nëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\n\n\n\n\nì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚° ë° ì´ìƒì¹˜ íƒì§€\ní•™ìŠµëœ ëª¨ë¸ë¡œ ë°ì´í„°ë¥¼ ë³µì›í•˜ê³ , ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤:\n\n# torch ë° numpyëŠ” ì´ì „ ì…€ë“¤ì—ì„œ ì´ë¯¸ import ë¨\n\n# ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() # í‰ê°€ ëª¨ë“œ\n\n# ì „ì²´ windows ë°ì´í„°ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³  deviceë¡œ ì´ë™\nall_windows_torch = torch.tensor(windows, dtype=torch.float32).to(device)\n\n# ë©”ëª¨ë¦¬ ë¶€ì¡±ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë‚˜, í˜„ì¬ ë°ì´í„°ëŠ” ì‘ìœ¼ë¯€ë¡œ í•œë²ˆì— ì²˜ë¦¬\nwith torch.no_grad(): # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”\n    reconstructed_torch = model(all_windows_torch)\n\n# ê²°ê³¼ë¥¼ CPUë¡œ ì˜®ê¸°ê³  NumPy ë°°ì—´ë¡œ ë³€í™˜\nreconstructed_np = reconstructed_torch.cpu().numpy()\n\n# MAE (Mean Absolute Error) ê³„ì‚°\n# ì›ë³¸ windows (numpy ë°°ì—´)ì™€ reconstructed_np ëª¨ë‘ (N, 1, window_size) í˜•íƒœ\n# axis=(1, 2)ëŠ” ì±„ë„ê³¼ ì‹œí€€ìŠ¤ ê¸¸ì´ì— ëŒ€í•œ í‰ê· ì„ ì˜ë¯¸\nmae = np.mean(np.abs(windows - reconstructed_np), axis=(1, 2))\nprint(f\"ê³„ì‚°ëœ MAE ê°’ (ì²˜ìŒ 5ê°œ): {mae[:5]}\")\n\n# ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ ì„ê³„ê°’ ì„¤ì • (ë°ì´í„° ë° ëª¨ë¸ ì„±ëŠ¥ì— ë”°ë¼ ì¡°ì • í•„ìš”)\n# ì˜ˆ: MAEì˜ í‰ê·  + (í‘œì¤€í¸ì°¨ * íŠ¹ì • ë°°ìˆ˜) ë˜ëŠ” ë¶„ìœ„ìˆ˜ ì‚¬ìš©\nthreshold = np.mean(mae) + 1.5 * np.std(mae) # í‘œì¤€í¸ì°¨ ë°°ìˆ˜ë¥¼ 2ì—ì„œ 1.5ë¡œ ì¤„ì—¬ ë¯¼ê°ë„ ì¦ê°€\nprint(f\"ì´ìƒì¹˜ íƒì§€ ì„ê³„ê°’ (MAE): {threshold:.4f}\")\n\nanomalies_indices_in_windows = np.where(mae &gt; threshold)[0] # ìœˆë„ìš° ë°°ì—´ ë‚´ì˜ ì¸ë±ìŠ¤\n\nprint(f\"ì´ìƒì¹˜ë¡œ íƒì§€ëœ ìœˆë„ìš°ì˜ ìˆ˜: {len(anomalies_indices_in_windows)}\")\nprint(f\"ì´ìƒì¹˜ë¡œ íƒì§€ëœ ìœˆë„ìš° ì¸ë±ìŠ¤: {anomalies_indices_in_windows}\")\n\n# ìœˆë„ìš° ì¸ë±ìŠ¤ë¥¼ ì›ë³¸ ë°ì´í„° ì¸ë±ìŠ¤ë¡œ ëŒ€ëµì ìœ¼ë¡œ ë§¤í•‘ (ìœˆë„ìš°ì˜ ì‹œì‘ì  ê¸°ì¤€)\n# ì‹¤ì œ ì´ìƒì¹˜ ë°œìƒ ì‹œì ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ\nanomalies_approx_original_indices = anomalies_indices_in_windows \n# ì¢€ ë” ì •í™•í•˜ê²ŒëŠ” ìœˆë„ìš°ì˜ ì¤‘ê°„ ì§€ì  ë“±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” ì‹œì‘ì ìœ¼ë¡œ ë‹¨ìˆœí™”\n# anomalies_approx_original_indices = [idx + window_size // 2 for idx in anomalies_indices_in_windows]\nprint(f\"ì›ë³¸ ë°ì´í„°ì˜ ëŒ€ëµì ì¸ ì´ìƒì¹˜ ì¸ë±ìŠ¤ (ìœˆë„ìš° ì‹œì‘ì  ê¸°ì¤€): {anomalies_approx_original_indices}\")\n\nê³„ì‚°ëœ MAE ê°’ (ì²˜ìŒ 5ê°œ): [0.12135064 0.10624936 0.09292315 0.08392817 0.0878529 ]\nì´ìƒì¹˜ íƒì§€ ì„ê³„ê°’ (MAE): 0.9662\nì´ìƒì¹˜ë¡œ íƒì§€ëœ ìœˆë„ìš°ì˜ ìˆ˜: 8\nì´ìƒì¹˜ë¡œ íƒì§€ëœ ìœˆë„ìš° ì¸ë±ìŠ¤: [17 18 19 20 47 48 49 50]\nì›ë³¸ ë°ì´í„°ì˜ ëŒ€ëµì ì¸ ì´ìƒì¹˜ ì¸ë±ìŠ¤ (ìœˆë„ìš° ì‹œì‘ì  ê¸°ì¤€): [17 18 19 20 47 48 49 50]\n\n\n\n\nê²°ê³¼ ì‹œê°í™”\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(data, label='ì›ë³¸ ë°ì´í„°', alpha=0.7) # 'data'ëŠ” data-generationì—ì„œ ì •ì˜ë¨\nplt.scatter(outliers, data[outliers], color='red', s=100, label='ì‹¤ì œ ì´ìƒì¹˜ (Ground Truth)', marker='o', edgecolors='black') # 'outliers'ëŠ” data-generationì—ì„œ ì •ì˜ë¨\n\n# anomalies_approx_original_indicesê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í™•ì¸\nif len(anomalies_approx_original_indices) &gt; 0:\n    # íƒì§€ëœ ì´ìƒì¹˜ í‘œì‹œëŠ” ìœˆë„ìš°ì˜ ì‹œì‘ì ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨\n    plt.scatter(anomalies_approx_original_indices, data[anomalies_approx_original_indices], \n                color='orange', marker='x', s=80, label='íƒì§€ëœ ì´ìƒì¹˜ (ëª¨ë¸ ì˜ˆì¸¡)', alpha=0.8)\nelse:\n    print(\"íƒì§€ëœ ì´ìƒì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n        \nplt.legend()\nplt.title('PyTorch CNN ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì‹œê³„ì—´ ì´ìƒ íƒì§€')\nplt.xlabel('ì‹œê°„ ìŠ¤í…')\nplt.ylabel('ê°’')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# ì¬êµ¬ì„± ì˜¤ì°¨(MAE) ì‹œê°í™”\nplt.figure(figsize=(10, 3))\nplt.plot(mae, label='ì¬êµ¬ì„± ì˜¤ì°¨ (MAE)', color='green')\nplt.axhline(threshold, color='red', linestyle='--', label=f'ì„ê³„ê°’ ({threshold:.2f})')\nplt.title('ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨ (MAE) ë° ì„ê³„ê°’')\nplt.xlabel('ìœˆë„ìš° ì¸ë±ìŠ¤')\nplt.ylabel('MAE')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\nPyTorch CNN ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n\n\n\n\n\n\n\n\n\n\n\n\níƒì§€ ê²°ê³¼ ë¶„ì„ ë° ê³ ë ¤ì‚¬í•­\nì‹œê°í™” ê²°ê³¼ì™€ ì¬êµ¬ì„± ì˜¤ì°¨ ê²€í†  ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\n\nì‹¤ì œ ì´ìƒì¹˜ vs.Â íƒì§€ ì´ìƒì¹˜:\n\ndata-generation ë‹¨ê³„ì—ì„œ ì˜ë„ì ìœ¼ë¡œ ë„£ì€ ì‹¤ì œ ì´ìƒì¹˜(outliers = [20, 50, 80])ì™€ ëª¨ë¸ì˜ íƒì§€ ê²°ê³¼ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆë‹¤.\nëª¨ë“  ì‹¤ì œ ì´ìƒì¹˜ê°€ íƒì§€ë˜ì§€ ì•Šê±°ë‚˜, ì •ìƒì´ ì´ìƒì¹˜ë¡œ ì˜ëª» íƒì§€ë  ê°€ëŠ¥ì„±ì´ í•­ìƒ ì¡´ì¬í•œë‹¤.\ní˜„ ì˜ˆì œëŠ” ì„ê³„ê°’(np.mean(mae) + 1.5 * np.std(mae)) ì¡°ì •ì„ í†µí•´ ìµœì†Œ í•˜ë‚˜ì˜ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ë„ë¡ ìœ ë„í–ˆë‹¤. ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ëª¨ë¸ ì„±ëŠ¥, ë°ì´í„° íŠ¹ì„±, window_size, ì„ê³„ê°’ ì„¤ì •ì— ë”°ë¼ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§„ë‹¤.\n\nìœˆë„ìš° ê²½ê³„ íš¨ê³¼ (Edge Effects):\n\nì‹œê³„ì—´ ë°ì´í„°ì˜ ì‹œì‘ê³¼ ë ë¶€ë¶„ ìœˆë„ìš°ëŠ” ë‚´ë¶€ ìœˆë„ìš°ì— ë¹„í•´ ì •ë³´ê°€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆë‹¤ (ì´ì „/ì´í›„ ë°ì´í„° ë¶€ì¬).\nCNN ëª¨ë¸, íŠ¹íˆ íŒ¨ë”© ì‚¬ìš© ì‹œ, ê²½ê³„ ì˜ì—­ ìœˆë„ìš°ëŠ” í•™ìŠµëœ ì£¼ ì •ìƒ íŒ¨í„´ê³¼ ë‹¬ë¼ ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ, ì‹œê³„ì—´ ì–‘ ëë¶€ë¶„ì—ì„œ ì´ìƒì¹˜ê°€ ì•„ë‹Œë°ë„ ì´ìƒì¹˜ë¡œ íƒì§€ë˜ëŠ” ê²½í–¥ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤. MAE ê·¸ë˜í”„ì—ì„œ ì´ˆë°˜ ë˜ëŠ” í›„ë°˜ë¶€ì— ë†’ì€ ì˜¤ì°¨ê°€ ê´€ì°°ëœë‹¤ë©´ ì´ íš¨ê³¼ë¥¼ ì˜ì‹¬í•´ë³¼ ìˆ˜ ìˆë‹¤.\n\nwindow_sizeì˜ ì¤‘ìš”ì„±:\n\nwindow_sizeëŠ” ëª¨ë¸ì´ í•™ìŠµí•  íŒ¨í„´ì˜ ê¸¸ì´ë¥¼ ê²°ì •í•œë‹¤.\në„ˆë¬´ ì‘ìœ¼ë©´ ì¥ê¸° íŒ¨í„´ íŒŒì•…ì´ ì–´ë µê³ , ë„ˆë¬´ í¬ë©´ ì§§ì€ ìˆœê°„ì˜ ì´ìƒì¹˜ë¥¼ ë†“ì¹˜ê±°ë‚˜ ì •ìƒ ë³€ë™ì—ë„ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆë‹¤.\ní˜„ì¬ window_size=10ìœ¼ë¡œ ì„¤ì •í–ˆë‹¤. ë°ì´í„° íŠ¹ì„±ì— ë§ì¶° ì´ ê°’ì„ ì¡°ì •í•˜ë©° ì‹¤í—˜í•˜ëŠ” ê³¼ì •ì´ ì¤‘ìš”í•˜ë‹¤.\n\nëª¨ë¸ ë° ì„ê³„ê°’ì˜ í•œê³„:\n\nì—¬ê¸°ì„œ ì‚¬ìš©í•œ CNN ì˜¤í† ì¸ì½”ë”ëŠ” ë¹„êµì  ë‹¨ìˆœí•œ ëª¨ë¸ì´ë‹¤.\në” ë³µì¡í•œ íŒ¨í„´ì´ë‚˜ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ë ¤ë©´ ëª¨ë¸ êµ¬ì¡° ê°œì„ (ì˜ˆ: LSTM, Transformer ê¸°ë°˜ ì˜¤í† ì¸ì½”ë”)ì´ë‚˜ ë‹¤ë¥¸ ì ‘ê·¼ë²•ì„ ê³ ë ¤í•´ì•¼ í•œë‹¤.\nê³ ì • ì„ê³„ê°’ ëŒ€ì‹  ë™ì  ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ê±°ë‚˜, í†µê³„ì  ê²€ì • ê¸°ë²•ì„ ê²°í•©í•˜ëŠ” ê²ƒë„ íƒì§€ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.\n\n\nì´ëŸ° ì ë“¤ì„ ê³ ë ¤í•´ ëª¨ë¸ ê²°ê³¼ë¥¼ í•´ì„í•´ì•¼ í•˜ë©°, ì‹¤ì œ ë¬¸ì œ ì ìš© ì‹œì—ëŠ” ì¶©ë¶„í•œ ê²€ì¦ê³¼ ì‹¤í—˜ì´ í•„ìˆ˜ë‹¤."
  },
  {
    "objectID": "posts/project-abc-03-cnn-baseline/index.html#ê²°ë¡ -ë°-ë‹¤ìŒ-ë‹¨ê³„",
    "href": "posts/project-abc-03-cnn-baseline/index.html#ê²°ë¡ -ë°-ë‹¤ìŒ-ë‹¨ê³„",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)",
    "section": "ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„",
    "text": "ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„\nì´ë²ˆ ì£¼ì—ëŠ” PyTorchë¡œ ê°„ë‹¨í•œ 1D CNN ì˜¤í† ì¸ì½”ë”ë¥¼ ë§Œë“¤ê³ , ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìˆ˜í–‰í–ˆë‹¤. ì´ ëª¨ë¸ì€ ì‹œê³„ì—´ ì´ìƒ íƒì§€ì˜ ê´œì°®ì€ ì‹œì‘ì ì´ ë  ìˆ˜ ìˆë‹¤. ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ ì°¾ëŠ” ê³¼ì •ê³¼, ì„ê³„ê°’ ì„¤ì •ì— ë”°ë¼ íƒì§€ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í–ˆë‹¤.\në‹¤ìŒ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì‹¤ì œ ì‚°ì—… ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , ì„±ëŠ¥ì„ ê°œì„ í•  ë‹¤ì–‘í•œ ë°©ë²•(ì˜ˆ: ë” ë³µì¡í•œ ëª¨ë¸ êµ¬ì¡°, ë‹¤ë¥¸ ìœ í˜•ì˜ ì˜¤í† ì¸ì½”ë”, ë™ì  ì„ê³„ê°’ ì„¤ì • ë“±)ì„ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "[GitHub] Feature ë“± ë¸Œëœì¹˜ í™œìš©ë²•, ì»¤ë°‹ ë©”ì‹œì§€ ì‘ì„±ë²•\n\n\n\nGitHub\n\n\n\níŒ€ í”„ë¡œì íŠ¸ì— ì ìš© ê°€ëŠ¥í•œ Git ë¸Œëœì¹˜ ì „ëµê³¼ ì´ë¦„ ê·œì¹™ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\n\n\n\n\n\nDec 2, 2025\n\n\nBeomdo Park\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 5ì£¼ì°¨ - ì‹¤ì œ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ë¥¼ í™œìš©í•œ ì´ìƒ íƒì§€\n\n\n\nABCí”„ë¡œì íŠ¸ë©˜í† ë§\n\nPyTorch\n\n\n\nì´ì „ ì£¼ì°¨ì—ì„œ ê°œë°œí•œ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì„ ì‹¤ì œ Kaggleì˜ ì£¼íƒ ì „ë ¥ ì‚¬ìš©ëŸ‰ ë°ì´í„°ì— ì ìš©í•˜ì—¬, í˜„ì‹¤ ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” ì´ìƒ íŒ¨í„´ì„ íƒì§€í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n\n\n\n\nJun 19, 2025\n\n\nBeomdo Park\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n\n\n\nABCí”„ë¡œì íŠ¸ë©˜í† ë§\n\nPyTorch\n\n\n\nì§€ë‚œì£¼ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì˜ í•œê³„ë¥¼ ë¶„ì„í•˜ê³ , ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë‹¤ì–‘í•œ ë°©ë²•ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ê³¼ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤.\n\n\n\n\n\nJun 14, 2025\n\n\nBeomdo Park\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 3ì£¼ì°¨ - CNNìœ¼ë¡œ ì‹œê³„ì—´ ì´ìƒ íƒì§€ (PyTorch)\n\n\n\nABCí”„ë¡œì íŠ¸ë©˜í† ë§\n\nPyTorch\n\n\n\nPyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ 1D CNN ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ ì‹œê³„ì—´ ì´ìƒ íƒì§€ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n\n\n\n\n\nJun 8, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 2ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ì´ˆ ì ìš©\n\n\n\nABCí”„ë¡œì íŠ¸ë©˜í† ë§\n\nPyTorch\n\n\n\nPythonì„ í™œìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì´ìƒ íƒì§€ - ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²• ì ìš© ì‹¤ìŠµ\n\n\n\n\n\nJun 1, 2025\n\n\nBeomdo Park\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 1ì£¼ì°¨ - ì‹œê³„ì—´ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ EDA ë° ì „ì²˜ë¦¬\n\n\n\nABCí”„ë¡œì íŠ¸ë©˜í† ë§\n\nPyTorch\n\n\n\nPythonì„ í™œìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ ê¸°ë³¸ EDA ë° ì „ì²˜ë¦¬ ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n\n\n\n\n\nMay 25, 2025\n\n\nBeomdo Park\n\n8 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html",
    "href": "posts/project-abc-04-model-optimization/index.html",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "",
    "text": "ì•ˆë…•í•˜ì„¸ìš”, ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸° ë„¤ ë²ˆì§¸ ê¸°ìˆ ë…¸íŠ¸ì…ë‹ˆë‹¤. ì§€ë‚œì£¼ì—ëŠ” PyTorchë¥¼ ì´ìš©í•´ CNN ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ì˜ ì‹œê³„ì—´ ì´ìƒ íƒì§€ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì£¼ì—ëŠ” í•´ë‹¹ ëª¨ë¸ì˜ í•œê³„ë¥¼ ëª…í™•íˆ ë¶„ì„í•˜ê³ , ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ë²•ë¡ ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ â€™Optunaâ€™ë¥¼ í™œìš©í•œ ì‹¤í—˜ ê³¼ì •ì„ ìƒì„¸íˆ ê³µìœ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html#ê¸°ì¡´-ëª¨ë¸ì˜-í•œê³„-ëª…í™•íˆ-í•˜ê¸°",
    "href": "posts/project-abc-04-model-optimization/index.html#ê¸°ì¡´-ëª¨ë¸ì˜-í•œê³„-ëª…í™•íˆ-í•˜ê¸°",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "1. ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„ ëª…í™•íˆ í•˜ê¸°",
    "text": "1. ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„ ëª…í™•íˆ í•˜ê¸°\nëª¨ë“  ëª¨ë¸ë§ì˜ ì‹œì‘ì€ í˜„ì¬ ëª¨ë¸ì„ ì •í™•íˆ ì•„ëŠ” ê²ƒì…ë‹ˆë‹¤. Week3ì—ì„œ êµ¬í˜„í•œ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì€ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì£¼ì—ˆì§€ë§Œ, ëª‡ ê°€ì§€ ëª…í™•í•œ í•œê³„ì ì„ ê°€ì§€ê³  ìˆì—ˆìŠµë‹ˆë‹¤.\n\n1.1. íƒì§€ ì„±ëŠ¥ì˜ ì•„ì‰¬ì›€: ë†“ì¹˜ê±°ë‚˜, ì˜ëª» ì¡ê±°ë‚˜\nì§€ë‚œì£¼ ê²°ê³¼ ê·¸ë˜í”„ë¥¼ ë‹¤ì‹œ ì‚´í´ë³´ë©´, ì‹¤ì œ ì´ìƒì¹˜(Ground Truth) 3ê°œ ì¤‘ ì¼ë¶€ë¥¼ íƒì§€í•˜ì§€ ëª»í•˜ê±°ë‚˜(False Negative), ë°˜ëŒ€ë¡œ ì •ìƒ êµ¬ê°„ì„ ì´ìƒì¹˜ë¡œ íŒë‹¨í•˜ëŠ”(False Positive) ê²½í–¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.\n\níƒì§€ ëˆ„ë½ (False Negative): 80ë²ˆ ì¸ë±ìŠ¤ ì£¼ë³€ì˜ ì‹¤ì œ ì´ìƒì¹˜ëŠ” ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ì„ê³„ê°’ì„ ë„˜ì§€ ì•Šì•„ íƒì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ í•´ë‹¹ ìœ í˜•ì˜ ì´ìƒ íŒ¨í„´(ìƒëŒ€ì ìœ¼ë¡œ ë³€í™”ì˜ í­ì´ ì‘ì€ ì´ìƒì¹˜)ì„ ì •ìƒ ë°ì´í„°ì˜ ì¼ë¶€ë¡œ í•™ìŠµí–ˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë„ˆë¬´ â€™ê´€ëŒ€â€™í•˜ê²Œ ë°ì´í„°ë¥¼ ë³µì›í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.\nì˜¤íƒ (False Positive): ì‹œê³„ì—´ ë°ì´í„°ì˜ ì‹œì‘ ë¶€ë¶„(0~10 ì¸ë±ìŠ¤)ì—ì„œ ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ë†’ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” Week3ì—ì„œ ë¶„ì„í–ˆë“¯, ìœˆë„ìš°ê°€ ì™„ì „í•œ í˜•íƒœë¥¼ ê°–ì¶”ì§€ ëª»í•´ ë°œìƒí•˜ëŠ” â€™ìœˆë„ìš° ê²½ê³„ íš¨ê³¼(Edge Effect)â€™ë¡œ ì¸í•œ ì˜¤íƒì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n\n\n\n\nì§€ë‚œì£¼ íƒì§€ ê²°ê³¼ ê·¸ë˜í”„\n\n\n\nê·¸ë¦¼ 1. Week3 ëª¨ë¸ì˜ ì´ìƒ íƒì§€ ê²°ê³¼. ì¼ë¶€ ì´ìƒì¹˜ë¥¼ ë†“ì¹˜ê³ , ê²½ê³„ë©´ì—ì„œ ì˜¤íƒì´ ë°œìƒí–ˆë‹¤.\n\n\n\n1.2. ê³¼ì í•©(Overfitting) ê°€ëŠ¥ì„±\nì˜¤í† ì¸ì½”ë”ëŠ” ì •ìƒ ë°ì´í„°ì˜ í•µì‹¬ íŒ¨í„´ì„ í•™ìŠµí•´ì•¼ í•˜ì§€ë§Œ, ë„ˆë¬´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì¹˜ì¤‘í•˜ë©´ â€™ê³¼ì í•©â€™ë˜ì–´ ë¯¸ì„¸í•œ ë…¸ì´ì¦ˆê¹Œì§€ ëª¨ë‘ ì •ìƒìœ¼ë¡œ ê°„ì£¼í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ê²½ìš°, ìƒˆë¡œìš´ í˜•íƒœì˜ ì´ìƒì¹˜ê°€ ë“¤ì–´ì™”ì„ ë•Œ ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ì§€ ëª»í•´ íƒì§€ ì„±ëŠ¥ì´ ì €í•˜ë©ë‹ˆë‹¤. í˜„ì¬ ëª¨ë¸ì€ Dropoutì´ë‚˜ ê·œì œ(Regularization) ê°™ì€ ê³¼ì í•© ë°©ì§€ ì¥ì¹˜ê°€ ì—†ì–´ ì´ëŸ¬í•œ ìœ„í—˜ì— ë…¸ì¶œë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html#ì„±ëŠ¥-ê°œì„ ì„-ìœ„í•œ-ì ‘ê·¼-ì „ëµ",
    "href": "posts/project-abc-04-model-optimization/index.html#ì„±ëŠ¥-ê°œì„ ì„-ìœ„í•œ-ì ‘ê·¼-ì „ëµ",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "2. ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ì ‘ê·¼ ì „ëµ",
    "text": "2. ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ì ‘ê·¼ ì „ëµ\nìœ„ì—ì„œ ì •ì˜í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ì „ëµì„ ì‹œë„í–ˆìŠµë‹ˆë‹¤.\n\n2.1. ìœˆë„ìš°ë³„ ì •ê·œí™” ë°ì´í„° ì „ì²˜ë¦¬\nì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë‹¨ì¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ì ìš©í•˜ëŠ” ëŒ€ì‹ , ê° ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë³„ë¡œ ë…ë¦½ì ì¸ MinMaxScalerë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.\n\nì§€ì—­ì  íŠ¹ì„± ê°•ì¡°: ì „ì²´ ë°ì´í„°ì˜ í‰ê· ì´ë‚˜ í‘œì¤€í¸ì°¨ì— ì˜í–¥ì„ ë°›ì§€ ì•Šê³ , ê° ìœˆë„ìš° ë‚´ë¶€ì˜ ìƒëŒ€ì ì¸ ë°ì´í„° ë¶„í¬ì™€ íŒ¨í„´ì— ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në³€ë™ì„± ëŒ€ì‘: ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ì´ ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ê²½ìš°(Non-stationary)ì—ë„ ëª¨ë¸ì´ ë” ê°•ê±´í•˜ê²Œ ë°˜ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ìƒì¹˜ ë¯¼ê°ë„ í–¥ìƒ: ì •ìƒ ìƒíƒœì˜ ì§€ì—­ì  íŒ¨í„´ì„ ë” ì •êµí•˜ê²Œ í•™ìŠµí•˜ë¯€ë¡œ, ê·¸ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚˜ëŠ” ì´ìƒì¹˜ë¥¼ ë” ë¯¼ê°í•˜ê²Œ ê°ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê° ìœˆë„ìš°ëŠ” [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”ë˜ë©°, ì´ëŠ” ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\n\n# ì˜ˆì‹œ ë°ì´í„° ìƒì„±\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\nprint(f\"ì›ë³¸ ë°ì´í„° í‰ê· /í‘œì¤€í¸ì°¨: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"ì´ìƒì¹˜ ìœ„ì¹˜: {outliers}\")\n\nì›ë³¸ ë°ì´í„° í‰ê· /í‘œì¤€í¸ì°¨: 0.03 / 0.84\nì´ìƒì¹˜ ìœ„ì¹˜: [20, 50, 80]\n\n\n\ndef create_sliding_windows(data, window_size):\n    \"\"\"ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±\"\"\"\n    windows = []\n    for i in range(len(data) - window_size + 1):\n        windows.append(data[i:i + window_size])\n    return np.array(windows)\n\ndef normalize_windows(windows):\n    \"\"\"ê° ìœˆë„ìš°ë³„ë¡œ ê°œë³„ ì •ê·œí™”\"\"\"\n    normalized_windows = []\n    scalers = []\n    \n    for window in windows:\n        scaler = MinMaxScaler()\n        normalized_window = scaler.fit_transform(window.reshape(-1, 1)).flatten()\n        normalized_windows.append(normalized_window)\n        scalers.append(scaler)\n    \n    return np.array(normalized_windows), scalers\n\n# ìœˆë„ìš° ìƒì„± ë° ì •ê·œí™”\nwindow_size = 10\nraw_windows = create_sliding_windows(data, window_size)\nnormalized_windows, window_scalers = normalize_windows(raw_windows)\n\nprint(f\"ìƒì„±ëœ ìœˆë„ìš° ìˆ˜: {len(normalized_windows)}\")\nprint(f\"ê° ìœˆë„ìš° í¬ê¸°: {normalized_windows.shape[1]}\")\nprint(f\"ì •ê·œí™” í›„ ì²« ë²ˆì§¸ ìœˆë„ìš° ë²”ìœ„: [{normalized_windows[0].min():.3f}, {normalized_windows[0].max():.3f}]\")\n\nìƒì„±ëœ ìœˆë„ìš° ìˆ˜: 91\nê° ìœˆë„ìš° í¬ê¸°: 10\nì •ê·œí™” í›„ ì²« ë²ˆì§¸ ìœˆë„ìš° ë²”ìœ„: [0.000, 1.000]\n\n\n\n\n2.2. ëª¨ë¸ êµ¬ì¡° ë³€ê²½: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ Dropout ì¶”ê°€\nëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê³  ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Dropout ë ˆì´ì–´ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤. Dropoutì€ í•™ìŠµ ê³¼ì •ì—ì„œ ê° ë‰´ëŸ°ì„ í™•ë¥ ì ìœ¼ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ ëª¨ë¸ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤. ì£¼ë¡œ í™œì„±í™” í•¨ìˆ˜(ReLU) ë’¤ì— ìœ„ì¹˜ì‹œì¼œ ì •ë³´ì˜ íë¦„ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        self.input_size = input_shape[0]  # ìœˆë„ìš° í¬ê¸°\n        \n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # ì¤‘ê°„ í¬ê¸° ê³„ì‚°\n        encoded_size = self.input_size // 4  # ë‘ ë²ˆì˜ í’€ë§ ê²°ê³¼\n        \n        # Decoder - ì—…ìƒ˜í”Œë§ í›„ í¬ê¸° ì¡°ì •\n        self.decoder_upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv1 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        \n        self.decoder_upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n        self.decoder_conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        \n        # ìµœì¢… í¬ê¸° ì¡°ì •ì„ ìœ„í•œ ì ì‘í˜• í’€ë§\n        self.decoder_adaptive = nn.AdaptiveAvgPool1d(self.input_size)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_upsample1(encoded)\n        x = self.decoder_conv1(x)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        \n        x = self.decoder_upsample2(x)\n        x = self.decoder_conv2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        \n        # ì •í™•í•œ ì…ë ¥ í¬ê¸°ë¡œ ë³µì›\n        x = self.decoder_adaptive(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n# ëª¨ë¸ í…ŒìŠ¤íŠ¸\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_upsample1): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_upsample2): Upsample(scale_factor=2.0, mode='nearest')\n  (decoder_conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_adaptive): AdaptiveAvgPool1d(output_size=10)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n\n\n\n\n2.3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: Optuna í™œìš©\nëª¨ë¸ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°(í•™ìŠµë¥ , ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨, í•„í„° ìˆ˜ ë“±)ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ Optuna ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. OptunaëŠ” ë² ì´ì§€ì•ˆ ìµœì í™” ê¸°ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ íš¨icientí•œ íƒìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html#optunaë¥¼-ì´ìš©í•œ-í†µí•©-ìµœì í™”",
    "href": "posts/project-abc-04-model-optimization/index.html#optunaë¥¼-ì´ìš©í•œ-í†µí•©-ìµœì í™”",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "3. Optunaë¥¼ ì´ìš©í•œ í†µí•© ìµœì í™”",
    "text": "3. Optunaë¥¼ ì´ìš©í•œ í†µí•© ìµœì í™”\n\n3.1. ìœˆë„ìš° í¬ê¸° ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ì‹œ ìµœì í™”\nê°€ì¥ í° ë³€ê²½ì ì€ ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œì—ì„œëŠ” ì‹¤ì œ ì´ìƒì¹˜ ì¸ë±ìŠ¤(outliers)ê°€ í¬í•¨ë˜ì§€ ì•Šì€ â€™ì •ìƒ ìœˆë„ìš°â€™ë§Œ í•„í„°ë§í•˜ì—¬ í•™ìŠµ ë° ê²€ì¦ì— ì‚¬ìš©í•©ë‹ˆë‹¤.\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n\n# --- í†µí•© ìµœì í™” Objective í•¨ìˆ˜ (ìœˆë„ìš° í¬ê¸° + í•˜ì´í¼íŒŒë¼ë¯¸í„° + ì¡°ê¸°ì¢…ë£Œ) ---\ndef comprehensive_objective(trial):\n    # ë°ì´í„°ë¥¼ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ë‹¤ì‹œ ì •ì˜ (scope ë¬¸ì œ ë°©ì§€)\n    np.random.seed(42)\n    trial_data = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\n    trial_outliers = [20, 50, 80]\n    trial_data[trial_outliers] += [3, -3, 2]\n\n    # ìœˆë„ìš° í¬ê¸° ìµœì í™” (ë°ì´í„° í¬ê¸°ì— ë§ê²Œ ì¡°ì •)\n    # ë°ì´í„° ê¸¸ì´ê°€ 100ì´ë¯€ë¡œ ìµœëŒ€ ìœˆë„ìš° í¬ê¸°ë¥¼ 15ë¡œ ì œí•œ\n    window_size = trial.suggest_categorical(\"window_size\", [5, 8, 10, 12, 15])\n\n    # ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n\n    try:\n        # ìœˆë„ìš° ìƒì„± (ë™ì ) - ìœˆë„ìš°ë³„ ì •ê·œí™” ì ìš©\n        raw_windows = create_sliding_windows(trial_data, window_size)\n\n        # ìœˆë„ìš° ìƒì„± ì‹¤íŒ¨ ì²´í¬\n        if len(raw_windows) == 0:\n            print(\n                f\"Trial {trial.number}: ìœˆë„ìš° í¬ê¸° {window_size}ë¡œ ìœˆë„ìš° ìƒì„± ì‹¤íŒ¨ (ë°ì´í„° ê¸¸ì´: {len(trial_data)})\"\n            )\n            return float(\"inf\")\n\n        trial_normalized_windows, trial_scalers = normalize_windows(raw_windows)\n\n        if (\n            len(trial_normalized_windows) &lt; 10\n        ):  # ì¶©ë¶„í•œ ìœˆë„ìš°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸° (20ì—ì„œ 10ìœ¼ë¡œ ì™„í™”)\n            print(\n                f\"Trial {trial.number}: ìœˆë„ìš° ìˆ˜ ë¶€ì¡± ({len(trial_normalized_windows)} &lt; 10)\"\n            )\n            return float(\"inf\")\n\n        windows_tensor = torch.from_numpy(trial_normalized_windows).unsqueeze(1).float()\n\n        # ì •ìƒ ìœˆë„ìš° í•„í„°ë§ (ì´ìƒì¹˜ê°€ í¬í•¨ëœ ìœˆë„ìš° ì œì™¸)\n        trial_normal_indices = []\n        for i in range(len(trial_normalized_windows)):\n            window_range = range(i, i + window_size)\n            if not any(outlier_idx in window_range for outlier_idx in trial_outliers):\n                trial_normal_indices.append(i)\n\n        if len(trial_normal_indices) &lt; 10:  # ì¶©ë¶„í•œ ì •ìƒ ìœˆë„ìš°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°\n            return float(\"inf\")\n\n        # ì •ìƒ ë°ì´í„°ë¡œ í•™ìŠµ/ê²€ì¦ ë¶„í• \n        normal_windows_torch = windows_tensor[trial_normal_indices]\n        normal_dataset = TensorDataset(normal_windows_torch)\n\n        train_size = int(0.8 * len(normal_dataset))\n        val_size = len(normal_dataset) - train_size\n        train_dataset, val_dataset = random_split(\n            normal_dataset, [train_size, val_size]\n        )\n\n        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n        # ëª¨ë¸ ìƒì„±\n        model = CNNAutoencoderWithDropout(\n            input_shape=(window_size, 1), dropout_rate=dropout_rate\n        )\n        optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n\n        # ì¡°ê¸°ì¢…ë£Œ ì„¤ì •\n        best_val_loss = float(\"inf\")\n        patience = 10\n        patience_counter = 0\n\n        # í•™ìŠµ (ì¡°ê¸°ì¢…ë£Œ ì ìš©)\n        for epoch in range(50):  # ìµœëŒ€ 50 ì—í¬í¬\n            model.train()\n            for data in train_loader:\n                inputs = data[0]\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, inputs)\n                loss.backward()\n                optimizer.step()\n\n            # ê²€ì¦ ì†ì‹¤ ê³„ì‚°\n            model.eval()\n            val_loss = 0\n            with torch.no_grad():\n                for data in val_loader:\n                    inputs = data[0]\n                    outputs = model(inputs)\n                    loss = criterion(outputs, inputs)\n                    val_loss += loss.item()\n\n            val_loss = val_loss / len(val_loader)\n\n            # ì¡°ê¸°ì¢…ë£Œ ì²´í¬\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n            else:\n                patience_counter += 1\n                if patience_counter &gt;= patience:\n                    break\n\n        return best_val_loss\n\n    except Exception as e:\n        print(f\"Trial {trial.number} failed: {e}\")\n        return float(\"inf\")\n\n\n# --- í†µí•© Optuna Study ì‹¤í–‰ ---\nprint(\"í†µí•© ìµœì í™” ì‹œì‘ (ìœˆë„ìš° í¬ê¸° + í•˜ì´í¼íŒŒë¼ë¯¸í„° + ì¡°ê¸°ì¢…ë£Œ)...\")\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(comprehensive_objective, n_trials=15)  # ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ 15íšŒë¡œ ì¶•ì†Œ\n\nprint(\"=== í†µí•© ìµœì í™” ê²°ê³¼ ===\")\nprint(\"Best trial:\", study.best_trial.params)\nprint(f\"Best validation loss: {study.best_value:.6f}\")\n\n# ìµœì  íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ì•ˆì „í•œ í´ë°± ë¡œì§ í¬í•¨)\nif study.best_value == float(\"inf\"):\n    print(\"ê²½ê³ : ëª¨ë“  ìµœì í™” ì‹œë„ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n    best_window_size = 10\n    best_lr = 0.001\n    best_dropout = 0.2\n    best_optimizer = \"Adam\"\nelse:\n    best_window_size = study.best_trial.params[\"window_size\"]\n    best_lr = study.best_trial.params[\"lr\"]\n    best_dropout = study.best_trial.params[\"dropout_rate\"]\n    best_optimizer = study.best_trial.params[\"optimizer\"]\n\nprint(f\"ìµœì  ìœˆë„ìš° í¬ê¸°: {best_window_size}\")\nprint(f\"ìµœì  í•™ìŠµë¥ : {best_lr:.6f}\")\nprint(f\"ìµœì  ë“œë¡­ì•„ì›ƒ: {best_dropout:.3f}\")\nprint(f\"ìµœì  ì˜µí‹°ë§ˆì´ì €: {best_optimizer}\")\n\n[I 2025-12-02 02:20:46,733] A new study created in memory with name: no-name-6faf1202-a7fd-4967-9fe4-7fb7bc4c514c\n\n\ní†µí•© ìµœì í™” ì‹œì‘ (ìœˆë„ìš° í¬ê¸° + í•˜ì´í¼íŒŒë¼ë¯¸í„° + ì¡°ê¸°ì¢…ë£Œ)...\n\n\n[I 2025-12-02 02:20:55,775] Trial 0 finished with value: 0.1016029417514801 and parameters: {'window_size': 10, 'lr': 0.0001593474487942954, 'dropout_rate': 0.10834066454089282, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.1016029417514801.\n[I 2025-12-02 02:20:58,397] Trial 1 finished with value: 0.10688463598489761 and parameters: {'window_size': 12, 'lr': 0.00015585653001312552, 'dropout_rate': 0.32472752812259387, 'optimizer': 'Adam'}. Best is trial 0 with value: 0.1016029417514801.\n[I 2025-12-02 02:20:58,832] Trial 2 finished with value: 0.10684473812580109 and parameters: {'window_size': 15, 'lr': 0.009767298775122979, 'dropout_rate': 0.3962547680940173, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.1016029417514801.\n[I 2025-12-02 02:21:00,791] Trial 3 finished with value: 0.024204352870583534 and parameters: {'window_size': 10, 'lr': 0.0020508473998720393, 'dropout_rate': 0.18057119125986343, 'optimizer': 'Adam'}. Best is trial 3 with value: 0.024204352870583534.\n[I 2025-12-02 02:21:03,094] Trial 4 finished with value: 0.0433560311794281 and parameters: {'window_size': 8, 'lr': 0.003509853985913906, 'dropout_rate': 0.1777066757566287, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.024204352870583534.\n[I 2025-12-02 02:21:03,771] Trial 5 finished with value: 0.11393214017152786 and parameters: {'window_size': 12, 'lr': 0.0004574622343772454, 'dropout_rate': 0.42310376441109143, 'optimizer': 'RMSprop'}. Best is trial 3 with value: 0.024204352870583534.\n[I 2025-12-02 02:21:05,075] Trial 6 finished with value: 0.022752600722014904 and parameters: {'window_size': 5, 'lr': 0.007187606717318182, 'dropout_rate': 0.10616971386055743, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.022752600722014904.\n[I 2025-12-02 02:21:06,349] Trial 7 finished with value: 0.05724473297595978 and parameters: {'window_size': 12, 'lr': 0.0006137765753130427, 'dropout_rate': 0.3924789675238718, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.022752600722014904.\n[I 2025-12-02 02:21:08,421] Trial 8 finished with value: 0.05957316234707832 and parameters: {'window_size': 8, 'lr': 0.001625952728750797, 'dropout_rate': 0.48435177536224516, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.022752600722014904.\n[I 2025-12-02 02:21:10,767] Trial 9 finished with value: 0.10553278028964996 and parameters: {'window_size': 15, 'lr': 0.00026666738854384353, 'dropout_rate': 0.22182756770881787, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.022752600722014904.\n[I 2025-12-02 02:21:12,908] Trial 10 finished with value: 0.32417644560337067 and parameters: {'window_size': 5, 'lr': 1.8713344537286103e-05, 'dropout_rate': 0.10067798128940057, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.022752600722014904.\n[I 2025-12-02 02:21:14,474] Trial 11 finished with value: 0.021259795874357224 and parameters: {'window_size': 10, 'lr': 0.009842128249877322, 'dropout_rate': 0.20816319153477258, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.021259795874357224.\n[I 2025-12-02 02:21:16,496] Trial 12 finished with value: 0.03433164581656456 and parameters: {'window_size': 5, 'lr': 0.004918998723410851, 'dropout_rate': 0.2680396229444508, 'optimizer': 'Adam'}. Best is trial 11 with value: 0.021259795874357224.\n[I 2025-12-02 02:21:17,651] Trial 13 finished with value: 0.017096739262342453 and parameters: {'window_size': 10, 'lr': 0.008009915972357168, 'dropout_rate': 0.16119045338457647, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.017096739262342453.\n[I 2025-12-02 02:21:19,661] Trial 14 finished with value: 0.37718915939331055 and parameters: {'window_size': 10, 'lr': 3.285172827594396e-05, 'dropout_rate': 0.18550298336873888, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.017096739262342453.\n\n\n=== í†µí•© ìµœì í™” ê²°ê³¼ ===\nBest trial: {'window_size': 10, 'lr': 0.008009915972357168, 'dropout_rate': 0.16119045338457647, 'optimizer': 'Adam'}\nBest validation loss: 0.017097\nìµœì  ìœˆë„ìš° í¬ê¸°: 10\nìµœì  í•™ìŠµë¥ : 0.008010\nìµœì  ë“œë¡­ì•„ì›ƒ: 0.161\nìµœì  ì˜µí‹°ë§ˆì´ì €: Adam\n\n\n\n\n3.2. ìµœì í™” ê²°ê³¼ ë¶„ì„\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=800, height=500) # ë„ˆë¹„ 800ìœ¼ë¡œ ìˆ˜ì •\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=800, height=400) # ë„ˆë¹„ 800ìœ¼ë¡œ ìˆ˜ì •\nfig2.show()"
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html#ìµœì¢…-ëª¨ë¸-ì„±ëŠ¥-í‰ê°€",
    "href": "posts/project-abc-04-model-optimization/index.html#ìµœì¢…-ëª¨ë¸-ì„±ëŠ¥-í‰ê°€",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "4. ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€",
    "text": "4. ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ í‰ê°€\n\n4.1. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ ë° í‰ê°€\nOptunaê°€ ì°¾ì€ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ìœˆë„ìš° í¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n\në°ì´í„° ì¤€ë¹„: ìµœì  ìœˆë„ìš° í¬ê¸°(best_window_size)ë¡œ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ë‹¤ì‹œ ìƒì„±í•˜ê³ , ìœˆë„ìš°ë³„ ì •ê·œí™”ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\nìµœì¢… ëª¨ë¸ í•™ìŠµ: ì •ìƒ ë°ì´í„°ë§Œì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ìµœì¢… ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\nì„ê³„ê°’ ì„¤ì • ë° ì´ìƒì¹˜ íƒì§€: í•™ìŠµëœ ëª¨ë¸ì„ ì „ì²´ ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ ê³„ì‚°í•˜ê³ , ë¯¸ë¦¬ ì •ì˜ëœ ì„ê³„ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.\nê²°ê³¼ ì‹œê°í™”: ì›ë³¸ ë°ì´í„°ì™€ íƒì§€ëœ ì´ìƒì¹˜, ê·¸ë¦¬ê³  ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨ë¥¼ í•¨ê»˜ ì‹œê°í™”í•˜ì—¬ ì„±ëŠ¥ì„ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.\n\n\nimport matplotlib.pyplot as plt\n\n# --- 1. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ë°ì´í„° ì¤€ë¹„ ---\nprint(f\"ë°ì´í„° ê¸¸ì´: {len(data)}, Optunaê°€ ì°¾ì€ ìµœì  ìœˆë„ìš° í¬ê¸°: {best_window_size}\")\n\n# ìœˆë„ìš° ìƒì„± ë° ì •ê·œí™”\noptimal_raw_windows = create_sliding_windows(data, best_window_size)\n\n# ìœˆë„ìš° ìƒì„± ì‹¤íŒ¨ ì‹œ í´ë°± ë¡œì§\nif len(optimal_raw_windows) == 0:\n    print(f\"ê²½ê³ : ìœˆë„ìš° í¬ê¸° {best_window_size}ë¡œ ìœˆë„ìš°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì‘ì€ í¬ê¸°ë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.\")\n    # ë°ì´í„° ê¸¸ì´ì— ë§ëŠ” ì•ˆì „í•œ ìœˆë„ìš° í¬ê¸° ë¦¬ìŠ¤íŠ¸\n    possible_sizes = [s for s in [15, 12, 10, 8, 5] if s &lt; len(data)]\n    for safe_size in sorted(possible_sizes, reverse=True):\n        optimal_raw_windows = create_sliding_windows(data, safe_size)\n        if len(optimal_raw_windows) &gt; 0:\n            best_window_size = safe_size\n            print(f\"ì„±ê³µ: ìœˆë„ìš° í¬ê¸°ë¥¼ {safe_size}ë¡œ ë³€ê²½í•˜ì—¬ {len(optimal_raw_windows)}ê°œ ìœˆë„ìš° ìƒì„±\")\n            break\n    if len(optimal_raw_windows) == 0:\n        raise ValueError(\"ë°ì´í„°ì— ë§ëŠ” ìœˆë„ìš°ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n\noptimal_normalized_windows, optimal_scalers = normalize_windows(optimal_raw_windows)\nall_windows_torch = torch.from_numpy(optimal_normalized_windows).unsqueeze(1).float()\n\n# ì •ìƒ ìœˆë„ìš° í•„í„°ë§\nnormal_window_indices = []\nfor i in range(len(optimal_normalized_windows)):\n    window_range = range(i, i + best_window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\nprint(f\"ìµœì¢… ìœˆë„ìš° í¬ê¸°: {best_window_size}\")\nprint(f\"ìƒì„±ëœ ì „ì²´ ìœˆë„ìš° ìˆ˜: {len(all_windows_torch)}\")\nprint(f\"ì •ìƒ ìœˆë„ìš° ìˆ˜: {len(normal_window_indices)}\")\n\nif len(normal_window_indices) &lt; 5:\n    raise ValueError(\"ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì •ìƒ ìœˆë„ìš° ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.\")\n\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# --- 2. ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ ---\nfinal_model = CNNAutoencoderWithDropout(input_shape=(best_window_size, 1), dropout_rate=best_dropout)\noptimizer = getattr(optim, best_optimizer)(final_model.parameters(), lr=best_lr)\ncriterion = nn.MSELoss()\n\nfull_normal_loader = DataLoader(normal_dataset, batch_size=min(16, len(normal_dataset)), shuffle=True)\nepochs = 100\nprint(\"ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\nfor epoch in range(epochs):\n    for data_batch in full_normal_loader:\n        inputs = data_batch[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. ì„ê³„ê°’ ì„¤ì • ë° ì´ìƒì¹˜ íƒì§€ ---\nfinal_model.eval()\n\n# í•™ìŠµ ë°ì´í„°(ì •ìƒ ìœˆë„ìš°)ì˜ ì¬êµ¬ì„± ì˜¤ì°¨ë¡œ ì„ê³„ê°’ ì„¤ì •\nwith torch.no_grad():\n    if len(normal_windows_torch) &gt; 0:\n        reconstructed_train = final_model(normal_windows_torch)\n        error_train = torch.mean((normal_windows_torch - reconstructed_train)**2, dim=(1, 2))\n        train_reconstruction_error = error_train.numpy()\n        \n        quantile_level = 0.995\n        threshold = np.quantile(train_reconstruction_error, quantile_level)\n        print(f\"ì„ê³„ê°’ ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n    else:\n        quantile_level = \"N/A\"\n        threshold = 0.05 \n        print(f\"ê²½ê³ : í•™ìŠµ ë°ì´í„°ê°€ ì—†ì–´ ê³ ì • ì„ê³„ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {threshold}\")\n\n# ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì¬êµ¬ì„± ì˜¤ì°¨ ê³„ì‚°\nwith torch.no_grad():\n    reconstructed_all = final_model(all_windows_torch)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed_all)**2, dim=(1, 2)).numpy()\n    pointwise_error = ((all_windows_torch - reconstructed_all)**2).squeeze().numpy()\n\nanomaly_window_indices = np.where(mean_error_per_window &gt; threshold)[0]\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    if window_idx &lt; len(pointwise_error):\n        errors_in_window = pointwise_error[window_idx]\n        max_error_idx_in_window = np.argmax(errors_in_window)\n        absolute_idx = window_idx + max_error_idx_in_window\n        predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"íƒì§€ëœ ì´ìƒì¹˜ í¬ì¸íŠ¸ ì¸ë±ìŠ¤: {predicted_anomaly_points}\")\n\n# --- 4. ê²°ê³¼ ì‹œê°í™” ---\nplt.figure(figsize=(10, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(data, label='ì›ë³¸ ë°ì´í„°', alpha=0.8)\nplt.scatter(outliers, data[outliers], color='red', s=120, label='ì‹¤ì œ ì´ìƒì¹˜', marker='o', edgecolors='black', zorder=5)\nif predicted_anomaly_points:\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(data)]\n    plt.scatter(valid_indices, data[valid_indices], color='orange', marker='x', s=120, linewidth=2, label='íƒì§€ëœ ì´ìƒì¹˜', zorder=5)\nplt.title('ìµœì¢… ëª¨ë¸ ì´ìƒ íƒì§€ ê²°ê³¼', fontsize=16)\nplt.xlabel('ì‹œê°„ ìŠ¤í…')\nplt.ylabel('ë°ì´í„° ê°’')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.subplot(2, 1, 2)\nplt.plot(mean_error_per_window, label='ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨', color='blue')\nif quantile_level != \"N/A\":\n    threshold_label = f'ì„ê³„ê°’ ({quantile_level*100:.1f}% Quantile = {threshold:.4f})'\nelse:\n    threshold_label = f'ê³ ì • ì„ê³„ê°’ ({threshold:.4f})'\nplt.axhline(y=threshold, color='r', linestyle='--', label=threshold_label)\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, mean_error_per_window[anomaly_window_indices], c='red', s=100, label='ì´ìƒì¹˜ë¡œ íƒì§€ëœ ìœˆë„ìš°', zorder=5)\nplt.title('ìœˆë„ìš°ë³„ ì¬êµ¬ì„± ì˜¤ì°¨', fontsize=16)\nplt.xlabel('ìœˆë„ìš° ì¸ë±ìŠ¤')\nplt.ylabel('ì¬êµ¬ì„± ì˜¤ì°¨ (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()\n\në°ì´í„° ê¸¸ì´: 100, Optunaê°€ ì°¾ì€ ìµœì  ìœˆë„ìš° í¬ê¸°: 10\nìµœì¢… ìœˆë„ìš° í¬ê¸°: 10\nìƒì„±ëœ ì „ì²´ ìœˆë„ìš° ìˆ˜: 91\nì •ìƒ ìœˆë„ìš° ìˆ˜: 61\nìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...\nEpoch [20/100], Loss: 0.036196\nEpoch [40/100], Loss: 0.026104\nEpoch [60/100], Loss: 0.021782\nEpoch [80/100], Loss: 0.013832\nEpoch [100/100], Loss: 0.010171\nì„ê³„ê°’ (99.5% Quantile): 0.032453\níƒì§€ëœ ì´ìƒì¹˜ í¬ì¸íŠ¸ ì¸ë±ìŠ¤: [19, 20, 21, 49, 50, 51, 52, 53, 54, 55, 74, 79, 80, 81]\n\n\n\n\n\n\n\n\n\n\n\n4.2. ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ vs ê°œì„  ëª¨ë¸\n\n\n\n\n\n\n\n\n\n\n\nêµ¬ë¶„\në°ì´í„° ì „ì²˜ë¦¬\nê³¼ì í•© ë°©ì§€\ní•˜ì´í¼íŒŒë¼ë¯¸í„°\nì„ê³„ê°’ ì„¤ì •\níƒì§€ëœ ì´ìƒì¹˜ (ì¸ë±ìŠ¤)\n\n\n\n\nWeek3 (ë² ì´ìŠ¤ë¼ì¸)\nSigmoid í™œì„±í™”\nì—†ìŒ\nìˆ˜ë™ ì„¤ì •\nê³ ì • ì„ê³„ê°’\n[17 18 19 20 47 48 49 50] (ìœˆë„ìš°)\n\n\nWeek4 (ê°œì„  ëª¨ë¸)\nìœˆë„ìš°ë³„ ì •ê·œí™”\nDropout\nOptuna ìµœì í™”\në™ì  Quantile\n[20, 50, 80] (ë‹¨ì¼ í¬ì¸íŠ¸)"
  },
  {
    "objectID": "posts/project-abc-04-model-optimization/index.html#ê²°ë¡ ",
    "href": "posts/project-abc-04-model-optimization/index.html#ê²°ë¡ ",
    "title": "[2025 ABC í”„ë¡œì íŠ¸ ë©˜í† ë§ 8ê¸°] 4ì£¼ì°¨ - ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”",
    "section": "5. ê²°ë¡ ",
    "text": "5. ê²°ë¡ \nì´ë²ˆ 4ì£¼ì°¨ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Week3ì—ì„œ êµ¬í˜„í•œ CNN ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì²´ê³„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ê³¼ì •ì„ ìƒì„¸íˆ ë‹¤ë£¨ì—ˆìŠµë‹ˆë‹¤.\n\nì£¼ìš” ê°œì„ ì‚¬í•­\n\në°ì´í„° ì „ì²˜ë¦¬ ë°©ì‹ ë³€ê²½: ê° ì‹œê³„ì—´ ìœˆë„ìš°ë³„ë¡œ ë…ë¦½ì ì¸ ì •ê·œí™”ë¥¼ ì ìš©í•˜ì—¬ ì§€ì—­ì  íŒ¨í„´ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤.\nê³¼ì í•© ë°©ì§€: Dropout ë ˆì´ì–´ ì¶”ê°€ë¡œ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.\ní•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”: Optunaë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµë¥ , ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ ë“± í•µì‹¬ íŒŒë¼ë¯¸í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.\nìœˆë„ìš° í¬ê¸° ìµœì í™”: ë°ì´í„°ì˜ íŠ¹ì„±ì— ë§ëŠ” ìµœì ì˜ ìœˆë„ìš° í¬ê¸°ë¥¼ ë™ì ìœ¼ë¡œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\níš¨ìœ¨ì ì¸ í•™ìŠµ: ì¡°ê¸° ì¢…ë£Œ(Early Stopping)ë¥¼ êµ¬í˜„í•˜ì—¬ ë¶ˆí•„ìš”í•œ í•™ìŠµì„ ë°©ì§€í•˜ê³  ìµœì ì˜ ëª¨ë¸ ìƒíƒœë¥¼ í¬ì°©í–ˆìŠµë‹ˆë‹¤.\n\n\n\ní•µì‹¬ ì„±ê³¼\níŠ¹íˆ, ì •ìƒ ë°ì´í„°ë§Œìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ì¬êµ¬ì„± ì˜¤ì°¨ì— ê¸°ë°˜í•œ ëª…í™•í•œ ì„ê³„ê°’ ì„¤ì •ì„ í†µí•´ ê¸°ì¡´ ëª¨ë¸ì´ ë†“ì³¤ë˜ ì‹¤ì œ ì´ìƒì¹˜(20, 50, 80ë²ˆ ì¸ë±ìŠ¤)ë¥¼ ëª¨ë‘ ì •í™•í•˜ê²Œ íƒì§€í•˜ëŠ” ë° ì„±ê³µí–ˆìŠµë‹ˆë‹¤. ë˜í•œ ìœˆë„ìš° í¬ê¸°ì™€ ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë™ì‹œì— ìµœì í™”í•¨ìœ¼ë¡œì¨, ìˆ˜ë™ ì„¤ì •ì— ë¹„í•´ í›¨ì”¬ ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤ë¥¼ ì •ë¦½í–ˆìŠµë‹ˆë‹¤.\n\n\ní•œê³„ ë° í–¥í›„ ê³¼ì œ\nì´ë²ˆì— ì‚¬ìš©í•œ ì˜ˆì œ ë°ì´í„°ëŠ” íŒ¨í„´ì´ ë¹„êµì  ë‹¨ìˆœí•˜ì§€ë§Œ, ì‹¤ì œ ë°ì´í„°ëŠ” ë” ë³µì¡í•œ ê³„ì ˆì„±ê³¼ ë…¸ì´ì¦ˆë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ë‹¤ìŒ 5ì£¼ì°¨ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ì‹¤ì œ ì‚°ì—… ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì´ë²ˆì— êµ¬ì¶•í•œ ëª¨ë¸ì˜ ì‹¤íš¨ì„±ì„ ê²€ì¦í•˜ê³ , ë” ë³µì¡í•œ ë°ì´í„° íŒ¨í„´ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ê³ ë„í™”ëœ ì „ì²˜ë¦¬ ê¸°ë²•ê³¼ ëª¨ë¸ êµ¬ì¡°ë¥¼ íƒêµ¬í•  ì˜ˆì •ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/guide-github-01-branch-strategy/index.html",
    "href": "posts/guide-github-01-branch-strategy/index.html",
    "title": "[GitHub] Feature ë“± ë¸Œëœì¹˜ í™œìš©ë²•, ì»¤ë°‹ ë©”ì‹œì§€ ì‘ì„±ë²•",
    "section": "",
    "text": "â€œmainì— ë°”ë¡œ í‘¸ì‹œí•˜ì§€ ë§ˆì„¸ìš”.â€ íŒ€ í”„ë¡œì íŠ¸ì—ì„œ ìì£¼ ë“£ëŠ” ë§ì…ë‹ˆë‹¤.\nì™œ ê·¸ë˜ì•¼ í•˜ëŠ”ì§€, ë˜ ì–´ë–»ê²Œ ì“°ë©´ í˜‘ì—…ì— ë” ë„ì›€ì´ ë˜ëŠ”ì§€ ì •ë¦¬í•´ ë´¤ìŠµë‹ˆë‹¤.\n\n\nì™œ ë¸Œëœì¹˜ë¥¼ ë‚˜ëˆ ì•¼ í• ê¹Œ?\n\nmainì€ ì–¸ì œë‚˜ ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ: ë°°í¬ìš© main ë¸Œëœì¹˜ëŠ” í•­ìƒ ê¹¨ë—í•˜ê²Œ ìœ ì§€ë¼ì•¼ ë¬¸ì œê°€ ìƒê²¨ë„ ë¹ ë¥´ê²Œ ë³µêµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê¸°ëŠ¥ë³„ë¡œ ì‘ì—…í•´ì„œ ì¶©ëŒ ì¤„ì´ê¸°: ê°ì ë¸Œëœì¹˜ë¥¼ ë”°ë¡œ ë§Œë“¤ì–´ ì‘ì—…í•˜ë©´, ì¶©ëŒì´ ë‚˜ë”ë¼ë„ ë‚´ê°€ ê±´ë“œë¦° ë¶€ë¶„ë§Œ ì§‘ì¤‘í•´ì„œ ê³ ì¹˜ë©´ ë©ë‹ˆë‹¤.\nì½”ë“œ ë¦¬ë·°ì— ìœ ë¦¬: ë¸Œëœì¹˜ë§ˆë‹¤ Pull Request(PR)ë¥¼ ë§Œë“¤ë©´ ë³€ê²½ ì‚¬í•­ì„ ë¬¶ìŒ ë‹¨ìœ„ë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´ ë¦¬ë·°ì™€ í† ë¡ ì´ í›¨ì”¬ ìˆ˜ì›”í•´ì§‘ë‹ˆë‹¤.\n\n\n\nì¶”ì²œ ë¸Œëœì¹˜ ì¢…ë¥˜\nì•„ë˜ëŠ” ì†Œê·œëª¨ íŒ€/ëŒ€í•™ íŒ€í”Œì—ì„œ ì“°ê¸° ì¢‹ì€ ê°„ë‹¨í•œ ë¸Œëœì¹˜ ì „ëµ ì˜ˆì‹œì…ë‹ˆë‹¤.\n\n\n\në¸Œëœì¹˜\nìš©ë„\níŠ¹ì§•\n\n\n\n\nmain\në°°í¬ìš© ìµœì¢… ì½”ë“œ\nì§ì ‘ ì»¤ë°‹ ê¸ˆì§€, PRë¡œë§Œ ë¨¸ì§€\n\n\ndev\nê°œë°œìš© í†µí•© ì½”ë“œ\nê¸°ëŠ¥ ë¸Œëœì¹˜ë“¤ì„ í•©ì¹˜ëŠ” ê¸°ì¤€ ë¸Œëœì¹˜\n\n\nfeature/*\nê¸°ëŠ¥ ê°œë°œ\ndevì—ì„œ ë¸Œëœì¹˜ë¥¼ ë§Œë“¤ì–´ ì‘ì—…\n\n\nhotfix/*\nê¸´ê¸‰ ë²„ê·¸ ìˆ˜ì •\nmainì—ì„œ ë°”ë¡œ ë¸Œëœì¹˜ë¥¼ ë”°ì„œ ìˆ˜ì •\n\n\n\n\n\nê°„ë‹¨í•œ ì‘ì—… íë¦„\nì‹¤ì œë¡œëŠ” devë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‘ì—…í•˜ëŠ” ê²½ìš°ê°€ ë§ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¥¼ ìœ„í•´ main ê¸°ì¤€ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n# 1. ìµœì‹  ì½”ë“œë¡œ ì—…ë°ì´íŠ¸\ngit checkout main\ngit pull origin main\n\n# 2. ê¸°ëŠ¥ ë¸Œëœì¹˜ ë§Œë“¤ê¸°\ngit checkout -b feature/12-login-ui\n\n# 3. ì½”ë“œ ì§œê³  ì»¤ë°‹\ngit add .\ngit commit -m \"feat: ë¡œê·¸ì¸ í¼ UI êµ¬í˜„\"\n\n# 4. ë‚´ ë¸Œëœì¹˜ì— í‘¸ì‹œ\ngit push -u origin feature/12-login-ui\níŒ€ì—ì„œ dev ë¸Œëœì¹˜ë¥¼ ìš´ì˜í•œë‹¤ë©´, ìœ„ ì˜ˆì‹œì—ì„œ mainì„ devë¡œ ë°”ê¿”ì„œ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n\n\nì¢‹ì€ ì»¤ë°‹ ë©”ì‹œì§€ ì‘ì„±í•˜ê¸°\nì»¤ë°‹ ë©”ì‹œì§€ëŠ” â€œë‚˜ì¤‘ì— ì´ë ¥ì´ ì´í•´ë˜ëŠëƒâ€ë¥¼ ì¢Œìš°í•˜ëŠ” ì¤‘ìš”í•œ ê¸°ë¡ì…ë‹ˆë‹¤.\níŒ€ì›ë“¤ì´ ë³€ê²½ ë‚´ìš©ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡, ì•„ë˜ì²˜ëŸ¼ ìµœì†Œí•œì˜ ê·œì¹™ì„ ë§ì¶° ë‘ëŠ” ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.\n\n1. ì»¤ë°‹ ìœ í˜•(Type) ëª…ì‹œ\n\n\n\nìœ í˜•\nì„¤ëª…\n\n\n\n\nfeat\nìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€\n\n\nfix\në²„ê·¸ ìˆ˜ì •\n\n\ndocs\në¬¸ì„œë§Œ ìˆ˜ì •\n\n\nstyle\nì½”ë“œ í¬ë§·íŒ…, ì„¸ë¯¸ì½œë¡  ëˆ„ë½ ë“± (ë™ì‘ ë³€í™” ì—†ìŒ)\n\n\nrefactor\nê¸°ëŠ¥ ë³€í™” ì—†ì´ ì½”ë“œ êµ¬ì¡° ê°œì„ \n\n\ntest\ní…ŒìŠ¤íŠ¸ ì½”ë“œ ì¶”ê°€/ìˆ˜ì •\n\n\nchore\në¹Œë“œ/ë°°í¬ ì„¤ì •, íŒ¨í‚¤ì§€ ê´€ë¦¬ ë“± (í”„ë¡œë•ì…˜ ì½”ë“œ ë³€í™” ì—†ìŒ)\n\n\n\n\n\n2. ì œëª©ê³¼ ë³¸ë¬¸ ë¶„ë¦¬\n\nì œëª©: 50ì ì´ë‚´, ê°€ëŠ¥í•œ í•œ â€œë¬´ì—‡ì„ í–ˆëŠ”ì§€â€ í•œ ì¤„ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. (ì˜ˆ: feat: ë¡œê·¸ì¸ ê¸°ëŠ¥ ì¶”ê°€)\në³¸ë¬¸: êµ¬í˜„ ë””í…Œì¼ë³´ë‹¤ëŠ” â€œë¬´ì—‡ì„, ì™œ ë°”ê¿¨ëŠ”ì§€â€ì— ì´ˆì ì„ ë§ì¶° ì ìŠµë‹ˆë‹¤.\n\nì˜ˆì‹œ:\nfeat: ë¡œê·¸ì¸ í¼ UI êµ¬í˜„\n\n- ì´ë©”ì¼/ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ í•„ë“œ ì¶”ê°€\n- í¼ ìœ íš¨ì„± ê²€ì‚¬ ë¡œì§ ì ìš©\n- ì˜ëª»ëœ ì…ë ¥ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ\nì´ ì •ë„ ê·œì¹™ë§Œ íŒ€ì—ì„œ ë§ì¶°ë„, ë‚˜ì¤‘ì— git logë¥¼ ë³´ë©´ì„œ â€œì´ ì»¤ë°‹ì´ ë­˜ ë°”ê¾¼ ê±´ì§€â€ë¥¼ í›¨ì”¬ ëœ ê³ ìƒí•˜ë©´ì„œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "assets/profile-summary.html",
    "href": "assets/profile-summary.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "ğŸ‘€ About Me"
  },
  {
    "objectID": "assets/profile-summary.html#tech-stack",
    "href": "assets/profile-summary.html#tech-stack",
    "title": "beomdo's ML-DL blog",
    "section": "Tech Stack",
    "text": "Tech Stack\n\nLanguages\n\n \n\n\n\nAI & Quantum Sciences\n\n   \n\n\n\nData & Tools"
  },
  {
    "objectID": "assets/profile-summary.html#activities",
    "href": "assets/profile-summary.html#activities",
    "title": "beomdo's ML-DL blog",
    "section": "Activities",
    "text": "Activities\n\n\n\nê¸°ê°„\ní™œë™ëª…\në‚´ìš©\nê¸°ê´€(ì¥ì†Œ)\n\n\n\n\n2026.01.20 ~ 01.27\nAAAI-26 Student Abstract and Poster Program (Accept)\nMulti-Stage Reinforcement Learning for Robust Charging of Quantum Batteries\nAAAI-26 (Singapore EXPO, Singapore)\n\n\n2025.11.20\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ í•™ë¶€ìƒ ìº¡ìŠ¤í†¤ ê²½ì§„ëŒ€íšŒ\n(ğŸ†ìš°ìˆ˜ìƒ ìˆ˜ìƒ) ì–‘ìë°°í„°ë¦¬ ì´ˆí¡ìˆ˜ ì´ë“ ê·¹ëŒ€í™”ë¥¼ ìœ„í•œ ê·¸ë˜í”„ ê¸°ë°˜ ê°•í™”í•™ìŠµ ì¶©ì „ ì œì–´ [ë°•ì¤€ì„±, ë°•ë²”ë„, ì¥í˜„ì„]\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.19 ~ 11.21\ní•œêµ­í†µì‹ í•™íšŒ ì¶”ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ ìš°ìˆ˜ë…¼ë¬¸ìƒ(í•™ë¶€ìƒ)\n(ğŸ†ì¥ë ¤ìƒ ìˆ˜ìƒ) ë‹¨ê³„ì  ê°•í™”í•™ìŠµ í”„ë ˆì„ì›Œí¬ë¥¼ í†µí•œ ê°•ê±´í•œ ì–‘ì ë°°í„°ë¦¬ ì¶©ì „(2ì €ì)\në¼í•œì…€ë ‰íŠ¸ ê²½ì£¼\n\n\n2025.11.01\nğŸªª TOPCIT ì •ê¸°í‰ê°€ (ìˆ˜ì¤€ 3 ë‹¬ì„±)\nìˆ˜ì¤€3: ê¸°ìˆ  ë° ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­ì— ëŒ€í•œ ì§€ì‹ê³¼ ìŠ¤í‚¬ì„ ì ìš©í•˜ì—¬ ê³¼ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€\nì •ë³´í†µì‹ ê¸°íší‰ê°€ì›(IITP)\n\n\n2025.10.31\níŠ¹í—ˆ ì¶œì›\nì»¤ë¦¬í˜ëŸ¼ ê°•í™”í•™ìŠµì„ ì´ìš©í•œ ì–‘ì ë°°í„°ë¦¬ì˜ ê°•ê±´í•œ ì¶©ì „ ì‹œìŠ¤í…œ / ê¸°ì—¬ë„ 15% (ì¶œì›/ì‹¬ì‚¬ì¤‘)\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì‚°í•™í˜‘ë ¥ë‹¨\n\n\n2025.10.30\nì»´í“¨í„°ê³µí•™ê³¼ í¬íŠ¸í´ë¦¬ì˜¤ ê²½ì§„ëŒ€íšŒ\nğŸ†ìš°ìˆ˜ìƒ\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.07.14\nA Comparative Study of Customized Algorithms for Anomaly Detection in Industry-Specific Power Data(SCIE)\nDBA K-Means ëª¨ë¸ ë¶€ë¶„(ê³µì €ì)\nMDPI journal energies\n\n\n2025.06.30 ~ 07.11\nê¸°ì—… ì¸í„´ì‹­\nììœ¨ì£¼í–‰ ë¡œë´‡ ì‹œë®¬ë ˆì´ì…˜ ë° ê²½ë¡œ ìµœì í™” (Gazebo ì´ìš©)\n(ì£¼) ì•„êµ°\n\n\n2025.06.24 ~ 06.27\nëŒ€í•œì „ìê³µí•™íšŒ í•˜ê³„ ì¢…í•©í•™ìˆ ëŒ€íšŒ\nê³ ì† í‘¸ë¦¬ì— ë³€í™˜(FFT) ê¸°ë°˜ ì£¼ê¸° ì¶”ì¶œ ë° ìœˆë„ìš° êµ¬ì„±ì„ í™œìš©í•œ GELU CNN-GRU AE ëª¨ë¸ì˜ ì‚°ì—… ì „ë ¥ ì‹œê³„ì—´ ì´ìƒì¹˜ íƒì§€\në¡¯ë°í˜¸í…” ì œì£¼(ì¤‘ë¬¸)\n\n\n2025.05.20 ~ 06.30\nABC í”„ë¡œì íŠ¸ ë©˜í† ë§\nì‚°ì—… ì „ë ¥ ì†Œë¹„ëŸ‰ ì´ìƒì¹˜ ê²€ì¶œ ë° ìµœì í™”\nìœ í´ë¦¬ë“œ ì†Œí”„íŠ¸\n\n\n2025.06.13\nğŸªª ë°ì´í„°ë¶„ì„ ì¤€ì „ë¬¸ê°€(ADsP) ìê²©ì¦ ì·¨ë“\në°ì´í„° ë¶„ì„ ê¸°íš ë° ìˆ˜í–‰ ì‹¤ë¬´ì ìê²©\ní•œêµ­ë°ì´í„°ì‚°ì—…ì§„í¥ì›\n\n\n2025.05.01 ~\nEco AI Lab ë©ì¥\nì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì§€ì›, ì¼ì • ì¡°ìœ¨, í–‰ì • ì—…ë¬´ ë“±\nêµ­ë¦½í•œë°­ëŒ€í•™êµ EcoAI Lab\n\n\n2025.04.07 ~\nì‚°í•™í˜‘ë ¥ í”„ë¡œì íŠ¸\nììœ¨ì£¼í–‰ ì£¼ì°¨ë¡œë´‡ ìš´ì˜ ì†Œí”„íŠ¸ì›¨ì–´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.24 ~\nì†Œì¤‘í•œ JUMP-UP Labs\nAI(ì¸ê³µì§€ëŠ¥)ë¥¼ í™œìš©í•œ ì‚°ì—…ì²´ ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nêµ­ë¦½í•œë°­ëŒ€í•™êµ SWì¤‘ì‹¬ëŒ€í•™ì‚¬ì—…ë‹¨\n\n\n2025.03.04 ~ 06.13\në°ì´í„°ì‚¬ì´ì–¸ìŠ¤(ì´ìƒê¸ˆ êµìˆ˜ë‹˜) ì‹¤ìŠµì¡°êµ 25-1í•™ê¸°\nì‹¤ìŠµìë£Œ ì œì‘ ë° ì§ˆì˜ì‘ë‹µ ê´€ë¦¬\nêµ­ë¦½í•œë°­ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼\n\n\n2025.02.05 ~ 02.07\ní•œêµ­í†µì‹ í•™íšŒ ë™ê³„ì¢…í•©í•™ìˆ ë°œí‘œíšŒ\nDBA K-Means êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ í™”ê³µì‚°ì—… ì „ë ¥ ì‚¬ìš©ëŸ‰ ì´ìƒì¹˜ íƒì§€\nê°•ì›ë„ ìš©í‰ë¦¬ì¡°íŠ¸\n\n\n2025.01.16 ~ 01.21\nIROC 2024/25 ì„¸ê³„ëŒ€íšŒ\nëŒ€íšŒ ì§„í–‰ìš”ì›\në¶€ì‚° ë²¡ìŠ¤ì½”"
  }
]