[
  {
    "objectID": "posts/ABC_week01_data analysis/index.html",
    "href": "posts/ABC_week01_data analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "",
    "text": "유클리드소프트에서 진행하는 ABC 프로젝트 멘토링에 8기로 참여하게 되었습니다.   [산업 전력 데이터의 이상치 탐지 성능 향상 솔루션 구축]을 주제로 다양한 데이터 분석 및 인공지능 기법을 학습하고 실제 프로젝트에 적용해볼 예정입니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 데이터란?",
    "text": "시계열 데이터란?\n시계열 데이터(Time Series Data)는 일정 시간 간격으로 기록된 데이터 포인트들의 순차적인 집합입니다. 예를 들어, 시간별 산업 설비의 전력 사용량, 일별 주가, 월별 웹사이트 방문자 수 등이 시계열 데이터에 해당합니다. 이러한 데이터는 시간의 흐름에 따른 변화와 패턴을 분석하는 데 사용되며, 특히 정상적인 패턴에서 벗어나는 ’이상치’를 탐지하는 데 중요한 기초 자료가 됩니다.\n시계열 데이터는 주로 다음과 같은 특징을 가집니다:\n\n추세 (Trend): 데이터가 장기적으로 증가하거나 감소하는 경향.\n계절성 (Seasonality): 특정 주기(예: 하루, 주, 월)에 따라 반복되는 패턴.\n주기성 (Cyclicality): 계절성보다 긴, 고정되지 않은 주기의 변동.\n불규칙 변동 (Irregular Fluctuations/Noise): 위 요소들로 설명되지 않는 무작위적 변동."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 이상 탐지에서 EDA와 전처리의 중요성",
    "text": "시계열 이상 탐지에서 EDA와 전처리의 중요성\n이상치(Anomaly) 또는 특이점(Outlier)은 일반적인 데이터 패턴에서 현저하게 벗어나는 관측치를 의미합니다. 산업 전력 데이터에서 이상치는 설비 고장, 에너지 누수, 비정상적 공정 운영 등 중요한 문제를 시사할 수 있습니다. 효과적인 이상 탐지를 위해서는 데이터에 대한 깊이 있는 이해가 선행되어야 하며, 탐색적 데이터 분석(EDA)과 적절한 전처리는 이 과정의 핵심입니다.\nEDA와 전처리가 중요한 이유:\n\n데이터 특성 파악: 데이터의 분포, 추세, 계절성 등 기본적인 통계적 특성을 이해하여 ‘정상’ 상태의 기준을 설정하는 데 도움을 줍니다.\n잠재적 이상치 식별: 시각화 등을 통해 예상치 못한 급증, 급감 또는 패턴 변화를 초기에 발견할 수 있습니다.\n데이터 품질 향상: 결측치 처리, 노이즈 제거 등을 통해 분석의 정확도를 높입니다.\n피처 엔지니어링 기반 마련: 분석 목적에 맞는 새로운 변수를 생성하거나 기존 변수를 변환하는 데 필요한 통찰력을 제공합니다.\n적절한 이상 탐지 모델 선택 지원: 데이터의 특성에 맞는 이상 탐지 알고리즘을 선택하는 데 중요한 정보를 제공합니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "href": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초",
    "text": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초\nPython의 pandas, numpy, matplotlib, seaborn 라이브러리를 사용하여 산업 전력 사용량 데이터를 가정하고, 이상 탐지를 위한 기본적인 EDA 및 전처리 과정을 살펴보겠습니다.\n\n1. 필요한 라이브러리 불러오기\n데이터 분석 및 시각화에 필요한 라이브러리를 가져옵니다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 향상된 시각화를 위한 Seaborn\nfrom datetime import datetime\n\n# 경고 메시지 무시 (선택 사항)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n2. 분석용 샘플 시계열 데이터 생성 (가상 산업 전력 사용량)\n실제 산업 전력 데이터와 유사한 특성을 갖도록 가상 데이터를 생성합니다. 여기에는 일정한 기본 사용량, 약간의 증가 추세, 주간 계절성(평일 사용량 증가, 주말 감소), 그리고 몇 개의 인위적인 이상치(스파이크 및 급감)를 포함시킵니다.\n\n# 재현성을 위한 시드 설정\nnp.random.seed(42)\n\n# 날짜 범위 생성 (약 1년)\ndate_rng = pd.date_range(start='2025-01-01', periods=365, freq='D')\ndata = pd.DataFrame(date_rng, columns=['date'])\n\n# 기본 전력 사용량 설정 및 추세 생성\nbaseline_usage = 100  # 기본 사용량 (예: kWh)\ntrend_factor = np.linspace(0, 20, len(date_rng)) # 선형 증가 추세\n\n# 주간 계절성 생성 (월:0 ~ 일:6)\n# 산업 데이터 특성상 평일 사용량 높고, 주말 낮음\nday_of_week_effect = np.array([15, 18, 20, 19, 17, 5, 3])\nseasonal_factor = np.array([day_of_week_effect[day.weekday()] for day in date_rng])\n\n# 임의의 노이즈 생성\nnoise = np.random.normal(0, 5, size=(len(date_rng))) # 평균 0, 표준편차 5\n\n# 데이터 생성 (전력 사용량 = 기본값 + 추세 + 계절성 + 노이즈)\ndata['power_usage'] = baseline_usage + trend_factor + seasonal_factor + noise\n\n# 인위적인 이상치(스파이크 및 급감) 추가\ndata.loc[data.index[50], 'power_usage'] += 70  # 51번째 날에 큰 스파이크\ndata.loc[data.index[150], 'power_usage'] -= 50 # 151번째 날에 큰 폭 하락\ndata.loc[data.index[250], 'power_usage'] += 80  # 251번째 날에 큰 스파이크\n\n# 데이터 값 보정 (음수 방지 및 최소값 설정)\ndata['power_usage'] = data['power_usage'].astype(float).clip(lower=10)\n\n# 'date' 컬럼을 인덱스로 설정\ndata.set_index('date', inplace=True)\n\nprint(\"생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\")\nprint(data.head())\nprint(\"\\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\")\nprint(data.tail())\n\n생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\n            power_usage\ndate                   \n2025-01-01   122.483571\n2025-01-02   118.363624\n2025-01-03   120.348333\n2025-01-04   112.779984\n2025-01-05   102.049013\n\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\n            power_usage\ndate                   \n2025-12-27   127.376952\n2025-12-28   130.498859\n2025-12-29   134.346309\n2025-12-30   139.953614\n2025-12-31   143.450720\n\n\n이 샘플 데이터는 power_usage라는 이름으로 전력 사용량 정보를 가지며, EDA 과정에서 이상치를 시각적으로 탐색하는 데 사용됩니다.\n\n\n\n3. 데이터 기본 탐색\n데이터의 구조와 기본적인 통계적 특성을 확인합니다.\n\nprint(\"데이터 정보:\")\ndata.info()\n\nprint(\"\\n기술 통계량:\")\nprint(data.describe())\n\nprint(f\"\\n결측치 확인: {data.isnull().sum().sum()} 개\")\n# data.isnull().sum() # 컬럼별 결측치 확인\n\n데이터 정보:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 365 entries, 2025-01-01 to 2025-12-31\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   power_usage  365 non-null    float64\ndtypes: float64(1)\nmemory usage: 5.7 KB\n\n기술 통계량:\n       power_usage\ncount   365.000000\nmean    124.197677\nstd      11.877300\nmin      64.494222\n25%     117.423353\n50%     124.093359\n75%     131.043033\nmax     202.431844\n\n결측치 확인: 0 개\n\n\ninfo()는 데이터 타입, 인덱스 정보, 메모리 사용량 등을 보여줍니다. describe()는 평균, 표준편차, 최소/최대값, 사분위수 등 주요 기술 통계량을 제공하여 데이터의 전반적인 분포를 파악하는 데 도움을 줍니다. 결측치가 있다면 이상 탐지 분석 전에 적절히 처리(예: 보간, 제거)해야 합니다. 이 샘플에서는 결측치가 없습니다.\n\n\n\n4. 주요 시각화를 통한 탐색적 데이터 분석 (EDA)\n시각화는 시계열 데이터의 패턴과 잠재적 이상치를 발견하는 데 매우 효과적입니다.\n\n4.1. 기본 시계열 플롯\n전체 기간에 대한 전력 사용량 변화를 시각화하여 추세, 계절성, 그리고 눈에 띄는 이상 패턴을 관찰합니다.\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='dodgerblue', linewidth=1.5)\nplt.title('일별 가상 산업 전력 사용량', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: 일별 가상 산업 전력 사용량\n\n\n\n\n\n위 그래프에서 전반적인 증가 추세와 주기적인 변동(계절성) 외에도, 몇몇 지점에서 급격한 스파이크나 하락(우리가 삽입한 이상치)이 시각적으로 확인됩니다. 실제 데이터 분석 시 이러한 지점들이 조사 대상이 됩니다.\n\n\n\n4.2. 데이터 분포 확인 (히스토그램 및 KDE)\n전력 사용량 값들의 분포를 확인하여 데이터가 특정 구간에 집중되어 있는지, 또는 분포에서 벗어나는 값들이 있는지 살펴봅니다.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data['power_usage'], kde=True, color='mediumseagreen', bins=30)\nplt.title('전력 사용량 분포 (히스토그램 및 KDE)', fontsize=16)\nplt.xlabel('전력 사용량 (kWh)', fontsize=12)\nplt.ylabel('빈도', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: 전력 사용량 분포\n\n\n\n\n\n히스토그램과 KDE(Kernel Density Estimate) 플롯은 데이터 값의 분포를 보여줍니다. 만약 분포의 꼬리 부분에 값이 드물게 나타난다면 이는 이상치일 가능성이 있습니다. 우리가 삽입한 인위적인 스파이크 값들이 분포의 오른쪽 꼬리 부분에 나타날 수 있습니다.\n\n\n\n4.3. 주기적 패턴 확인 (요일별 Box Plot)\n산업 데이터는 요일이나 월별로 뚜렷한 주기성을 가질 수 있습니다. Box plot을 사용하면 이러한 주기성 내에서 평소와 다른 패턴을 보이는 시점을 파악하는 데 유용합니다.\n\n# 분석을 위해 'day_of_week' 컬럼 추가 (월요일=0, 일요일=6)\ndata['day_of_week'] = data.index.dayofweek\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='day_of_week', y='power_usage', data=data, palette='coolwarm')\nplt.title('요일별 전력 사용량 분포', fontsize=16)\nplt.xlabel('요일 (0:월, 1:화, 2:수, 3:목, 4:금, 5:토, 6:일)', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.xticks(ticks=range(7), labels=['월', '화', '수', '목', '금', '토', '일'])\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: 요일별 전력 사용량 분포\n\n\n\n\n\n요일별 Box plot은 각 요일의 전력 사용량 분포를 보여줍니다. 각 박스는 해당 요일 데이터의 중앙 50%(IQR: Interquartile Range)를 나타내며, 박스 외부의 점들은 잠재적인 이상치(outliers)를 의미합니다. 이 샘플에서는 주말(토, 일) 사용량이 평일보다 낮은 패턴이 뚜렷하며, 우리가 인위적으로 삽입한 이상치들이 특정 요일의 일반적인 범위를 벗어나 점으로 표시될 수 있습니다. 예를 들어, 화요일(1)에 발생시킨 스파이크는 화요일의 박스 플롯에서 상단 이상치로 나타날 가능성이 큽니다.\n\n\n\n\n5. 이동 평균을 활용한 추세 및 변동성 관찰\n이동 평균(Moving Average)은 단기적인 변동을 완화하여 장기적인 추세를 파악하거나, 데이터의 일반적인 수준을 나타내는 기준으로 활용될 수 있습니다. 원본 데이터와 이동 평균선을 함께 시각화하면, 이동 평균에서 크게 벗어나는 지점들을 이상치 후보로 간주할 수 있습니다.\n\n# 7일 이동 평균 계산\ndata['rolling_mean_7'] = data['power_usage'].rolling(window=7, center=True).mean() # center=True로 설정하여 lag 감소 효과\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='lightskyblue', alpha=0.8, linewidth=1)\nplt.plot(data.index, data['rolling_mean_7'], label='7일 이동 평균 (중앙 정렬)', color='orangered', linewidth=2)\nplt.title('일별 전력 사용량 및 7일 이동 평균', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 4: 전력 사용량과 7일 이동 평균\n\n\n\n\n\n\n# 이동 평균과의 차이(잔차와 유사한 개념)를 통해 이상치 강조\ndata['deviation_from_ma'] = data['power_usage'] - data['rolling_mean_7']\n\nplt.figure(figsize=(9,5))\nplt.plot(data.index, data['deviation_from_ma'], label='이동 평균과의 편차', color='teal', linewidth=1, marker='o', markersize=3, linestyle='None')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8) # 기준선\n\n# 편차의 임계값을 설정하여 이상치 후보 시각화 (예: 편차의 표준편차 기반)\n# 이동 평균 계산 시 초반/후반 NaN 값이 있을 수 있으므로 dropna() 사용\ndeviation_std = data['deviation_from_ma'].dropna().std()\nupper_threshold = 3 * deviation_std\nlower_threshold = -3 * deviation_std\n\nplt.axhline(upper_threshold, color='red', linestyle=':', linewidth=1.5, label=f'+3σ ({upper_threshold:.2f})')\nplt.axhline(lower_threshold, color='red', linestyle=':', linewidth=1.5, label=f'-3σ ({lower_threshold:.2f})')\nplt.title('이동 평균과의 편차 (이상치 탐색 보조)', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('편차 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 5: 이동 평균과의 편차 (이상치 탐색 보조)\n\n\n\n\n\n7일 이동 평균선은 데이터의 단기적 변동을 평탄화하여 보여줍니다. rolling() 함수에서 center=True 옵션을 사용하면 이동 평균 계산 시 윈도우의 중앙에 값을 위치시켜 시각화 시 원본 데이터와의 지연(lag)을 줄이는 데 도움이 됩니다.\n두 번째 그래프는 원본 데이터와 이동 평균과의 편차를 보여줍니다. 이 편차가 특정 임계값(예: 편차의 3 표준편차, ±3σ)을 넘어서는 지점들은 잠재적인 이상치로 간주할 수 있습니다. 우리가 삽입한 인위적인 스파이크와 급감 지점에서 편차가 크게 나타나는 것을 확인할 수 있습니다. 이러한 방법은 간단하면서도 효과적인 이상치 탐색의 기초가 됩니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#요약",
    "href": "posts/ABC_week01_data analysis/index.html#요약",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "요약",
    "text": "요약\n이 포스트에서는 Python을 사용하여 가상의 산업 전력 사용량 데이터를 생성하고, 이상 탐지를 위한 기본적인 탐색적 데이터 분석(EDA) 및 전처리 과정을 살펴보았습니다. 시계열 플롯, 분포 확인, 주기성 분석(요일별 Box Plot), 이동 평균 활용 등은 데이터의 특성을 이해하고 잠재적인 이상치를 식별하는 데 효과적인 방법입니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html",
    "href": "posts/ABC_week04_model_optimization/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 네 번째 기술노트입니다. 지난주에는 PyTorch를 이용해 CNN 오토인코더 기반의 시계열 이상 탐지 베이스라인 모델을 구현했습니다. 이번 주에는 해당 모델의 한계를 명확히 분석하고, 이를 개선하기 위한 구체적인 방법론과 하이퍼파라미터 최적화 라이브러리 ’Optuna’를 활용한 실험 과정을 상세히 공유합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "href": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "1. 기존 모델의 한계 명확히 하기",
    "text": "1. 기존 모델의 한계 명확히 하기\n모든 모델링의 시작은 현재 모델을 정확히 아는 것입니다. Week3에서 구현한 베이스라인 모델은 가능성을 보여주었지만, 몇 가지 명확한 한계점을 가지고 있었습니다.\n\n1.1. 탐지 성능의 아쉬움: 놓치거나, 잘못 잡거나\n지난주 결과 그래프를 다시 살펴보면, 실제 이상치(Ground Truth) 3개 중 일부를 탐지하지 못하거나(False Negative), 반대로 정상 구간을 이상치로 판단하는(False Positive) 경향을 보였습니다.\n\n탐지 누락 (False Negative): 80번 인덱스 주변의 실제 이상치는 재구성 오차가 임계값을 넘지 않아 탐지되지 않았습니다. 이는 모델이 해당 유형의 이상 패턴(상대적으로 변화의 폭이 작은 이상치)을 정상 데이터의 일부로 학습했음을 의미합니다. 모델이 너무 ’관대’하게 데이터를 복원하고 있는 것입니다.\n오탐 (False Positive): 시계열 데이터의 시작 부분(0~10 인덱스)에서 재구성 오차가 높게 나타났습니다. 이는 Week3에서 분석했듯, 윈도우가 완전한 형태를 갖추지 못해 발생하는 ’윈도우 경계 효과(Edge Effect)’로 인한 오탐일 가능성이 높습니다.\n\n\n\n\n지난주 탐지 결과 그래프\n\n\n\n그림 1. Week3 모델의 이상 탐지 결과. 일부 이상치를 놓치고, 경계면에서 오탐이 발생했다.\n\n\n\n1.2. 과적합(Overfitting) 가능성\n오토인코더는 정상 데이터의 핵심 패턴을 학습해야 하지만, 너무 학습 데이터에만 치중하면 ’과적합’되어 미세한 노이즈까지 모두 정상으로 간주하게 됩니다. 이 경우, 새로운 형태의 이상치가 들어왔을 때 재구성 오차를 효과적으로 만들어내지 못해 탐지 성능이 저하됩니다. 현재 모델은 Dropout이나 규제(Regularization) 같은 과적합 방지 장치가 없어 이러한 위험에 노출되어 있습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "href": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "2. 성능 개선을 위한 접근 전략",
    "text": "2. 성능 개선을 위한 접근 전략\n위에서 정의한 문제들을 해결하기 위해 다음과 같은 세 가지 전략을 시도했습니다.\n\n2.1. 데이터 전처리 방식 변경: StandardScaler 도입\n기존 모델은 별도의 스케일링 없이 마지막 레이어의 Sigmoid 활성화 함수를 통해 출력을 0과 1 사이로 맞췄습니다. 이는 데이터의 분포가 0과 1 사이에 고르게 분포하지 않을 경우 정보 손실을 야기할 수 있고, 이상치의 특성을 약화시킬 수 있습니다.\n데이터를 평균 0, 표준편차 1을 갖도록 정규화하는 StandardScaler를 적용하여 모델이 데이터의 분포 특성을 더 잘 학습하고, 이상치와 정상 데이터 간의 차이를 더 명확하게 인지하도록 유도했습니다.\n\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 예시 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# StandardScaler 적용\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n\nprint(f\"원본 데이터 평균/표준편차: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"스케일링 후 평균/표준편차: {np.mean(scaled_data):.2f} / {np.std(scaled_data):.2f}\")\n\n원본 데이터 평균/표준편차: 0.03 / 0.84\n스케일링 후 평균/표준편차: 0.00 / 1.00\n\n\n\n\n2.2. 모델 구조 변경: 과적합 방지를 위한 Dropout 추가\n모델의 일반화 성능을 높이고 과적합을 방지하기 위해 Dropout 레이어를 추가했습니다. Dropout은 학습 과정에서 각 뉴런을 확률적으로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다. 주로 활성화 함수(ReLU) 뒤에 위치시켜 정보의 흐름을 조절합니다.\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        x = self.decoder_conv_final(x)\n        # StandardScaler를 사용하므로 마지막 Sigmoid 활성화 함수는 제거\n        return x\n\n# 모델 테스트\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n\n\n\n\n2.3. 하이퍼파라미터 최적화: Optuna 활용\n모델 성능에 영향을 미치는 하이퍼파라미터(학습률, 드롭아웃 비율, 필터 수 등)를 체계적으로 찾기 위해 Optuna 라이브러리를 사용합니다. Optuna는 베이지안 최적화 기법을 기반으로 효율적인 탐색을 수행합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "3. Optuna를 이용한 최적화 실험 (수정)",
    "text": "3. Optuna를 이용한 최적화 실험 (수정)\n\n3.1. objective 함수 정의 (수정)\n가장 큰 변경점은 정상 데이터만으로 모델을 학습하고 검증하는 것입니다. 아래 코드에서는 실제 이상치 인덱스(outliers)가 포함되지 않은 ’정상 윈도우’만 필터링하여 학습 및 검증에 사용합니다.\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 데이터 준비 (정상/이상 분리) ---\nwindow_size = 10\n\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# data-scaling 셀의 scaled_data(이상치 포함)로 전체 윈도우 생성\nall_windows = sliding_window(scaled_data, window_size)\nall_windows_torch = torch.from_numpy(all_windows[..., np.newaxis].transpose(0, 2, 1)).float()\n\n# 학습에 사용할 정상 윈도우만 필터링\noutliers = [20, 50, 80]\nnormal_window_indices = []\nfor i in range(len(all_windows)):\n    window_range = range(i, i + window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\n# 정상 윈도우만으로 학습 데이터셋 구성\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# 정상 데이터셋을 학습용과 검증용으로 분리\ntrain_size = int(0.8 * len(normal_dataset))\nval_size = len(normal_dataset) - train_size\ntrain_dataset, val_dataset = random_split(normal_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# --- Objective 함수 정의 ---\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n\n    model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # 정상 데이터로만 학습\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for data in train_loader:\n            inputs = data[0]\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            loss.backward()\n            optimizer.step()\n\n    # 정상 데이터로만 검증\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs = data[0]\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader)\n\n# --- Optuna Study 실행 ---\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint(\"Best trial:\", study.best_trial.params)\n\n[I 2025-06-19 14:22:02,606] A new study created in memory with name: no-name-84ad3100-838c-474c-9791-e027d32a7067\n[I 2025-06-19 14:22:04,084] Trial 0 finished with value: 0.22401034832000732 and parameters: {'lr': 0.00012557853930539835, 'dropout_rate': 0.2652604866315663, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.22401034832000732.\n[I 2025-06-19 14:22:04,478] Trial 1 finished with value: 0.09066235274076462 and parameters: {'lr': 0.0015713537109304843, 'dropout_rate': 0.2335156567324039, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.09066235274076462.\n[I 2025-06-19 14:22:04,875] Trial 2 finished with value: 0.7524880766868591 and parameters: {'lr': 2.8026593502100282e-05, 'dropout_rate': 0.36543423770275474, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.09066235274076462.\n[I 2025-06-19 14:22:05,294] Trial 3 finished with value: 0.10812652111053467 and parameters: {'lr': 0.008313167299816333, 'dropout_rate': 0.37714139188699025, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.09066235274076462.\n[I 2025-06-19 14:22:05,694] Trial 4 finished with value: 0.7878335118293762 and parameters: {'lr': 1.1267459063695082e-05, 'dropout_rate': 0.40339470259725396, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.09066235274076462.\n[I 2025-06-19 14:22:06,091] Trial 5 finished with value: 0.7551539540290833 and parameters: {'lr': 3.4526605159712544e-05, 'dropout_rate': 0.3309668280630509, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.09066235274076462.\n[I 2025-06-19 14:22:06,511] Trial 6 finished with value: 0.06270325183868408 and parameters: {'lr': 0.009951231234556898, 'dropout_rate': 0.3462049752301646, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:06,933] Trial 7 finished with value: 0.1035865992307663 and parameters: {'lr': 0.006948439294055532, 'dropout_rate': 0.34610124969628964, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:07,329] Trial 8 finished with value: 0.15405158698558807 and parameters: {'lr': 0.008607526706601133, 'dropout_rate': 0.2906556195863057, 'optimizer': 'RMSprop'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:07,742] Trial 9 finished with value: 0.08376972377300262 and parameters: {'lr': 0.006034571642521467, 'dropout_rate': 0.2176296631785849, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:08,168] Trial 10 finished with value: 0.2679348886013031 and parameters: {'lr': 0.0007256818459816103, 'dropout_rate': 0.4961711147893547, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:08,586] Trial 11 finished with value: 0.06666197627782822 and parameters: {'lr': 0.0016815488307221525, 'dropout_rate': 0.13987412463901305, 'optimizer': 'Adam'}. Best is trial 6 with value: 0.06270325183868408.\n[I 2025-06-19 14:22:09,003] Trial 12 finished with value: 0.05026326701045036 and parameters: {'lr': 0.0019210536563522298, 'dropout_rate': 0.1018332798461999, 'optimizer': 'Adam'}. Best is trial 12 with value: 0.05026326701045036.\n[I 2025-06-19 14:22:09,417] Trial 13 finished with value: 0.043296586722135544 and parameters: {'lr': 0.002163476283033001, 'dropout_rate': 0.11791292316733978, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.043296586722135544.\n[I 2025-06-19 14:22:09,831] Trial 14 finished with value: 0.080149345099926 and parameters: {'lr': 0.0003285639853606509, 'dropout_rate': 0.10585132090654581, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.043296586722135544.\n[I 2025-06-19 14:22:10,250] Trial 15 finished with value: 0.0599081851541996 and parameters: {'lr': 0.002022609695389369, 'dropout_rate': 0.16941342145208646, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.043296586722135544.\n[I 2025-06-19 14:22:10,668] Trial 16 finished with value: 0.10552643239498138 and parameters: {'lr': 0.0005599819773007766, 'dropout_rate': 0.1739496940972589, 'optimizer': 'Adam'}. Best is trial 13 with value: 0.043296586722135544.\n[I 2025-06-19 14:22:11,082] Trial 17 finished with value: 0.04299716651439667 and parameters: {'lr': 0.0035576929322481176, 'dropout_rate': 0.10355324720392145, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.04299716651439667.\n[I 2025-06-19 14:22:11,498] Trial 18 finished with value: 0.0726260393857956 and parameters: {'lr': 0.003598391642675231, 'dropout_rate': 0.1943756686401611, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.04299716651439667.\n[I 2025-06-19 14:22:11,911] Trial 19 finished with value: 0.3195308446884155 and parameters: {'lr': 0.00017675389436408752, 'dropout_rate': 0.14317128064099835, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.04299716651439667.\n[I 2025-06-19 14:22:12,330] Trial 20 finished with value: 0.12942011654376984 and parameters: {'lr': 0.0007352017169626118, 'dropout_rate': 0.2478321740310732, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.04299716651439667.\n[I 2025-06-19 14:22:12,746] Trial 21 finished with value: 0.03126920387148857 and parameters: {'lr': 0.0030207175233280424, 'dropout_rate': 0.1118649926150015, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:13,163] Trial 22 finished with value: 0.068967305123806 and parameters: {'lr': 0.0032486387482756982, 'dropout_rate': 0.13378648126345566, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:13,575] Trial 23 finished with value: 0.04798810929059982 and parameters: {'lr': 0.003755823228308781, 'dropout_rate': 0.10610935536954408, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:13,986] Trial 24 finished with value: 0.0809517651796341 and parameters: {'lr': 0.0011333734430049969, 'dropout_rate': 0.19430509438932747, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:14,404] Trial 25 finished with value: 0.04943772032856941 and parameters: {'lr': 0.0036284272820284826, 'dropout_rate': 0.15679827745688657, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:14,817] Trial 26 finished with value: 0.129292830824852 and parameters: {'lr': 0.0004382019307847098, 'dropout_rate': 0.20341676121482138, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:15,230] Trial 27 finished with value: 0.06301166117191315 and parameters: {'lr': 0.0008824795414913412, 'dropout_rate': 0.12434274912876894, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:15,657] Trial 28 finished with value: 0.5758655667304993 and parameters: {'lr': 0.00018952510665520316, 'dropout_rate': 0.4441686591116678, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:16,058] Trial 29 finished with value: 0.11239314824342728 and parameters: {'lr': 0.005104126501579748, 'dropout_rate': 0.29734355388962813, 'optimizer': 'RMSprop'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:16,470] Trial 30 finished with value: 0.0735754445195198 and parameters: {'lr': 0.00247251389094086, 'dropout_rate': 0.17438644122374908, 'optimizer': 'Adam'}. Best is trial 21 with value: 0.03126920387148857.\n[I 2025-06-19 14:22:16,884] Trial 31 finished with value: 0.028971437364816666 and parameters: {'lr': 0.0039087434088371864, 'dropout_rate': 0.10865105140607251, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:17,300] Trial 32 finished with value: 0.05338962376117706 and parameters: {'lr': 0.0013220867062565768, 'dropout_rate': 0.12384683680706209, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:17,713] Trial 33 finished with value: 0.04324572533369064 and parameters: {'lr': 0.004563050185715014, 'dropout_rate': 0.15222921697863911, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:18,107] Trial 34 finished with value: 0.0544058121740818 and parameters: {'lr': 0.00511067264234623, 'dropout_rate': 0.15419511880499862, 'optimizer': 'RMSprop'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:18,525] Trial 35 finished with value: 0.0834207683801651 and parameters: {'lr': 0.0028549137865903564, 'dropout_rate': 0.26195486664756895, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:18,923] Trial 36 finished with value: 0.5418447256088257 and parameters: {'lr': 7.919640110847299e-05, 'dropout_rate': 0.2262395317669446, 'optimizer': 'RMSprop'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:19,342] Trial 37 finished with value: 0.0699092373251915 and parameters: {'lr': 0.001155902756189356, 'dropout_rate': 0.15551778231567334, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:19,756] Trial 38 finished with value: 0.034857913851737976 and parameters: {'lr': 0.005033651097271871, 'dropout_rate': 0.10268786637646189, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:20,149] Trial 39 finished with value: 0.07647036015987396 and parameters: {'lr': 0.00947431691837837, 'dropout_rate': 0.10317237006563705, 'optimizer': 'RMSprop'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:20,565] Trial 40 finished with value: 0.038811128586530685 and parameters: {'lr': 0.005982600630872737, 'dropout_rate': 0.12768245060002875, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:20,977] Trial 41 finished with value: 0.04254757985472679 and parameters: {'lr': 0.006575919438077675, 'dropout_rate': 0.12684578199725755, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:21,389] Trial 42 finished with value: 0.040295034646987915 and parameters: {'lr': 0.0065720685669415895, 'dropout_rate': 0.12911433446649137, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:21,802] Trial 43 finished with value: 0.04573948308825493 and parameters: {'lr': 0.0061968601622023065, 'dropout_rate': 0.17871808551586665, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:22,215] Trial 44 finished with value: 0.03517727181315422 and parameters: {'lr': 0.00770977694856558, 'dropout_rate': 0.1327983760131477, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:22,627] Trial 45 finished with value: 0.06075937673449516 and parameters: {'lr': 0.009902097704256228, 'dropout_rate': 0.14188802487897498, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:23,042] Trial 46 finished with value: 0.08123942464590073 and parameters: {'lr': 0.0025190690267874664, 'dropout_rate': 0.21235092689817736, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:23,445] Trial 47 finished with value: 0.7718703746795654 and parameters: {'lr': 2.8352862947905485e-05, 'dropout_rate': 0.4006485896475348, 'optimizer': 'RMSprop'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:23,855] Trial 48 finished with value: 0.04531075432896614 and parameters: {'lr': 0.00475768439741137, 'dropout_rate': 0.11790176623838457, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n[I 2025-06-19 14:22:24,268] Trial 49 finished with value: 0.04515334218740463 and parameters: {'lr': 0.007580373727857704, 'dropout_rate': 0.18193727902672108, 'optimizer': 'Adam'}. Best is trial 31 with value: 0.028971437364816666.\n\n\nBest trial: {'lr': 0.0039087434088371864, 'dropout_rate': 0.10865105140607251, 'optimizer': 'Adam'}\n\n\n\n\n3.2. 최적화 결과 분석 및 시각화\n(이 부분은 기존과 동일합니다. Optuna 실행 후 주석을 해제하여 사용하세요.)\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=800, height=500)\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=800, height=400)\nfig2.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "4. 최종 모델 성능 비교 및 결론 (수정)",
    "text": "4. 최종 모델 성능 비교 및 결론 (수정)\n\n4.1. 최적 파라미터로 모델 재학습 및 평가 (수정)\nOptuna가 찾은 최적 파라미터로 정상 데이터 전체를 사용해 최종 모델을 학습합니다. 그 후, 정상 데이터의 재구성 오차를 기준으로 임계값을 설정하고, 이 기준을 전체 데이터에 적용하여 이상치를 탐지합니다.\n\n# --- 1. 최적 파라미터로 최종 모델 정의 ---\nbest_params = study.best_params \n\nfinal_model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=best_params['dropout_rate'])\noptimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['lr'])\ncriterion = nn.MSELoss()\n\n# --- 2. \"정상 데이터 전체\"로 모델 학습 ---\n# optuna-objective-fixed 셀에서 생성된 normal_dataset 사용\nfull_normal_loader = DataLoader(normal_dataset, batch_size=16, shuffle=True)\nepochs = 100\nprint(\"최종 모델 학습 시작...\")\nfor epoch in range(epochs):\n    for data in full_normal_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. \"정상 데이터의 재구성 오차\"로 임계값 설정 ---\nfinal_model.eval()\ntrain_reconstruction_error = []\nwith torch.no_grad():\n    # normal_windows_torch는 optuna-objective-fixed 셀에서 생성\n    reconstructed = final_model(normal_windows_torch)\n    error = torch.mean((normal_windows_torch - reconstructed)**2, dim=(1, 2))\n    train_reconstruction_error = error.numpy()\n\n# # 표준편차 기반 임계값 대신, Quantile(백분위수)을 사용하여 더 강건한 임계값 설정\n# # 정상 데이터의 재구성 오차 중 상위 0.3%에 해당하는 값을 임계값으로 사용 (오탐에 더 강해짐)\n# quantile_level = 0.997\n# threshold = np.quantile(train_reconstruction_error, quantile_level)\nthreshold=0.25 # MSE 0.25 넘으면 이상 윈도우 탐지\n# print(f\"\\n임계값 (Threshold) 설정 완료 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# --- 4. \"전체 데이터\"에 대한 재구성 오차 계산 및 이상치 탐지 (수정) ---\nfinal_model.eval()\nwith torch.no_grad():\n    # all_windows_torch의 shape: (num_windows, 1, window_size)\n    reconstructed = final_model(all_windows_torch)\n    \n    # 윈도우별 평균 오차 계산 (임계값 비교용)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed)**2, dim=(1, 2)).numpy()\n    \n    # 개별 데이터 포인트별 오차 계산 (가장 큰 오차 지점 탐색용)\n    pointwise_error = (all_windows_torch - reconstructed)**2\n    pointwise_error = pointwise_error.squeeze().numpy() # Shape: (num_windows, window_size)\n\n# 1. 임계값을 초과하는 \"이상치 윈도우\" 식별\nanomaly_window_indices = np.where(mean_error_per_window &gt; threshold)[0]\nprint(f\"탐지된 이상치 윈도우 인덱스: {anomaly_window_indices}\")\n\n# 2. 각 이상치 윈도우 내에서 오차가 가장 큰 \"단일 데이터 포인트\"의 인덱스 찾기\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    # 현재 윈도우의 개별 포인트 오차\n    errors_in_window = pointwise_error[window_idx]\n    # 오차가 가장 큰 포인트의 \"윈도우 내 상대적 인덱스\"\n    max_error_idx_in_window = np.argmax(errors_in_window)\n    # \"전체 데이터에서의 절대적 인덱스\" 계산\n    absolute_idx = window_idx + max_error_idx_in_window\n    predicted_anomaly_points.append(absolute_idx)\n\n# 중복 제거 및 정렬\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 인덱스: {predicted_anomaly_points}\")\n\n# 비교를 위해 실제 이상치가 포함된 윈도우 인덱스 출력\nanomaly_window_indices_ground_truth = sorted(list(set(range(len(all_windows))) - set(normal_window_indices)))\nprint(f\"실제 이상치 포함 윈도우 인덱스: {anomaly_window_indices_ground_truth}\")\n\n# all_reconstruction_error 변수는 하단 그래프에서 사용하므로 윈도우별 평균 오차를 할당\nall_reconstruction_error = mean_error_per_window\n\n최종 모델 학습 시작...\nEpoch [20/100], Loss: 0.059661\nEpoch [40/100], Loss: 0.034297\nEpoch [60/100], Loss: 0.026056\nEpoch [80/100], Loss: 0.018484\nEpoch [100/100], Loss: 0.017025\n탐지된 이상치 윈도우 인덱스: [11 12 13 14 15 16 17 18 19 20 41 42 43 44 45 46 47 48 49 50 71 72 73 74\n 75 76 77 78 79 80]\n탐지된 이상치 포인트 인덱스: [np.int64(20), np.int64(50), np.int64(80)]\n실제 이상치 포함 윈도우 인덱스: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n\n\n\n\n4.2. 베이스라인 모델 vs 개선 모델\n\n\n\n\n\n\n\n\n\n\n구분\n데이터 전처리\n과적합 방지\n하이퍼파라미터\n탐지된 이상치 (인덱스)\n\n\n\n\nWeek3 (베이스라인)\nSigmoid 활성화\n없음\n수동 설정\n[17 18 19 20 47 48 49 50] (윈도우)\n\n\nWeek4 (개선 모델)\nStandardScaler\nDropout\nOptuna 최적화\n[20, 50, 80] (단일 포인트)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8)) \n\n# --- 상단: 전체 데이터와 탐지 결과 ---\nplt.subplot(2, 1, 1)\nplt.plot(scaled_data, label='정규화된 데이터', alpha=0.8)\nplt.scatter(outliers, scaled_data[outliers], color='red', s=120, label='실제 이상치', marker='o', edgecolors='black', zorder=5)\n\n# 탐지된 단일 이상치 포인트를 표시\nif len(predicted_anomaly_points) &gt; 0:\n    # Check if indices are within bounds of scaled_data\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(scaled_data)]\n    plt.scatter(valid_indices, scaled_data[valid_indices],\n                color='orange', marker='x', s=120, linewidth=2, label='탐지된 이상치', zorder=5)\n\nplt.title('개선된 모델(Week4) 최종 이상 탐지 결과', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('정규화된 값')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# --- 하단: 재구성 오차와 임계값 ---\nplt.subplot(2, 1, 2)\nplt.plot(all_reconstruction_error, label='윈도우별 재구성 오차', color='blue')\nplt.axhline(y=threshold, color='r', linestyle='--', label=f'임계값 ({threshold:.4f})')\n\n# 임계값을 넘은 \"윈도우\"를 표시\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, all_reconstruction_error[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\n\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론",
    "href": "posts/ABC_week04_model_optimization/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론",
    "text": "5. 결론\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 개선하기 위한 여정을 상세히 다루었습니다. StandardScaler를 이용해 데이터 전처리를 표준화하고, Dropout을 추가하여 과적합을 방지했습니다. 또한, Optuna 라이브러리를 활용하여 학습률, 드롭아웃 비율 등 최적의 하이퍼파라미터를 체계적으로 탐색했습니다.\n특히, 탐지 정확도를 높이기 위해 정상 데이터만으로 모델을 학습하고, 재구성 오차의 Quantile(백분위수)을 이용해 통계적으로 강건한 임계값을 설정했습니다. 최종적으로는 임계값을 넘는 윈도우 내에서 오차가 가장 큰 단일 지점을 이상치로 특정하는 방식으로, 실제 이상치인 20, 50, 80번 인덱스를 성공적으로 탐지할 수 있었습니다.\n물론, 학습 과정의 미세한 무작위성으로 인해 간혹 오탐(False Positive)이 발생하는 한계도 확인했습니다. 이는 향후 모델 구조 자체를 개선하거나 더 다양한 데이터를 활용하여 안정성을 높여야 할 과제로 남았습니다. 다음 포스트에서는 또 다른 접근법으로 이 문제를 해결해 나가겠습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n이전 주차에서 개발한 CNN 오토인코더 모델을 실제 Kaggle의 주택 전력 사용량 데이터에 적용하여, 현실 데이터에서 발생하는 이상 패턴을 탐지하는 과정을 다룹니다.\n\n\n\n\n\nJun 19, 2025\n\n\nBeomdo Park\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n지난주 CNN 오토인코더 모델의 한계를 분석하고, 성능 개선을 위한 다양한 방법과 하이퍼파라미터 최적화 과정을 기록합니다.\n\n\n\n\n\nJun 14, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\nPyTorch를 사용하여 1D CNN 오토인코더 기반 시계열 이상 탐지 베이스라인 모델을 구현합니다.\n\n\n\n\n\nJun 8, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지 - 머신러닝 기법 적용 실습\n\n\n\n\n\nJun 1, 2025\n\n\nBeomdo Park\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지를 위한 기본 EDA 및 전처리 방법을 다룹니다.\n\n\n\n\n\nMay 25, 2025\n\n\nBeomdo Park\n\n8 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "컴퓨터공학과 3학년입니다.\n\nData Scientist AI-powered problem solver who applies research and technology to real-world challenges."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html",
    "href": "posts/ABC_week03_cnn_baseline/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 세 번째 기술노트입니다. 이번 주는 시계열 데이터의 ’패턴’을 학습할 수 있는 딥러닝, 그중에서도 CNN을 활용한 이상 탐지의 첫걸음을 PyTorch로 구현해 보겠습니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "href": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)",
    "text": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)\n시계열 데이터를 CNN 모델에 입력하려면 연속된 데이터를 일정한 길이의 조각(window)으로 나누는 ‘슬라이딩 윈도우’ 기법이 필요합니다. 이 방법은 데이터의 시간적 패턴을 학습하는 데 유용합니다.\n\n슬라이딩 윈도우 구현\n아래는 numpy를 사용해 슬라이딩 윈도우를 구현하는 간단한 Python 함수입니다:\n\nimport numpy as np\n\ndef sliding_window(data, window_size, step_size=1):\n    \"\"\"시계열 데이터를 슬라이딩 윈도우로 변환\"\"\"\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 예제 데이터\ndata = np.sin(np.linspace(0, 20, 100))\nwindowed_data = sliding_window(data, window_size=10)\nprint(\"윈도우 형태:\", windowed_data.shape)\n\n윈도우 형태: (91, 10)"
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "href": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)",
    "text": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)\n\n오토인코더란?\n오토인코더는 데이터를 압축(인코더)했다가 다시 복원(디코더)하도록 학습하는 딥러닝 모델입니다. 정상 데이터는 잘 복원되지만, 이상 데이터는 복원이 잘 되지 않아 재구성 오차가 커지는 특징을 활용합니다.\n\n\n모델 구조\n\n인코더 (Encoder): Conv1D와 MaxPooling1D 층을 사용해 입력 데이터의 특징을 추출하고 압축합니다.\n디코더 (Decoder): ConvTranspose1D (또는 Upsample + Conv1D) 층을 사용해 데이터를 복원합니다.\n\n\n\nPyTorch 구현\n아래는 PyTorch를 사용한 간단한 1D CNN 오토인코더 모델 구현입니다:\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoder(nn.Module):\n    def __init__(self, input_shape): # input_shape: (sequence_length, num_features)\n        super(CNNAutoencoder, self).__init__()\n        # Encoder\n        # input_shape[1]은 특성 수 (in_channels로 사용)\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/2로 감소\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/4로 감소\n\n        # Decoder\n        # 인코더에서 시퀀스 길이가 1/4로 줄었으므로, 디코더에서 원래 길이로 복원\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1) # 원본 특성 수로 복원\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_conv_final(x)\n        decoded = self.sigmoid(x)\n        return decoded\n\n# 모델 생성 및 컴파일은 data-generation 셀 이후로 이동합니다.\n# input_shape도 window_size를 사용하도록 수정됩니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "href": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "3. 모델 학습 및 이상치 탐지",
    "text": "3. 모델 학습 및 이상치 탐지\n\n데이터 생성\nWeek2에서 사용한 샘플 데이터를 기반으로 정상/비정상 데이터를 생성합니다:\n\n# numpy는 sliding_window_implementation 셀에서 이미 import 됨\n\n# 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# 슬라이딩 윈도우 적용\nwindow_size = 10\nwindows = sliding_window(data, window_size) # (N, L) -&gt; (N, window_size)\nwindows = windows[..., np.newaxis]  # (N, L, C) -&gt; (N, window_size, 1)\n# PyTorch Conv1d는 (N, C, L) 입력을 기대하므로 차원 변경\nwindows = windows.transpose(0, 2, 1) # (N, C, L) -&gt; (N, 1, window_size)\nprint(f\"윈도우 데이터 형태 (N, C, L): {windows.shape}\")\n\n윈도우 데이터 형태 (N, C, L): (91, 1, 10)\n\n\n\nimport torch.optim as optim # PyTorch 옵티마이저\n\n# 모델 생성\n# input_shape은 (window_size, 1) 이어야 합니다. (sequence_length, num_features)\n# data-generation 셀에서 windows는 (N, 1, window_size) 형태로 준비됨.\n# CNNAutoencoder의 __init__은 input_shape=(window_size, 1)을 받아 input_shape[1]=1을 in_channels로 사용.\nmodel_input_shape = (window_size, 1) # (sequence_length, num_features)\nmodel = CNNAutoencoder(model_input_shape) # cnn-autoencoder-definition 셀에서 정의된 클래스 사용\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss() # 평균 제곱 오차 손실\n\nprint(\"PyTorch 모델 구조:\")\nprint(model)\n\nPyTorch 모델 구조:\nCNNAutoencoder(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n  (sigmoid): Sigmoid()\n)\n\n\n\n\n모델 학습\n정상 데이터만 사용해 모델을 학습합니다:\n\n# torch는 cnn-autoencoder-definition 셀에서 이미 import 됨\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 정상 데이터로 학습\n# 'outliers'는 원본 'data' 배열의 인덱스입니다.\n# 'windows' 배열에서 이상치가 포함된 윈도우를 식별하여 제외합니다.\ncontaminated_window_indices = set()\n# 'outliers', 'window_size', 'windows' 변수는 이전 셀들에서 정의되어 있어야 합니다.\nfor outlier_data_idx in outliers: \n    start_contaminated_win_idx = max(0, outlier_data_idx - window_size + 1)\n    end_contaminated_win_idx = outlier_data_idx \n    \n    for win_idx in range(start_contaminated_win_idx, end_contaminated_win_idx + 1):\n        if win_idx &lt; len(windows): # 윈도우 인덱스가 유효한 범위 내에 있는지 확인\n            contaminated_window_indices.add(win_idx)\n\nnormal_windows_mask = np.ones(len(windows), dtype=bool)\nif contaminated_window_indices: # set이 비어있지 않은 경우에만 인덱싱\n    normal_windows_mask[list(contaminated_window_indices)] = False\n\nnormal_windows_np = windows[normal_windows_mask]\n\nif len(normal_windows_np) == 0:\n    print(\"경고: 학습에 사용할 정상 윈도우가 없습니다. Outlier 정의, window_size 또는 데이터 길이를 확인하세요.\")\nelse:\n    # PyTorch 데이터셋 및 로더 준비\n    normal_windows_torch = torch.tensor(normal_windows_np, dtype=torch.float32)\n    train_dataset = TensorDataset(normal_windows_torch) # 오토인코더는 입력과 타겟이 동일\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n    # 모델 학습\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device)\n    \n    epochs = 50 # 에포크 수 설정\n    print_every_epochs = 10\n\n    model.train() # 학습 모드\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_data_list in train_loader:\n            inputs = batch_data_list[0].to(device)\n            targets = inputs # 오토인코더의 타겟은 입력과 동일\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item() * inputs.size(0) # 배치 손실 누적 (loss.item()은 평균 손실)\n        \n        epoch_loss /= len(train_loader.dataset) # 에포크 평균 손실\n        if (epoch + 1) % print_every_epochs == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n    print(\"모델 학습 완료.\")\n\nUsing device: cuda\nEpoch [10/50], Loss: 0.474734\nEpoch [20/50], Loss: 0.271595\nEpoch [30/50], Loss: 0.231434\nEpoch [40/50], Loss: 0.217045\nEpoch [50/50], Loss: 0.210623\n모델 학습 완료.\n\n\n\n\n재구성 오차 계산 및 이상치 탐지\n학습된 모델로 데이터를 복원하고, 재구성 오차를 계산합니다:\n\n# torch 및 numpy는 이전 셀들에서 이미 import 됨\n\n# 재구성 오차 계산\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() # 평가 모드\n\n# 전체 windows 데이터를 PyTorch 텐서로 변환하고 device로 이동\nall_windows_torch = torch.tensor(windows, dtype=torch.float32).to(device)\n\n# 메모리 부족을 방지하기 위해 배치 단위로 처리할 수 있으나, 현재 데이터는 작으므로 한번에 처리\nwith torch.no_grad(): # 그래디언트 계산 비활성화\n    reconstructed_torch = model(all_windows_torch)\n\n# 결과를 CPU로 옮기고 NumPy 배열로 변환\nreconstructed_np = reconstructed_torch.cpu().numpy()\n\n# MAE (Mean Absolute Error) 계산\n# 원본 windows (numpy 배열)와 reconstructed_np 모두 (N, 1, window_size) 형태\n# axis=(1, 2)는 채널과 시퀀스 길이에 대한 평균을 의미\nmae = np.mean(np.abs(windows - reconstructed_np), axis=(1, 2))\nprint(f\"계산된 MAE 값 (처음 5개): {mae[:5]}\")\n\n# 이상치 탐지를 위한 임계값 설정 (데이터 및 모델 성능에 따라 조정 필요)\n# 예: MAE의 평균 + (표준편차 * 특정 배수) 또는 분위수 사용\nthreshold = np.mean(mae) + 1.5 * np.std(mae) # 표준편차 배수를 2에서 1.5로 줄여 민감도 증가\nprint(f\"이상치 탐지 임계값 (MAE): {threshold:.4f}\")\n\nanomalies_indices_in_windows = np.where(mae &gt; threshold)[0] # 윈도우 배열 내의 인덱스\n\nprint(f\"이상치로 탐지된 윈도우의 수: {len(anomalies_indices_in_windows)}\")\nprint(f\"이상치로 탐지된 윈도우 인덱스: {anomalies_indices_in_windows}\")\n\n# 윈도우 인덱스를 원본 데이터 인덱스로 대략적으로 매핑 (윈도우의 시작점 기준)\n# 실제 이상치 발생 시점과 정확히 일치하지 않을 수 있음\nanomalies_approx_original_indices = anomalies_indices_in_windows \n# 좀 더 정확하게는 윈도우의 중간 지점 등을 고려할 수 있으나, 여기서는 시작점으로 단순화\n# anomalies_approx_original_indices = [idx + window_size // 2 for idx in anomalies_indices_in_windows]\nprint(f\"원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): {anomalies_approx_original_indices}\")\n\n계산된 MAE 값 (처음 5개): [0.13974112 0.11385794 0.07825096 0.08110552 0.07896732]\n이상치 탐지 임계값 (MAE): 0.9588\n이상치로 탐지된 윈도우의 수: 8\n이상치로 탐지된 윈도우 인덱스: [17 18 19 20 47 48 49 50]\n원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): [17 18 19 20 47 48 49 50]\n\n\n\n\n결과 시각화\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(data, label='원본 데이터', alpha=0.7) # 'data'는 data-generation에서 정의됨\nplt.scatter(outliers, data[outliers], color='red', s=100, label='실제 이상치 (Ground Truth)', marker='o', edgecolors='black') # 'outliers'는 data-generation에서 정의됨\n\n# anomalies_approx_original_indices가 비어있을 수 있으므로 확인\nif len(anomalies_approx_original_indices) &gt; 0:\n    # 탐지된 이상치 표시는 윈도우의 시작점을 기준으로 함\n    plt.scatter(anomalies_approx_original_indices, data[anomalies_approx_original_indices], \n                color='orange', marker='x', s=80, label='탐지된 이상치 (모델 예측)', alpha=0.8)\nelse:\n    print(\"탐지된 이상치가 없습니다.\")\n        \nplt.legend()\nplt.title('PyTorch CNN 오토인코더 기반 시계열 이상 탐지')\nplt.xlabel('시간 스텝')\nplt.ylabel('값')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# 재구성 오차(MAE) 시각화\nplt.figure(figsize=(10, 3))\nplt.plot(mae, label='재구성 오차 (MAE)', color='green')\nplt.axhline(threshold, color='red', linestyle='--', label=f'임계값 ({threshold:.2f})')\nplt.title('윈도우별 재구성 오차 (MAE) 및 임계값')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('MAE')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\nPyTorch CNN 오토인코더 기반 이상 탐지 결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n탐지 결과 분석 및 고려사항\n시각화 결과와 재구성 오차 검토 시 다음 사항을 고려해야 한다.\n\n실제 이상치 vs. 탐지 이상치:\n\ndata-generation 단계에서 의도적으로 넣은 실제 이상치(outliers = [20, 50, 80])와 모델의 탐지 결과는 다를 수 있다.\n모든 실제 이상치가 탐지되지 않거나, 정상이 이상치로 잘못 탐지될 가능성이 항상 존재한다.\n현 예제는 임계값(np.mean(mae) + 1.5 * np.std(mae)) 조정을 통해 최소 하나의 이상치를 탐지하도록 유도했다. 실제 상황에서는 모델 성능, 데이터 특성, window_size, 임계값 설정에 따라 결과가 크게 달라진다.\n\n윈도우 경계 효과 (Edge Effects):\n\n시계열 데이터의 시작과 끝 부분 윈도우는 내부 윈도우에 비해 정보가 불완전할 수 있다 (이전/이후 데이터 부재).\nCNN 모델, 특히 패딩 사용 시, 경계 영역 윈도우는 학습된 주 정상 패턴과 달라 재구성 오차가 상대적으로 커질 수 있다.\n결과적으로, 시계열 양 끝부분에서 이상치가 아닌데도 이상치로 탐지되는 경향이 나타날 수 있다. MAE 그래프에서 초반 또는 후반부에 높은 오차가 관찰된다면 이 효과를 의심해볼 수 있다.\n\nwindow_size의 중요성:\n\nwindow_size는 모델이 학습할 패턴의 길이를 결정한다.\n너무 작으면 장기 패턴 파악이 어렵고, 너무 크면 짧은 순간의 이상치를 놓치거나 정상 변동에도 민감하게 반응할 수 있다.\n현재 window_size=10으로 설정했다. 데이터 특성에 맞춰 이 값을 조정하며 실험하는 과정이 중요하다.\n\n모델 및 임계값의 한계:\n\n여기서 사용한 CNN 오토인코더는 비교적 단순한 모델이다.\n더 복잡한 패턴이나 다양한 유형의 이상치를 탐지하려면 모델 구조 개선(예: LSTM, Transformer 기반 오토인코더)이나 다른 접근법을 고려해야 한다.\n고정 임계값 대신 동적 임계값을 사용하거나, 통계적 검정 기법을 결합하는 것도 탐지 성능을 높이는 데 도움이 될 수 있다.\n\n\n이런 점들을 고려해 모델 결과를 해석해야 하며, 실제 문제 적용 시에는 충분한 검증과 실험이 필수다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "href": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "결론 및 다음 단계",
    "text": "결론 및 다음 단계\n이번 주에는 PyTorch로 간단한 1D CNN 오토인코더를 만들고, 시계열 이상 탐지를 수행했다. 이 모델은 시계열 이상 탐지의 괜찮은 시작점이 될 수 있다. 재구성 오차를 기반으로 이상치를 찾는 과정과, 임계값 설정에 따라 탐지 결과가 어떻게 달라지는지 확인했다.\n다음 포스트에서는 실제 산업 데이터를 사용해 모델을 학습시키고, 성능을 개선할 다양한 방법(예: 더 복잡한 모델 구조, 다른 유형의 오토인코더, 동적 임계값 설정 등)을 살펴볼 예정이다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html",
    "href": "posts/ABC_week05_real_data_analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 다섯 번째 기술노트입니다. Week04에서 개발한 CNN 오토인코더 모델을 실제 Kaggle 공개 데이터셋(주택 전력 사용량 3년치)에 적용해, 실전 환경에서의 이상 탐지 성능과 한계를 점검합니다. 이 과정을 통해, 이론적 모델이 실제 데이터에서 어떻게 동작하는지, 그리고 실무에서 마주칠 수 있는 문제와 해결책을 탐구합니다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "1. 데이터 소개 및 준비",
    "text": "1. 데이터 소개 및 준비\n이번 주에 사용할 데이터는 Kaggle에 공개된 ‘Residential Power Usage 3-Years Data’입니다. 한 가정의 3년간 전력 사용량이 분 단위로 기록된 시계열 데이터로, 실제 환경에서 발생하는 다양한 패턴과 이상 현상을 포함하고 있습니다.\n먼저, GitHub Raw URL을 통해 데이터를 불러오고, 시계열 분석을 위해 날짜 컬럼을 인덱스로 변환한 뒤, 전력 사용량 컬럼만 추출합니다.\n\n\n데이터 로딩 시작...\n데이터 로딩 완료.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35952 entries, 0 to 35951\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   StartDate    35952 non-null  object \n 1   Value (kWh)  35952 non-null  float64\n 2   day_of_week  35952 non-null  int64  \n 3   notes        35952 non-null  object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n데이터는 ‘StartDate’, ‘Value (kWh)’, ‘day_of_week’, ‘notes’ 등의 컬럼으로 구성되어 있습니다. 시계열 분석을 위해 ‘StartDate’를 datetime 형식으로 변환하고 인덱스로 설정한 뒤, ’Value (kWh)’ 컬럼만 사용하겠습니다. 전체 데이터를 사용하여 분석을 진행합니다.\n\nprint(\"데이터 전처리 시작...\")\n# 'StartDate'를 datetime으로 변환하고 인덱스로 설정\ndf['StartDate'] = pd.to_datetime(df['StartDate'])\ndf = df.set_index('StartDate')\n\n# 시간순으로 정렬\ndf.sort_index(inplace=True)\nprint(\"데이터 시간순 정렬 완료.\")\n\n# 'Value (kWh)' 컬럼만 선택\n# df.info() 결과에 따라 'Value (kWh)'를 사용합니다.\nvalue_col = 'Value (kWh)'\ndf_value = df[[value_col]].copy()\n\n# 데이터 시각화 (전체 기간)\nplt.figure(figsize=(15, 6))\nplt.plot(df_value.index, df_value[value_col], label='전력 사용량 (전체 기간)')\nplt.title('시간에 따른 전력 사용량 (전체 기간)')\nplt.xlabel('날짜')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"전체 데이터 크기: {df_value.shape}\")\n\n데이터 전처리 시작...\n데이터 시간순 정렬 완료.\n\n\n\n\n\n\n\n\n\n전체 데이터 크기: (35952, 1)"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-윈도우-생성",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-윈도우-생성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "2. 데이터 전처리 및 윈도우 생성",
    "text": "2. 데이터 전처리 및 윈도우 생성\n시계열 데이터의 추세(Trend) 영향을 줄이고, 각 윈도우의 지역적 패턴에 집중하기 위해 윈도우별로 독립적인 정규화를 수행합니다. 이 방법은 데이터의 전반적인 스케일 변화에 덜 민감한 모델을 만드는 데 도움이 됩니다.\n\nprint(\"윈도우 생성 및 정규화 시작...\")\n# 원본 데이터에서 바로 윈도우 생성\nraw_data = df_value[value_col].values\n\n# 슬라이딩 윈도우 생성 함수\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 윈도우 생성 (윈도우 크기는 60분(1시간)으로 설정)\nwindow_size = 60\nraw_windows = sliding_window(raw_data, window_size)\n\n# 윈도우별 정규화\nscaled_windows = np.array([StandardScaler().fit_transform(window.reshape(-1, 1)).flatten() for window in raw_windows])\n\n# PyTorch 텐서 변환 (shape: [batch_size, channels, sequence_length])\nall_windows_torch = torch.from_numpy(scaled_windows).unsqueeze(1).float()  # [N, 1, window_size]\n\nprint(f\"생성된 윈도우 데이터 shape: {all_windows_torch.shape}\")\n\n윈도우 생성 및 정규화 시작...\n생성된 윈도우 데이터 shape: torch.Size([35893, 1, 60])"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#안정-구간을-이용한-모델-학습-전략",
    "href": "posts/ABC_week05_real_data_analysis/index.html#안정-구간을-이용한-모델-학습-전략",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "3. 안정 구간을 이용한 모델 학습 전략",
    "text": "3. 안정 구간을 이용한 모델 학습 전략\n실제 데이터에서는 초반부와 이후 구간의 패턴이 다를 수 있습니다(구간 변화, Regime Change). 전체 데이터를 학습하면 변화 자체를 이상으로 탐지할 수 있으므로, 패턴이 안정화된 구간만을 ’정상’으로 정의하고 해당 구간 데이터로만 모델을 학습합니다.\n\n# 학습 데이터와 전체 평가 데이터 분리\n# 패턴이 안정화된 구간을 학습 데이터로 사용 (전체 데이터의 1/3 지점부터)\ntrain_start_index = min(5000, len(all_windows_torch) // 3)\ntrain_windows_torch = all_windows_torch[train_start_index:]\n\nprint(f\"전체 윈도우 수: {all_windows_torch.shape[0]}\")\nprint(f\"학습 시작 인덱스: {train_start_index}\")\nprint(f\"학습에 사용할 윈도우 수: {train_windows_torch.shape[0]}\")\n\n전체 윈도우 수: 35893\n학습 시작 인덱스: 5000\n학습에 사용할 윈도우 수: 30893\n\n\n이제 CNNAutoencoderWithDropout 모델을 이 안정 구간 데이터로만 학습시킵니다.\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_channels=1, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=7, padding=3)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=7, padding=3)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=2, stride=2)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=2, stride=2)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_channels, kernel_size=7, padding=3)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n# 모델 정의 및 학습\nmodel = CNNAutoencoderWithDropout(input_channels=1, dropout_rate=0.2)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.MSELoss()\n\n# 안정 구간 데이터로만 학습\ndataset = TensorDataset(train_windows_torch)\ndata_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n\nprint(\"모델 학습 시작 (안정 구간 데이터)...\")\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    for data in data_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\nprint(\"모델 학습 완료.\")\n\n모델 학습 시작 (안정 구간 데이터)...\nEpoch [5/10], Loss: 0.132806\nEpoch [10/10], Loss: 0.122969\n모델 학습 완료."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#모델-평가-및-이상치-탐지",
    "href": "posts/ABC_week05_real_data_analysis/index.html#모델-평가-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "4. 모델 평가 및 이상치 탐지",
    "text": "4. 모델 평가 및 이상치 탐지\n학습된 모델로 전체 데이터의 재구성 오차를 계산하고, 학습 데이터의 재구성 오차 분포를 기반으로 임계값(Quantile 기준)을 설정해 이상치를 탐지합니다.\n\nmodel.eval()\n\n# 1. 학습 데이터의 재구성 오차를 기반으로 임계값 설정\nwith torch.no_grad():\n    reconstructed_train = model(train_windows_torch)\n    error_train = torch.mean((train_windows_torch - reconstructed_train)**2, dim=(1, 2))\n    train_reconstruction_error = error_train.numpy()\n\nquantile_level = 0.995\nthreshold = np.quantile(train_reconstruction_error, quantile_level)\nprint(f\"임계값 (학습 데이터 기준 {quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# 2. 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = model(all_windows_torch)\n    error_all = torch.mean((all_windows_torch - reconstructed_all)**2, dim=(1, 2))\n    reconstruction_error = error_all.numpy()\n\n# 3. 임계값을 기준으로 전체 데이터에서 이상치 탐지\nanomaly_window_indices = np.where(reconstruction_error &gt; threshold)[0]\n\n# 윈도우 내 최대 오차 지점 탐색\npointwise_error = ((all_windows_torch - reconstructed_all)**2).squeeze().numpy()\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    max_error_idx_in_window = np.argmax(pointwise_error[window_idx])\n    absolute_idx = window_idx + max_error_idx_in_window\n    predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n\n# 안전성 체크: 유효한 인덱스만 필터링\npredicted_anomaly_points = [idx for idx in predicted_anomaly_points if idx &lt; len(raw_data)]\nprint(f\"유효한 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n\n임계값 (학습 데이터 기준 99.5% Quantile): 0.295568\n탐지된 이상치 포인트 수: 50\n유효한 이상치 포인트 수: 50\n\n\n\n탐지 결과 시각화\n이상치로 탐지된 구간과 원본 데이터를 함께 시각화하여, 모델이 어떤 패턴을 ’이상’으로 간주했는지 확인합니다.\n\n# 시각화를 위해 원본 데이터 사용\noriginal_data = df_value[value_col].values\n\nplt.figure(figsize=(15, 8))\n\n# 상단: 전체 데이터와 탐지 결과\nplt.subplot(2, 1, 1)\nplt.plot(original_data, label='원본 전력 사용량', alpha=0.8)\n\nif len(predicted_anomaly_points) &gt; 0:\n    # 유효한 인덱스만 사용\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(original_data)]\n    if len(valid_indices) &gt; 0:\n        plt.scatter(valid_indices, original_data[valid_indices],\n                    color='red', marker='x', s=100, linewidth=2, label='탐지된 이상치', zorder=5)\n\nplt.title('실제 전력 사용량 데이터 이상 탐지 결과 (안정 구간 학습)', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# 하단: 재구성 오차와 임계값\nplt.subplot(2, 1, 2)\nplt.plot(reconstruction_error, label='윈도우별 재구성 오차', color='blue')\nplt.axhline(y=threshold, color='r', linestyle='--', label=f'임계값 ({threshold:.4f})')\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, reconstruction_error[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\n\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#결론-및-고찰",
    "href": "posts/ABC_week05_real_data_analysis/index.html#결론-및-고찰",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "5. 결론 및 고찰",
    "text": "5. 결론 및 고찰\n이번 실습을 통해, Week04에서 구축한 CNN 오토인코더 모델을 실제 주택 전력 사용량 데이터에 성공적으로 적용해 보았습니다. 가상의 데이터가 아닌 실제 데이터에 적용함으로써, 모델이 예측 불가능하고 불규칙한 패턴 속에서도 의미 있는 이상 신호를 포착할 수 있음을 확인했습니다.\n특히, 데이터의 특정 안정 구간만을 ’정상’으로 정의하고 학습시키는 전략을 통해, 데이터 전체에 걸친 패턴 변화(Regime Change)와 국소적인 이상치를 효과적으로 구분하여 탐지할 수 있었습니다. 시각화 결과, 모델이 학습하지 않은 초반부 구간은 전반적으로 높은 재구성 오차를 보이며 ’이상 구간’으로 탐지되었고, 학습에 사용된 안정 구간 내에서도 급격한 전력 사용량 변화를 성공적으로 포착했습니다.\n\n주요 인사이트\n\n안정 구간 학습의 효과: ‘정상’ 상태에 대한 명확한 정의는 비지도 학습 기반 이상 탐지 모델의 성능을 크게 향상시킬 수 있습니다. 데이터의 모든 부분을 동등하게 보는 대신, 도메인 지식이나 사전 분석을 통해 기준이 되는 구간을 선택하는 것이 중요합니다.\n단일 변수 시계열 데이터에서도 CNN 오토인코더는 효과적으로 정상 패턴을 학습하고, 급격한 전력 사용량 변화 등을 이상치로 탐지할 수 있습니다.\nQuantile 기반의 임계값 설정은 라벨이 없는 실제 데이터에서 통계적으로 안정적인 이상치 탐지 기준을 제공하는 유용한 방법입니다.\n실제 데이터는 예측치 못한 노이즈와 계절성, 추세 등 복합적인 패턴을 포함하고 있어, 모델이 이를 얼마나 잘 일반화하여 ’정상’으로 학습하는지가 이상 탐지 성능의 관건이 됩니다.\n\n\n\n한계 및 개선 방향\n\n정량적 평가의 어려움: 실제 이상치 라벨이 없어 모델의 성능을 정량적으로 평가(e.g., F1-Score)하기 어렵습니다.\n단변량의 한계: 현재 모델은 ‘전력 사용량’ 외의 다른 정보(요일, 시간, 날씨 등)를 활용하지 못해, 복합적인 요인으로 발생하는 이상을 탐지하는 데 한계가 있습니다.\n임계값의 민감도: Quantile 기반 임계값은 통계적으로 유용하지만, 데이터의 분포에 따라 탐지 민감도가 크게 달라질 수 있습니다. 실제 운영 환경에서는 여러 임계값을 테스트하며 최적의 균형점을 찾아야 합니다.\n\n향후에는 도메인 지식을 활용하여 이상치에 대한 명확한 기준을 정의하고, 다변량 시계열 모델을 도입하여 더 풍부한 정보를 바탕으로 이상 탐지 성능을 고도화하는 연구를 진행해 볼 수 있을 것입니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html",
    "href": "posts/ABC_week02_time_series_anomaly/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "",
    "text": "안녕하세요 이번 포스트는 ABC 프로젝트 멘토링 8기 2주차 실습 기록입니다. 지난주엔 시계열 데이터 EDA랑 전처리만 했는데, 이번엔 간단한 머신러닝 모델로 이상치 탐지 기법을 소개하려 합니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "1. 데이터 준비",
    "text": "1. 데이터 준비\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.random.seed(42)\nt = np.arange(0, 100, 1)\ny = np.sin(0.2 * t) + np.random.normal(0, 0.2, size=len(t))\n# 여러 위치에 인위적으로 이상치 추가\noutlier_indices = [15, 35, 55, 75, 90]\noutlier_values = [2, -2, 2.5, -2.5, 3]\nfor idx, val in zip(outlier_indices, outlier_values):\n    y[idx] += val\ndf = pd.DataFrame({'time': t, 'value': y})\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df.loc[outlier_indices, 'time'], df.loc[outlier_indices, 'value'], color='red', label='부여한 이상값')\nplt.legend()\nplt.title('이상값이 포함된 시계열 데이터')\nplt.show()\n\n\n\n\n이상값이 포함된 시계열 데이터"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)",
    "text": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)\n\n모델별 특징 및 한계\n\n\n\n\n\n\n\n\n모델\n장점\n한계/주의점\n\n\n\n\nIsolation Forest\n대용량/고차원 데이터에 강함, 빠름\n이상치 비율(contamination) 추정 필요\n\n\nDBSCAN\n군집/밀도 기반, 파라미터 직관적\neps, min_samples에 민감, 1차원 한계\n\n\nOne-Class SVM\n비선형 경계, 소규모 데이터에 적합\n느릴 수 있음, 파라미터 튜닝 필요\n\n\n\n\n\nIsolation Forest\n\nfrom sklearn.ensemble import IsolationForest\nmodel = IsolationForest(contamination=0.05, random_state=42)\ndf['anomaly_isof'] = model.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_isof']==-1]['time'], df[df['anomaly_isof']==-1]['value'], color='red', label='탐지된 이상값')\nplt.legend()\nplt.title('Isolation Forest 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nIsolation Forest 기반 이상 탐지 결과\n\n\n\n\n\n\nDBSCAN (밀도 기반 이상 탐지)\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['value']])\ndbscan = DBSCAN(eps=0.25, min_samples=3)  # eps와 min_samples를 조정해 민감도 조정\ndf['anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_dbscan']==-1]['time'], df[df['anomaly_dbscan']==-1]['value'], color='orange', label='탐지된 이상값(DBSCAN)')\nplt.legend()\nplt.title('DBSCAN 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nDBSCAN 기반 이상 탐지 결과\n\n\n\n\n\n\nOne-Class SVM (서포트 벡터 머신 기반 이상 탐지)\n\nfrom sklearn.svm import OneClassSVM\n# 기본 파라미터로는 이상치 탐지가 잘 안 됨 (F1이 0.14 수준)\nocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\ndf['anomaly_ocsvm'] = ocsvm.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm']==-1]['time'], df[df['anomaly_ocsvm']==-1]['value'], color='purple', label='탐지된 이상값(OCSVM)')\nplt.legend()\nplt.title('One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nOne-Class SVM 기반 이상 탐지 결과\n\n\n\n\n\nSVM 파라미터 튜닝 시도\n\n# gamma 값을 더 크게, nu 값을 더 높게 조정해서 민감도를 높임\nocsvm_tuned = OneClassSVM(nu=0.12, kernel='rbf', gamma=2)\ndf['anomaly_ocsvm_tuned'] = ocsvm_tuned.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm_tuned']==-1]['time'], df[df['anomaly_ocsvm_tuned']==-1]['value'], color='blue', label='탐지된 이상값(튜닝 SVM)')\nplt.legend()\nplt.title('튜닝된 One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\n튜닝된 One-Class SVM 이상 탐지 결과\n\n\n\n\n\n\n\n이상치 탐지 및 평가지표(Precision, Recall, F1)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef anomaly_metrics(true_outliers, pred_outliers, n):\n    true = [1 if i in true_outliers else 0 for i in range(n)]\n    pred = [1 if i in pred_outliers else 0 for i in range(n)]\n    p = precision_score(true, pred)\n    r = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    return p, r, f1\n\nn = len(df)\ntrue_outliers = outlier_indices\npred_isof = df.index[df['anomaly_isof']==-1].tolist()\np_isof, r_isof, f1_isof = anomaly_metrics(true_outliers, pred_isof, n)\npred_dbscan = df.index[df['anomaly_dbscan']==-1].tolist()\np_dbscan, r_dbscan, f1_dbscan = anomaly_metrics(true_outliers, pred_dbscan, n)\npred_ocsvm = df.index[df['anomaly_ocsvm']==-1].tolist()\np_ocsvm, r_ocsvm, f1_ocsvm = anomaly_metrics(true_outliers, pred_ocsvm, n)\npred_ocsvm_tuned = df.index[df['anomaly_ocsvm_tuned']==-1].tolist()\np_ocsvm_t, r_ocsvm_t, f1_ocsvm_t = anomaly_metrics(true_outliers, pred_ocsvm_tuned, n)\n\nprint(f\"Isolation Forest - Precision: {p_isof:.2f}, Recall: {r_isof:.2f}, F1: {f1_isof:.2f}\")\nprint(f\"DBSCAN           - Precision: {p_dbscan:.2f}, Recall: {r_dbscan:.2f}, F1: {f1_dbscan:.2f}\")\nprint(f\"One-Class SVM    - Precision: {p_ocsvm:.2f}, Recall: {r_ocsvm:.2f}, F1: {f1_ocsvm:.2f}\")\nprint(f\"튜닝 SVM         - Precision: {p_ocsvm_t:.2f}, Recall: {r_ocsvm_t:.2f}, F1: {f1_ocsvm_t:.2f}\")\n\nIsolation Forest - Precision: 1.00, Recall: 1.00, F1: 1.00\nDBSCAN           - Precision: 1.00, Recall: 1.00, F1: 1.00\nOne-Class SVM    - Precision: 0.14, Recall: 0.40, F1: 0.21\n튜닝 SVM         - Precision: 0.21, Recall: 1.00, F1: 0.34"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "3. 결과 해석 및 정리",
    "text": "3. 결과 해석 및 정리\n\nOne-Class SVM은 기본 파라미터로는 이상치 탐지가 잘 되지 않았으나, gamma와 nu를 조정해 튜닝하면 성능이 개선되는 것을 확인할 수 있다. 이 과정에서 파라미터 튜닝의 중요성을 경험했다.\n각 모델별로 이상치 탐지 결과와 평가지표(Precision, Recall, F1)가 다르게 나타난다. Isolation Forest는 인위적으로 넣은 이상치를 대부분 탐지했고, DBSCAN은 파라미터에 따라 민감하게 반응한다. One-Class SVM은 데이터 분포와 파라미터에 따라 결과가 크게 달라진다.\nPrecision(정밀도), Recall(재현율), F1-score는 모델의 이상치 탐지 성능을 종합적으로 평가하는 지표로, 실제 데이터 분석에서는 여러 방법을 비교하고 도메인 지식과 함께 해석하는 것이 중요하다."
  }
]