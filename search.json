[
  {
    "objectID": "posts/ABC_week01_data analysis/index.html",
    "href": "posts/ABC_week01_data analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "",
    "text": "유클리드소프트에서 진행하는 ABC 프로젝트 멘토링에 8기로 참여하게 되었습니다.   [산업 전력 데이터의 이상치 탐지 성능 향상 솔루션 구축]을 주제로 다양한 데이터 분석 및 인공지능 기법을 학습하고 실제 프로젝트에 적용해볼 예정입니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-데이터란",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 데이터란?",
    "text": "시계열 데이터란?\n시계열 데이터(Time Series Data)는 일정 시간 간격으로 기록된 데이터 포인트들의 순차적인 집합입니다. 예를 들어, 시간별 산업 설비의 전력 사용량, 일별 주가, 월별 웹사이트 방문자 수 등이 시계열 데이터에 해당합니다. 이러한 데이터는 시간의 흐름에 따른 변화와 패턴을 분석하는 데 사용되며, 특히 정상적인 패턴에서 벗어나는 ’이상치’를 탐지하는 데 중요한 기초 자료가 됩니다.\n시계열 데이터는 주로 다음과 같은 특징을 가집니다:\n\n추세 (Trend): 데이터가 장기적으로 증가하거나 감소하는 경향.\n계절성 (Seasonality): 특정 주기(예: 하루, 주, 월)에 따라 반복되는 패턴.\n주기성 (Cyclicality): 계절성보다 긴, 고정되지 않은 주기의 변동.\n불규칙 변동 (Irregular Fluctuations/Noise): 위 요소들로 설명되지 않는 무작위적 변동."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "href": "posts/ABC_week01_data analysis/index.html#시계열-이상-탐지에서-eda와-전처리의-중요성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "시계열 이상 탐지에서 EDA와 전처리의 중요성",
    "text": "시계열 이상 탐지에서 EDA와 전처리의 중요성\n이상치(Anomaly) 또는 특이점(Outlier)은 일반적인 데이터 패턴에서 현저하게 벗어나는 관측치를 의미합니다. 산업 전력 데이터에서 이상치는 설비 고장, 에너지 누수, 비정상적 공정 운영 등 중요한 문제를 시사할 수 있습니다. 효과적인 이상 탐지를 위해서는 데이터에 대한 깊이 있는 이해가 선행되어야 하며, 탐색적 데이터 분석(EDA)과 적절한 전처리는 이 과정의 핵심입니다.\nEDA와 전처리가 중요한 이유:\n\n데이터 특성 파악: 데이터의 분포, 추세, 계절성 등 기본적인 통계적 특성을 이해하여 ‘정상’ 상태의 기준을 설정하는 데 도움을 줍니다.\n잠재적 이상치 식별: 시각화 등을 통해 예상치 못한 급증, 급감 또는 패턴 변화를 초기에 발견할 수 있습니다.\n데이터 품질 향상: 결측치 처리, 노이즈 제거 등을 통해 분석의 정확도를 높입니다.\n피처 엔지니어링 기반 마련: 분석 목적에 맞는 새로운 변수를 생성하거나 기존 변수를 변환하는 데 필요한 통찰력을 제공합니다.\n적절한 이상 탐지 모델 선택 지원: 데이터의 특성에 맞는 이상 탐지 알고리즘을 선택하는 데 중요한 정보를 제공합니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "href": "posts/ABC_week01_data analysis/index.html#python을-이용한-시계열-데이터-eda-및-전처리-기초",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초",
    "text": "Python을 이용한 시계열 데이터 EDA 및 전처리 기초\nPython의 pandas, numpy, matplotlib, seaborn 라이브러리를 사용하여 산업 전력 사용량 데이터를 가정하고, 이상 탐지를 위한 기본적인 EDA 및 전처리 과정을 살펴보겠습니다.\n\n1. 필요한 라이브러리 불러오기\n데이터 분석 및 시각화에 필요한 라이브러리를 가져옵니다.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # 향상된 시각화를 위한 Seaborn\nfrom datetime import datetime\n\n# 경고 메시지 무시 (선택 사항)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\n2. 분석용 샘플 시계열 데이터 생성 (가상 산업 전력 사용량)\n실제 산업 전력 데이터와 유사한 특성을 갖도록 가상 데이터를 생성합니다. 여기에는 일정한 기본 사용량, 약간의 증가 추세, 주간 계절성(평일 사용량 증가, 주말 감소), 그리고 몇 개의 인위적인 이상치(스파이크 및 급감)를 포함시킵니다.\n\n# 재현성을 위한 시드 설정\nnp.random.seed(42)\n\n# 날짜 범위 생성 (약 1년)\ndate_rng = pd.date_range(start='2025-01-01', periods=365, freq='D')\ndata = pd.DataFrame(date_rng, columns=['date'])\n\n# 기본 전력 사용량 설정 및 추세 생성\nbaseline_usage = 100  # 기본 사용량 (예: kWh)\ntrend_factor = np.linspace(0, 20, len(date_rng)) # 선형 증가 추세\n\n# 주간 계절성 생성 (월:0 ~ 일:6)\n# 산업 데이터 특성상 평일 사용량 높고, 주말 낮음\nday_of_week_effect = np.array([15, 18, 20, 19, 17, 5, 3])\nseasonal_factor = np.array([day_of_week_effect[day.weekday()] for day in date_rng])\n\n# 임의의 노이즈 생성\nnoise = np.random.normal(0, 5, size=(len(date_rng))) # 평균 0, 표준편차 5\n\n# 데이터 생성 (전력 사용량 = 기본값 + 추세 + 계절성 + 노이즈)\ndata['power_usage'] = baseline_usage + trend_factor + seasonal_factor + noise\n\n# 인위적인 이상치(스파이크 및 급감) 추가\ndata.loc[data.index[50], 'power_usage'] += 70  # 51번째 날에 큰 스파이크\ndata.loc[data.index[150], 'power_usage'] -= 50 # 151번째 날에 큰 폭 하락\ndata.loc[data.index[250], 'power_usage'] += 80  # 251번째 날에 큰 스파이크\n\n# 데이터 값 보정 (음수 방지 및 최소값 설정)\ndata['power_usage'] = data['power_usage'].astype(float).clip(lower=10)\n\n# 'date' 컬럼을 인덱스로 설정\ndata.set_index('date', inplace=True)\n\nprint(\"생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\")\nprint(data.head())\nprint(\"\\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\")\nprint(data.tail())\n\n생성된 가상 전력 사용량 데이터 샘플 (상위 5개):\n            power_usage\ndate                   \n2025-01-01   122.483571\n2025-01-02   118.363624\n2025-01-03   120.348333\n2025-01-04   112.779984\n2025-01-05   102.049013\n\n생성된 가상 전력 사용량 데이터 샘플 (하위 5개):\n            power_usage\ndate                   \n2025-12-27   127.376952\n2025-12-28   130.498859\n2025-12-29   134.346309\n2025-12-30   139.953614\n2025-12-31   143.450720\n\n\n이 샘플 데이터는 power_usage라는 이름으로 전력 사용량 정보를 가지며, EDA 과정에서 이상치를 시각적으로 탐색하는 데 사용됩니다.\n\n\n\n3. 데이터 기본 탐색\n데이터의 구조와 기본적인 통계적 특성을 확인합니다.\n\nprint(\"데이터 정보:\")\ndata.info()\n\nprint(\"\\n기술 통계량:\")\nprint(data.describe())\n\nprint(f\"\\n결측치 확인: {data.isnull().sum().sum()} 개\")\n# data.isnull().sum() # 컬럼별 결측치 확인\n\n데이터 정보:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 365 entries, 2025-01-01 to 2025-12-31\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   power_usage  365 non-null    float64\ndtypes: float64(1)\nmemory usage: 5.7 KB\n\n기술 통계량:\n       power_usage\ncount   365.000000\nmean    124.197677\nstd      11.877300\nmin      64.494222\n25%     117.423353\n50%     124.093359\n75%     131.043033\nmax     202.431844\n\n결측치 확인: 0 개\n\n\ninfo()는 데이터 타입, 인덱스 정보, 메모리 사용량 등을 보여줍니다. describe()는 평균, 표준편차, 최소/최대값, 사분위수 등 주요 기술 통계량을 제공하여 데이터의 전반적인 분포를 파악하는 데 도움을 줍니다. 결측치가 있다면 이상 탐지 분석 전에 적절히 처리(예: 보간, 제거)해야 합니다. 이 샘플에서는 결측치가 없습니다.\n\n\n\n4. 주요 시각화를 통한 탐색적 데이터 분석 (EDA)\n시각화는 시계열 데이터의 패턴과 잠재적 이상치를 발견하는 데 매우 효과적입니다.\n\n4.1. 기본 시계열 플롯\n전체 기간에 대한 전력 사용량 변화를 시각화하여 추세, 계절성, 그리고 눈에 띄는 이상 패턴을 관찰합니다.\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='dodgerblue', linewidth=1.5)\nplt.title('일별 가상 산업 전력 사용량', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 1: 일별 가상 산업 전력 사용량\n\n\n\n\n\n위 그래프에서 전반적인 증가 추세와 주기적인 변동(계절성) 외에도, 몇몇 지점에서 급격한 스파이크나 하락(우리가 삽입한 이상치)이 시각적으로 확인됩니다. 실제 데이터 분석 시 이러한 지점들이 조사 대상이 됩니다.\n\n\n\n4.2. 데이터 분포 확인 (히스토그램 및 KDE)\n전력 사용량 값들의 분포를 확인하여 데이터가 특정 구간에 집중되어 있는지, 또는 분포에서 벗어나는 값들이 있는지 살펴봅니다.\n\nplt.figure(figsize=(10, 6))\nsns.histplot(data['power_usage'], kde=True, color='mediumseagreen', bins=30)\nplt.title('전력 사용량 분포 (히스토그램 및 KDE)', fontsize=16)\nplt.xlabel('전력 사용량 (kWh)', fontsize=12)\nplt.ylabel('빈도', fontsize=12)\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 2: 전력 사용량 분포\n\n\n\n\n\n히스토그램과 KDE(Kernel Density Estimate) 플롯은 데이터 값의 분포를 보여줍니다. 만약 분포의 꼬리 부분에 값이 드물게 나타난다면 이는 이상치일 가능성이 있습니다. 우리가 삽입한 인위적인 스파이크 값들이 분포의 오른쪽 꼬리 부분에 나타날 수 있습니다.\n\n\n\n4.3. 주기적 패턴 확인 (요일별 Box Plot)\n산업 데이터는 요일이나 월별로 뚜렷한 주기성을 가질 수 있습니다. Box plot을 사용하면 이러한 주기성 내에서 평소와 다른 패턴을 보이는 시점을 파악하는 데 유용합니다.\n\n# 분석을 위해 'day_of_week' 컬럼 추가 (월요일=0, 일요일=6)\ndata['day_of_week'] = data.index.dayofweek\n\nplt.figure(figsize=(9, 5))\nsns.boxplot(x='day_of_week', y='power_usage', data=data, palette='coolwarm')\nplt.title('요일별 전력 사용량 분포', fontsize=16)\nplt.xlabel('요일 (0:월, 1:화, 2:수, 3:목, 4:금, 5:토, 6:일)', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.xticks(ticks=range(7), labels=['월', '화', '수', '목', '금', '토', '일'])\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 3: 요일별 전력 사용량 분포\n\n\n\n\n\n요일별 Box plot은 각 요일의 전력 사용량 분포를 보여줍니다. 각 박스는 해당 요일 데이터의 중앙 50%(IQR: Interquartile Range)를 나타내며, 박스 외부의 점들은 잠재적인 이상치(outliers)를 의미합니다. 이 샘플에서는 주말(토, 일) 사용량이 평일보다 낮은 패턴이 뚜렷하며, 우리가 인위적으로 삽입한 이상치들이 특정 요일의 일반적인 범위를 벗어나 점으로 표시될 수 있습니다. 예를 들어, 화요일(1)에 발생시킨 스파이크는 화요일의 박스 플롯에서 상단 이상치로 나타날 가능성이 큽니다.\n\n\n\n\n5. 이동 평균을 활용한 추세 및 변동성 관찰\n이동 평균(Moving Average)은 단기적인 변동을 완화하여 장기적인 추세를 파악하거나, 데이터의 일반적인 수준을 나타내는 기준으로 활용될 수 있습니다. 원본 데이터와 이동 평균선을 함께 시각화하면, 이동 평균에서 크게 벗어나는 지점들을 이상치 후보로 간주할 수 있습니다.\n\n# 7일 이동 평균 계산\ndata['rolling_mean_7'] = data['power_usage'].rolling(window=7, center=True).mean() # center=True로 설정하여 lag 감소 효과\n\nplt.figure(figsize=(9, 6))\nplt.plot(data.index, data['power_usage'], label='일별 전력 사용량', color='lightskyblue', alpha=0.8, linewidth=1)\nplt.plot(data.index, data['rolling_mean_7'], label='7일 이동 평균 (중앙 정렬)', color='orangered', linewidth=2)\nplt.title('일별 전력 사용량 및 7일 이동 평균', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('전력 사용량 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\nFigure 4: 전력 사용량과 7일 이동 평균\n\n\n\n\n\n\n# 이동 평균과의 차이(잔차와 유사한 개념)를 통해 이상치 강조\ndata['deviation_from_ma'] = data['power_usage'] - data['rolling_mean_7']\n\nplt.figure(figsize=(9,5))\nplt.plot(data.index, data['deviation_from_ma'], label='이동 평균과의 편차', color='teal', linewidth=1, marker='o', markersize=3, linestyle='None')\nplt.axhline(0, color='black', linestyle='--', linewidth=0.8) # 기준선\n\n# 편차의 임계값을 설정하여 이상치 후보 시각화 (예: 편차의 표준편차 기반)\n# 이동 평균 계산 시 초반/후반 NaN 값이 있을 수 있으므로 dropna() 사용\ndeviation_std = data['deviation_from_ma'].dropna().std()\nupper_threshold = 3 * deviation_std\nlower_threshold = -3 * deviation_std\n\nplt.axhline(upper_threshold, color='red', linestyle=':', linewidth=1.5, label=f'+3σ ({upper_threshold:.2f})')\nplt.axhline(lower_threshold, color='red', linestyle=':', linewidth=1.5, label=f'-3σ ({lower_threshold:.2f})')\nplt.title('이동 평균과의 편차 (이상치 탐색 보조)', fontsize=16)\nplt.xlabel('날짜', fontsize=12)\nplt.ylabel('편차 (kWh)', fontsize=12)\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\nFigure 5: 이동 평균과의 편차 (이상치 탐색 보조)\n\n\n\n\n\n7일 이동 평균선은 데이터의 단기적 변동을 평탄화하여 보여줍니다. rolling() 함수에서 center=True 옵션을 사용하면 이동 평균 계산 시 윈도우의 중앙에 값을 위치시켜 시각화 시 원본 데이터와의 지연(lag)을 줄이는 데 도움이 됩니다.\n두 번째 그래프는 원본 데이터와 이동 평균과의 편차를 보여줍니다. 이 편차가 특정 임계값(예: 편차의 3 표준편차, ±3σ)을 넘어서는 지점들은 잠재적인 이상치로 간주할 수 있습니다. 우리가 삽입한 인위적인 스파이크와 급감 지점에서 편차가 크게 나타나는 것을 확인할 수 있습니다. 이러한 방법은 간단하면서도 효과적인 이상치 탐색의 기초가 됩니다."
  },
  {
    "objectID": "posts/ABC_week01_data analysis/index.html#요약",
    "href": "posts/ABC_week01_data analysis/index.html#요약",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리",
    "section": "요약",
    "text": "요약\n이 포스트에서는 Python을 사용하여 가상의 산업 전력 사용량 데이터를 생성하고, 이상 탐지를 위한 기본적인 탐색적 데이터 분석(EDA) 및 전처리 과정을 살펴보았습니다. 시계열 플롯, 분포 확인, 주기성 분석(요일별 Box Plot), 이동 평균 활용 등은 데이터의 특성을 이해하고 잠재적인 이상치를 식별하는 데 효과적인 방법입니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html",
    "href": "posts/ABC_week04_model_optimization/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 네 번째 기술노트입니다. 지난주에는 PyTorch를 이용해 CNN 오토인코더 기반의 시계열 이상 탐지 베이스라인 모델을 구현했습니다. 이번 주에는 해당 모델의 한계를 명확히 분석하고, 이를 개선하기 위한 구체적인 방법론과 하이퍼파라미터 최적화 라이브러리 ’Optuna’를 활용한 실험 과정을 상세히 공유합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "href": "posts/ABC_week04_model_optimization/index.html#기존-모델의-한계-명확히-하기",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "1. 기존 모델의 한계 명확히 하기",
    "text": "1. 기존 모델의 한계 명확히 하기\n모든 모델링의 시작은 현재 모델을 정확히 아는 것입니다. Week3에서 구현한 베이스라인 모델은 가능성을 보여주었지만, 몇 가지 명확한 한계점을 가지고 있었습니다.\n\n1.1. 탐지 성능의 아쉬움: 놓치거나, 잘못 잡거나\n지난주 결과 그래프를 다시 살펴보면, 실제 이상치(Ground Truth) 3개 중 일부를 탐지하지 못하거나(False Negative), 반대로 정상 구간을 이상치로 판단하는(False Positive) 경향을 보였습니다.\n\n탐지 누락 (False Negative): 80번 인덱스 주변의 실제 이상치는 재구성 오차가 임계값을 넘지 않아 탐지되지 않았습니다. 이는 모델이 해당 유형의 이상 패턴(상대적으로 변화의 폭이 작은 이상치)을 정상 데이터의 일부로 학습했음을 의미합니다. 모델이 너무 ’관대’하게 데이터를 복원하고 있는 것입니다.\n오탐 (False Positive): 시계열 데이터의 시작 부분(0~10 인덱스)에서 재구성 오차가 높게 나타났습니다. 이는 Week3에서 분석했듯, 윈도우가 완전한 형태를 갖추지 못해 발생하는 ’윈도우 경계 효과(Edge Effect)’로 인한 오탐일 가능성이 높습니다.\n\n\n\n\n지난주 탐지 결과 그래프\n\n\n\n그림 1. Week3 모델의 이상 탐지 결과. 일부 이상치를 놓치고, 경계면에서 오탐이 발생했다.\n\n\n\n1.2. 과적합(Overfitting) 가능성\n오토인코더는 정상 데이터의 핵심 패턴을 학습해야 하지만, 너무 학습 데이터에만 치중하면 ’과적합’되어 미세한 노이즈까지 모두 정상으로 간주하게 됩니다. 이 경우, 새로운 형태의 이상치가 들어왔을 때 재구성 오차를 효과적으로 만들어내지 못해 탐지 성능이 저하됩니다. 현재 모델은 Dropout이나 규제(Regularization) 같은 과적합 방지 장치가 없어 이러한 위험에 노출되어 있습니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "href": "posts/ABC_week04_model_optimization/index.html#성능-개선을-위한-접근-전략",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "2. 성능 개선을 위한 접근 전략",
    "text": "2. 성능 개선을 위한 접근 전략\n위에서 정의한 문제들을 해결하기 위해 다음과 같은 세 가지 전략을 시도했습니다.\n\n2.1. 데이터 전처리 방식 변경: StandardScaler 도입\n기존 모델은 별도의 스케일링 없이 마지막 레이어의 Sigmoid 활성화 함수를 통해 출력을 0과 1 사이로 맞췄습니다. 이는 데이터의 분포가 0과 1 사이에 고르게 분포하지 않을 경우 정보 손실을 야기할 수 있고, 이상치의 특성을 약화시킬 수 있습니다.\n데이터를 평균 0, 표준편차 1을 갖도록 정규화하는 StandardScaler를 적용하여 모델이 데이터의 분포 특성을 더 잘 학습하고, 이상치와 정상 데이터 간의 차이를 더 명확하게 인지하도록 유도했습니다.\n\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n# 예시 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# StandardScaler 적용\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data.reshape(-1, 1)).flatten()\n\nprint(f\"원본 데이터 평균/표준편차: {np.mean(data):.2f} / {np.std(data):.2f}\")\nprint(f\"스케일링 후 평균/표준편차: {np.mean(scaled_data):.2f} / {np.std(scaled_data):.2f}\")\n\n원본 데이터 평균/표준편차: 0.03 / 0.84\n스케일링 후 평균/표준편차: 0.00 / 1.00\n\n\n\n\n2.2. 모델 구조 변경: 과적합 방지를 위한 Dropout 추가\n모델의 일반화 성능을 높이고 과적합을 방지하기 위해 Dropout 레이어를 추가했습니다. Dropout은 학습 과정에서 각 뉴런을 확률적으로 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 막습니다. 주로 활성화 함수(ReLU) 뒤에 위치시켜 정보의 흐름을 조절합니다.\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_shape, dropout_rate=0.2):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        x = self.decoder_conv_final(x)\n        # StandardScaler를 사용하므로 마지막 Sigmoid 활성화 함수는 제거\n        return x\n\n# 모델 테스트\nwindow_size = 10\nmodel = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=0.2)\nprint(model)\n\nCNNAutoencoderWithDropout(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_drop1): Dropout(p=0.2, inplace=False)\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_drop2): Dropout(p=0.2, inplace=False)\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_drop3): Dropout(p=0.2, inplace=False)\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_drop4): Dropout(p=0.2, inplace=False)\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n)\n\n\n\n\n2.3. 하이퍼파라미터 최적화: Optuna 활용\n모델 성능에 영향을 미치는 하이퍼파라미터(학습률, 드롭아웃 비율, 필터 수 등)를 체계적으로 찾기 위해 Optuna 라이브러리를 사용합니다. Optuna는 베이지안 최적화 기법을 기반으로 효율적인 탐색을 수행합니다."
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#optuna를-이용한-최적화-실험-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "3. Optuna를 이용한 최적화 실험 (수정)",
    "text": "3. Optuna를 이용한 최적화 실험 (수정)\n\n3.1. objective 함수 정의 (수정)\n가장 큰 변경점은 정상 데이터만으로 모델을 학습하고 검증하는 것입니다. 아래 코드에서는 실제 이상치 인덱스(outliers)가 포함되지 않은 ’정상 윈도우’만 필터링하여 학습 및 검증에 사용합니다.\n\nimport optuna\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader, random_split\n\n# --- 데이터 준비 (정상/이상 분리) ---\nwindow_size = 10\n\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# data-scaling 셀의 scaled_data(이상치 포함)로 전체 윈도우 생성\nall_windows = sliding_window(scaled_data, window_size)\nall_windows_torch = torch.from_numpy(all_windows[..., np.newaxis].transpose(0, 2, 1)).float()\n\n# 학습에 사용할 정상 윈도우만 필터링\noutliers = [20, 50, 80]\nnormal_window_indices = []\nfor i in range(len(all_windows)):\n    window_range = range(i, i + window_size)\n    if not any(outlier_idx in window_range for outlier_idx in outliers):\n        normal_window_indices.append(i)\n\n# 정상 윈도우만으로 학습 데이터셋 구성\nnormal_windows_torch = all_windows_torch[normal_window_indices]\nnormal_dataset = TensorDataset(normal_windows_torch)\n\n# 정상 데이터셋을 학습용과 검증용으로 분리\ntrain_size = int(0.8 * len(normal_dataset))\nval_size = len(normal_dataset) - train_size\ntrain_dataset, val_dataset = random_split(normal_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n\n# --- Objective 함수 정의 ---\ndef objective(trial):\n    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n\n    model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=dropout_rate)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    criterion = nn.MSELoss()\n\n    # 정상 데이터로만 학습\n    epochs = 50\n    for epoch in range(epochs):\n        model.train()\n        for data in train_loader:\n            inputs = data[0]\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            loss.backward()\n            optimizer.step()\n\n    # 정상 데이터로만 검증\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs = data[0]\n            outputs = model(inputs)\n            loss = criterion(outputs, inputs)\n            val_loss += loss.item()\n    \n    return val_loss / len(val_loader)\n\n# --- Optuna Study 실행 ---\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint(\"Best trial:\", study.best_trial.params)\n\n[I 2025-06-19 14:45:28,868] A new study created in memory with name: no-name-bc54e835-17b1-4f71-a61d-d223ced9b8d3\n[I 2025-06-19 14:45:30,456] Trial 0 finished with value: 0.06334652751684189 and parameters: {'lr': 0.003421916009997713, 'dropout_rate': 0.28028168133112175, 'optimizer': 'RMSprop'}. Best is trial 0 with value: 0.06334652751684189.\n[I 2025-06-19 14:45:30,874] Trial 1 finished with value: 0.046124957501888275 and parameters: {'lr': 0.0043746199623402775, 'dropout_rate': 0.2967113014644945, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:31,280] Trial 2 finished with value: 0.735363781452179 and parameters: {'lr': 1.1746722794402529e-05, 'dropout_rate': 0.13206784629853102, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:31,684] Trial 3 finished with value: 0.24190421402454376 and parameters: {'lr': 0.0003434999019824716, 'dropout_rate': 0.47729138634839596, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:32,080] Trial 4 finished with value: 0.13211888074874878 and parameters: {'lr': 0.0002794533812576449, 'dropout_rate': 0.2728249979334575, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:32,488] Trial 5 finished with value: 0.7103785276412964 and parameters: {'lr': 5.8891220415794844e-05, 'dropout_rate': 0.19564089747763735, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:32,907] Trial 6 finished with value: 0.12117379158735275 and parameters: {'lr': 0.0024778648083909286, 'dropout_rate': 0.434223365318973, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:33,340] Trial 7 finished with value: 0.22438548505306244 and parameters: {'lr': 0.0006544562100311633, 'dropout_rate': 0.46539091332213123, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:33,746] Trial 8 finished with value: 0.2257339507341385 and parameters: {'lr': 0.00020202494082413542, 'dropout_rate': 0.1482842960758194, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:34,154] Trial 9 finished with value: 0.6840315461158752 and parameters: {'lr': 7.28353829552139e-05, 'dropout_rate': 0.2119168216757689, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:34,558] Trial 10 finished with value: 0.1671738475561142 and parameters: {'lr': 0.0058692941262655784, 'dropout_rate': 0.36934152548315935, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:34,963] Trial 11 finished with value: 0.14934806525707245 and parameters: {'lr': 0.0072995681734922165, 'dropout_rate': 0.2989399825571527, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:35,367] Trial 12 finished with value: 0.08916617929935455 and parameters: {'lr': 0.0017782169962648183, 'dropout_rate': 0.34347603590470405, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:35,769] Trial 13 finished with value: 0.06546976417303085 and parameters: {'lr': 0.0018653158332503196, 'dropout_rate': 0.2417847164610769, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:36,188] Trial 14 finished with value: 0.13770273327827454 and parameters: {'lr': 0.009339313067282973, 'dropout_rate': 0.3579857032185798, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:36,590] Trial 15 finished with value: 0.08588606119155884 and parameters: {'lr': 0.0028675358515582957, 'dropout_rate': 0.3224243927298959, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:37,010] Trial 16 finished with value: 0.15133486688137054 and parameters: {'lr': 0.0008488495238381222, 'dropout_rate': 0.39974843282756645, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:37,411] Trial 17 finished with value: 0.11048386245965958 and parameters: {'lr': 0.0037708915717061245, 'dropout_rate': 0.2655184269950121, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:37,811] Trial 18 finished with value: 0.05382830649614334 and parameters: {'lr': 0.0012637034914403224, 'dropout_rate': 0.1814697317686519, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.046124957501888275.\n[I 2025-06-19 14:45:38,221] Trial 19 finished with value: 0.04041090980172157 and parameters: {'lr': 0.0008660070343830133, 'dropout_rate': 0.10585194173788441, 'optimizer': 'Adam'}. Best is trial 19 with value: 0.04041090980172157.\n[I 2025-06-19 14:45:38,630] Trial 20 finished with value: 0.07391653209924698 and parameters: {'lr': 0.0004874965160309772, 'dropout_rate': 0.1223252094056152, 'optimizer': 'Adam'}. Best is trial 19 with value: 0.04041090980172157.\n[I 2025-06-19 14:45:39,038] Trial 21 finished with value: 0.058838557451963425 and parameters: {'lr': 0.0011270255984152185, 'dropout_rate': 0.17787993169245814, 'optimizer': 'Adam'}. Best is trial 19 with value: 0.04041090980172157.\n[I 2025-06-19 14:45:39,444] Trial 22 finished with value: 0.03987562656402588 and parameters: {'lr': 0.0010755975419083992, 'dropout_rate': 0.1505110823738097, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.03987562656402588.\n[I 2025-06-19 14:45:39,851] Trial 23 finished with value: 0.6426147818565369 and parameters: {'lr': 9.391935765209017e-05, 'dropout_rate': 0.10495835229744474, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.03987562656402588.\n[I 2025-06-19 14:45:40,258] Trial 24 finished with value: 0.46252188086509705 and parameters: {'lr': 0.00015842486398898683, 'dropout_rate': 0.15497601869974317, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.03987562656402588.\n[I 2025-06-19 14:45:40,668] Trial 25 finished with value: 0.0674772560596466 and parameters: {'lr': 0.005072170270391801, 'dropout_rate': 0.22771299511099863, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.03987562656402588.\n[I 2025-06-19 14:45:41,074] Trial 26 finished with value: 0.08447146415710449 and parameters: {'lr': 0.00045801829481807374, 'dropout_rate': 0.15572088946203208, 'optimizer': 'Adam'}. Best is trial 22 with value: 0.03987562656402588.\n[I 2025-06-19 14:45:41,481] Trial 27 finished with value: 0.034343745559453964 and parameters: {'lr': 0.0014570304252435212, 'dropout_rate': 0.10279797820641706, 'optimizer': 'Adam'}. Best is trial 27 with value: 0.034343745559453964.\n[I 2025-06-19 14:45:41,888] Trial 28 finished with value: 0.03833875060081482 and parameters: {'lr': 0.0008830367605645098, 'dropout_rate': 0.11431392978926035, 'optimizer': 'Adam'}. Best is trial 27 with value: 0.034343745559453964.\n[I 2025-06-19 14:45:42,292] Trial 29 finished with value: 0.02804953046143055 and parameters: {'lr': 0.0014743110104494105, 'dropout_rate': 0.10138570353174961, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:42,698] Trial 30 finished with value: 0.02884967252612114 and parameters: {'lr': 0.0016122168877391852, 'dropout_rate': 0.1030855720762458, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:43,103] Trial 31 finished with value: 0.042230911552906036 and parameters: {'lr': 0.0015908396223430984, 'dropout_rate': 0.10831120166315042, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:43,510] Trial 32 finished with value: 0.03667662292718887 and parameters: {'lr': 0.002452678540482687, 'dropout_rate': 0.1381709823161591, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:43,917] Trial 33 finished with value: 0.03893901780247688 and parameters: {'lr': 0.0028870980837578672, 'dropout_rate': 0.13358422125894706, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:44,326] Trial 34 finished with value: 0.0441921129822731 and parameters: {'lr': 0.003910481403779853, 'dropout_rate': 0.17201627263358743, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:44,737] Trial 35 finished with value: 0.7218798398971558 and parameters: {'lr': 2.89733708395414e-05, 'dropout_rate': 0.13607288008797913, 'optimizer': 'Adam'}. Best is trial 29 with value: 0.02804953046143055.\n[I 2025-06-19 14:45:45,144] Trial 36 finished with value: 0.025479132309556007 and parameters: {'lr': 0.0019327569889410559, 'dropout_rate': 0.10020178778269362, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:45,552] Trial 37 finished with value: 0.06841593235731125 and parameters: {'lr': 0.000579271091813191, 'dropout_rate': 0.18860172608506665, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:45,957] Trial 38 finished with value: 0.06766275316476822 and parameters: {'lr': 0.0003495961594395624, 'dropout_rate': 0.10049468392777304, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:46,366] Trial 39 finished with value: 0.07880651205778122 and parameters: {'lr': 0.0018532410174156256, 'dropout_rate': 0.21328057179233384, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:46,773] Trial 40 finished with value: 0.07380738109350204 and parameters: {'lr': 0.0007284135775471217, 'dropout_rate': 0.16766816846104, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:47,180] Trial 41 finished with value: 0.031130582094192505 and parameters: {'lr': 0.002211946061154381, 'dropout_rate': 0.13488192167746885, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:47,588] Trial 42 finished with value: 0.037563204765319824 and parameters: {'lr': 0.002228842683331732, 'dropout_rate': 0.12197404279442708, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:47,994] Trial 43 finished with value: 0.04060780256986618 and parameters: {'lr': 0.0013377501613337011, 'dropout_rate': 0.12373991234073858, 'optimizer': 'Adam'}. Best is trial 36 with value: 0.025479132309556007.\n[I 2025-06-19 14:45:48,398] Trial 44 finished with value: 0.025220464915037155 and parameters: {'lr': 0.0036588340445744675, 'dropout_rate': 0.1000764589949432, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n[I 2025-06-19 14:45:48,803] Trial 45 finished with value: 0.0375518724322319 and parameters: {'lr': 0.006119901976553171, 'dropout_rate': 0.13776469255283372, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n[I 2025-06-19 14:45:49,212] Trial 46 finished with value: 0.04620819538831711 and parameters: {'lr': 0.0038702245499651873, 'dropout_rate': 0.1620824360629894, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n[I 2025-06-19 14:45:49,621] Trial 47 finished with value: 0.042421579360961914 and parameters: {'lr': 0.00949946060782026, 'dropout_rate': 0.20126948009637663, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n[I 2025-06-19 14:45:50,026] Trial 48 finished with value: 0.04602474346756935 and parameters: {'lr': 0.0028650358620585558, 'dropout_rate': 0.12656833408768345, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n[I 2025-06-19 14:45:50,434] Trial 49 finished with value: 0.05839777737855911 and parameters: {'lr': 0.004871413562404475, 'dropout_rate': 0.24900469563060434, 'optimizer': 'Adam'}. Best is trial 44 with value: 0.025220464915037155.\n\n\nBest trial: {'lr': 0.0036588340445744675, 'dropout_rate': 0.1000764589949432, 'optimizer': 'Adam'}\n\n\n\n\n3.2. 최적화 결과 분석 및 시각화\n(이 부분은 기존과 동일합니다. Optuna 실행 후 주석을 해제하여 사용하세요.)\n\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\nfig1 = plot_optimization_history(study)\nfig1.update_layout(width=800, height=500)\nfig1.show()\nfig2 = plot_param_importances(study)\nfig2.update_layout(width=800, height=400)\nfig2.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "href": "posts/ABC_week04_model_optimization/index.html#최종-모델-성능-비교-및-결론-수정",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "4. 최종 모델 성능 비교 및 결론 (수정)",
    "text": "4. 최종 모델 성능 비교 및 결론 (수정)\n\n4.1. 최적 파라미터로 모델 재학습 및 평가 (수정)\nOptuna가 찾은 최적 파라미터로 정상 데이터 전체를 사용해 최종 모델을 학습합니다. 그 후, 정상 데이터의 재구성 오차를 기준으로 임계값을 설정하고, 이 기준을 전체 데이터에 적용하여 이상치를 탐지합니다.\n\n# --- 1. 최적 파라미터로 최종 모델 정의 ---\nbest_params = study.best_params \n\nfinal_model = CNNAutoencoderWithDropout(input_shape=(window_size, 1), dropout_rate=best_params['dropout_rate'])\noptimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['lr'])\ncriterion = nn.MSELoss()\n\n# --- 2. \"정상 데이터 전체\"로 모델 학습 ---\n# optuna-objective-fixed 셀에서 생성된 normal_dataset 사용\nfull_normal_loader = DataLoader(normal_dataset, batch_size=16, shuffle=True)\nepochs = 100\nprint(\"최종 모델 학습 시작...\")\nfor epoch in range(epochs):\n    for data in full_normal_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = final_model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")\n\n# --- 3. \"정상 데이터의 재구성 오차\"로 임계값 설정 ---\nfinal_model.eval()\ntrain_reconstruction_error = []\nwith torch.no_grad():\n    # normal_windows_torch는 optuna-objective-fixed 셀에서 생성\n    reconstructed = final_model(normal_windows_torch)\n    error = torch.mean((normal_windows_torch - reconstructed)**2, dim=(1, 2))\n    train_reconstruction_error = error.numpy()\n\n# # 표준편차 기반 임계값 대신, Quantile(백분위수)을 사용하여 더 강건한 임계값 설정\n# # 정상 데이터의 재구성 오차 중 상위 0.3%에 해당하는 값을 임계값으로 사용 (오탐에 더 강해짐)\n# quantile_level = 0.997\n# threshold = np.quantile(train_reconstruction_error, quantile_level)\nthreshold=0.25 # MSE 0.25 넘으면 이상 윈도우 탐지\n# print(f\"\\n임계값 (Threshold) 설정 완료 ({quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# --- 4. \"전체 데이터\"에 대한 재구성 오차 계산 및 이상치 탐지 (수정) ---\nfinal_model.eval()\nwith torch.no_grad():\n    # all_windows_torch의 shape: (num_windows, 1, window_size)\n    reconstructed = final_model(all_windows_torch)\n    \n    # 윈도우별 평균 오차 계산 (임계값 비교용)\n    mean_error_per_window = torch.mean((all_windows_torch - reconstructed)**2, dim=(1, 2)).numpy()\n    \n    # 개별 데이터 포인트별 오차 계산 (가장 큰 오차 지점 탐색용)\n    pointwise_error = (all_windows_torch - reconstructed)**2\n    pointwise_error = pointwise_error.squeeze().numpy() # Shape: (num_windows, window_size)\n\n# 1. 임계값을 초과하는 \"이상치 윈도우\" 식별\nanomaly_window_indices = np.where(mean_error_per_window &gt; threshold)[0]\nprint(f\"탐지된 이상치 윈도우 인덱스: {anomaly_window_indices}\")\n\n# 2. 각 이상치 윈도우 내에서 오차가 가장 큰 \"단일 데이터 포인트\"의 인덱스 찾기\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    # 현재 윈도우의 개별 포인트 오차\n    errors_in_window = pointwise_error[window_idx]\n    # 오차가 가장 큰 포인트의 \"윈도우 내 상대적 인덱스\"\n    max_error_idx_in_window = np.argmax(errors_in_window)\n    # \"전체 데이터에서의 절대적 인덱스\" 계산\n    absolute_idx = window_idx + max_error_idx_in_window\n    predicted_anomaly_points.append(absolute_idx)\n\n# 중복 제거 및 정렬\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 인덱스: {predicted_anomaly_points}\")\n\n# 비교를 위해 실제 이상치가 포함된 윈도우 인덱스 출력\nanomaly_window_indices_ground_truth = sorted(list(set(range(len(all_windows))) - set(normal_window_indices)))\nprint(f\"실제 이상치 포함 윈도우 인덱스: {anomaly_window_indices_ground_truth}\")\n\n# all_reconstruction_error 변수는 하단 그래프에서 사용하므로 윈도우별 평균 오차를 할당\nall_reconstruction_error = mean_error_per_window\n\n최종 모델 학습 시작...\nEpoch [20/100], Loss: 0.034458\nEpoch [40/100], Loss: 0.036697\nEpoch [60/100], Loss: 0.032333\nEpoch [80/100], Loss: 0.016779\nEpoch [100/100], Loss: 0.017931\n탐지된 이상치 윈도우 인덱스: [11 12 13 14 15 16 17 18 19 20 41 42 43 44 45 46 47 48 49 50 71 72 73 74\n 75 76 77 78 79 80]\n탐지된 이상치 포인트 인덱스: [np.int64(20), np.int64(50), np.int64(80)]\n실제 이상치 포함 윈도우 인덱스: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80]\n\n\n\n\n4.2. 베이스라인 모델 vs 개선 모델\n\n\n\n\n\n\n\n\n\n\n구분\n데이터 전처리\n과적합 방지\n하이퍼파라미터\n탐지된 이상치 (인덱스)\n\n\n\n\nWeek3 (베이스라인)\nSigmoid 활성화\n없음\n수동 설정\n[17 18 19 20 47 48 49 50] (윈도우)\n\n\nWeek4 (개선 모델)\nStandardScaler\nDropout\nOptuna 최적화\n[20, 50, 80] (단일 포인트)\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 8)) \n\n# --- 상단: 전체 데이터와 탐지 결과 ---\nplt.subplot(2, 1, 1)\nplt.plot(scaled_data, label='정규화된 데이터', alpha=0.8)\nplt.scatter(outliers, scaled_data[outliers], color='red', s=120, label='실제 이상치', marker='o', edgecolors='black', zorder=5)\n\n# 탐지된 단일 이상치 포인트를 표시\nif len(predicted_anomaly_points) &gt; 0:\n    # Check if indices are within bounds of scaled_data\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(scaled_data)]\n    plt.scatter(valid_indices, scaled_data[valid_indices],\n                color='orange', marker='x', s=120, linewidth=2, label='탐지된 이상치', zorder=5)\n\nplt.title('개선된 모델(Week4) 최종 이상 탐지 결과', fontsize=16)\nplt.xlabel('시간 스텝')\nplt.ylabel('정규화된 값')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\n# --- 하단: 재구성 오차와 임계값 ---\nplt.subplot(2, 1, 2)\nplt.plot(all_reconstruction_error, label='윈도우별 재구성 오차', color='blue')\nplt.axhline(y=threshold, color='r', linestyle='--', label=f'임계값 ({threshold:.4f})')\n\n# 임계값을 넘은 \"윈도우\"를 표시\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, all_reconstruction_error[anomaly_window_indices], c='red', s=100, label='이상치로 탐지된 윈도우', zorder=5)\n\nplt.title('윈도우별 재구성 오차', fontsize=16)\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/ABC_week04_model_optimization/index.html#결론",
    "href": "posts/ABC_week04_model_optimization/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화",
    "section": "5. 결론",
    "text": "5. 결론\n이번 4주차 포스트에서는 Week3에서 구현한 CNN 오토인코더 모델의 성능을 개선하기 위한 여정을 상세히 다루었습니다. StandardScaler를 이용해 데이터 전처리를 표준화하고, Dropout을 추가하여 과적합을 방지했습니다. 또한, Optuna 라이브러리를 활용하여 학습률, 드롭아웃 비율 등 최적의 하이퍼파라미터를 체계적으로 탐색했습니다.\n특히, 탐지 정확도를 높이기 위해 정상 데이터만으로 모델을 학습하고, 재구성 오차의 Quantile(백분위수)을 이용해 통계적으로 강건한 임계값을 설정했습니다. 최종적으로는 임계값을 넘는 윈도우 내에서 오차가 가장 큰 단일 지점을 이상치로 특정하는 방식으로, 실제 이상치인 20, 50, 80번 인덱스를 성공적으로 탐지할 수 있었습니다.\n물론, 학습 과정의 미세한 무작위성으로 인해 간혹 오탐(False Positive)이 발생하는 한계도 확인했습니다. 이는 향후 모델 구조 자체를 개선하거나 더 다양한 데이터를 활용하여 안정성을 높여야 할 과제로 남았습니다. 다음 포스트에서는 또 다른 접근법으로 이 문제를 해결해 나가겠습니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "beomdo's ML-DL blog",
    "section": "",
    "text": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n이전 주차에서 개발한 CNN 오토인코더 모델을 실제 Kaggle의 주택 전력 사용량 데이터에 적용하여, 현실 데이터에서 발생하는 이상 패턴을 탐지하는 과정을 다룹니다.\n\n\n\n\n\nJun 19, 2025\n\n\nBeomdo Park\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 4주차 - 모델 성능 개선 및 하이퍼파라미터 최적화\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\n지난주 CNN 오토인코더 모델의 한계를 분석하고, 성능 개선을 위한 다양한 방법과 하이퍼파라미터 최적화 과정을 기록합니다.\n\n\n\n\n\nJun 14, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\nPyTorch\n\n\n\nPyTorch를 사용하여 1D CNN 오토인코더 기반 시계열 이상 탐지 베이스라인 모델을 구현합니다.\n\n\n\n\n\nJun 8, 2025\n\n\nBeomdo Park\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지 - 머신러닝 기법 적용 실습\n\n\n\n\n\nJun 1, 2025\n\n\nBeomdo Park\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n[2025 ABC 프로젝트 멘토링 8기] 1주차 - 시계열 이상 탐지를 위한 EDA 및 전처리\n\n\n\nABC프로젝트멘토링\n\n유클리드소프트\n\n고용노동부\n\n대한상공회의소\n\n미래내일일경험사업\n\n\n\nPython을 활용한 시계열 데이터 이상 탐지를 위한 기본 EDA 및 전처리 방법을 다룹니다.\n\n\n\n\n\nMay 25, 2025\n\n\nBeomdo Park\n\n8 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "컴퓨터공학과 3학년입니다.\n\nData Scientist AI-powered problem solver who applies research and technology to real-world challenges."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html",
    "href": "posts/ABC_week03_cnn_baseline/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 세 번째 기술노트입니다. 이번 주는 시계열 데이터의 ’패턴’을 학습할 수 있는 딥러닝, 그중에서도 CNN을 활용한 이상 탐지의 첫걸음을 PyTorch로 구현해 보겠습니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "href": "posts/ABC_week03_cnn_baseline/index.html#시계열-데이터를-cnn에-입력하는-방법-윈도잉windowing",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)",
    "text": "1. 시계열 데이터를 CNN에 입력하는 방법: 윈도잉(Windowing)\n시계열 데이터를 CNN 모델에 입력하려면 연속된 데이터를 일정한 길이의 조각(window)으로 나누는 ‘슬라이딩 윈도우’ 기법이 필요합니다. 이 방법은 데이터의 시간적 패턴을 학습하는 데 유용합니다.\n\n슬라이딩 윈도우 구현\n아래는 numpy를 사용해 슬라이딩 윈도우를 구현하는 간단한 Python 함수입니다:\n\nimport numpy as np\n\ndef sliding_window(data, window_size, step_size=1):\n    \"\"\"시계열 데이터를 슬라이딩 윈도우로 변환\"\"\"\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 예제 데이터\ndata = np.sin(np.linspace(0, 20, 100))\nwindowed_data = sliding_window(data, window_size=10)\nprint(\"윈도우 형태:\", windowed_data.shape)\n\n윈도우 형태: (91, 10)"
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "href": "posts/ABC_week03_cnn_baseline/index.html#기본-이상-탐지-모델-cnn-오토인코더-autoencoder",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)",
    "text": "2. 기본 이상 탐지 모델: CNN 오토인코더 (Autoencoder)\n\n오토인코더란?\n오토인코더는 데이터를 압축(인코더)했다가 다시 복원(디코더)하도록 학습하는 딥러닝 모델입니다. 정상 데이터는 잘 복원되지만, 이상 데이터는 복원이 잘 되지 않아 재구성 오차가 커지는 특징을 활용합니다.\n\n\n모델 구조\n\n인코더 (Encoder): Conv1D와 MaxPooling1D 층을 사용해 입력 데이터의 특징을 추출하고 압축합니다.\n디코더 (Decoder): ConvTranspose1D (또는 Upsample + Conv1D) 층을 사용해 데이터를 복원합니다.\n\n\n\nPyTorch 구현\n아래는 PyTorch를 사용한 간단한 1D CNN 오토인코더 모델 구현입니다:\n\nimport torch\nimport torch.nn as nn\n\nclass CNNAutoencoder(nn.Module):\n    def __init__(self, input_shape): # input_shape: (sequence_length, num_features)\n        super(CNNAutoencoder, self).__init__()\n        # Encoder\n        # input_shape[1]은 특성 수 (in_channels로 사용)\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_shape[1], out_channels=32, kernel_size=3, padding=1)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/2로 감소\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2) # 시퀀스 길이 1/4로 감소\n\n        # Decoder\n        # 인코더에서 시퀀스 길이가 1/4로 줄었으므로, 디코더에서 원래 길이로 복원\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=4, stride=2, padding=1, output_padding=1)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1, output_padding=1)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_shape[1], kernel_size=3, padding=1) # 원본 특성 수로 복원\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_conv_final(x)\n        decoded = self.sigmoid(x)\n        return decoded\n\n# 모델 생성 및 컴파일은 data-generation 셀 이후로 이동합니다.\n# input_shape도 window_size를 사용하도록 수정됩니다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "href": "posts/ABC_week03_cnn_baseline/index.html#모델-학습-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "3. 모델 학습 및 이상치 탐지",
    "text": "3. 모델 학습 및 이상치 탐지\n\n데이터 생성\nWeek2에서 사용한 샘플 데이터를 기반으로 정상/비정상 데이터를 생성합니다:\n\n# numpy는 sliding_window_implementation 셀에서 이미 import 됨\n\n# 데이터 생성\nnp.random.seed(42)\ndata = np.sin(0.2 * np.arange(0, 100)) + np.random.normal(0, 0.1, 100)\noutliers = [20, 50, 80]\ndata[outliers] += [3, -3, 2]\n\n# 슬라이딩 윈도우 적용\nwindow_size = 10\nwindows = sliding_window(data, window_size) # (N, L) -&gt; (N, window_size)\nwindows = windows[..., np.newaxis]  # (N, L, C) -&gt; (N, window_size, 1)\n# PyTorch Conv1d는 (N, C, L) 입력을 기대하므로 차원 변경\nwindows = windows.transpose(0, 2, 1) # (N, C, L) -&gt; (N, 1, window_size)\nprint(f\"윈도우 데이터 형태 (N, C, L): {windows.shape}\")\n\n윈도우 데이터 형태 (N, C, L): (91, 1, 10)\n\n\n\nimport torch.optim as optim # PyTorch 옵티마이저\n\n# 모델 생성\n# input_shape은 (window_size, 1) 이어야 합니다. (sequence_length, num_features)\n# data-generation 셀에서 windows는 (N, 1, window_size) 형태로 준비됨.\n# CNNAutoencoder의 __init__은 input_shape=(window_size, 1)을 받아 input_shape[1]=1을 in_channels로 사용.\nmodel_input_shape = (window_size, 1) # (sequence_length, num_features)\nmodel = CNNAutoencoder(model_input_shape) # cnn-autoencoder-definition 셀에서 정의된 클래스 사용\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss() # 평균 제곱 오차 손실\n\nprint(\"PyTorch 모델 구조:\")\nprint(model)\n\nPyTorch 모델 구조:\nCNNAutoencoder(\n  (encoder_conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu1): ReLU()\n  (encoder_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n  (encoder_relu2): ReLU()\n  (encoder_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (decoder_conv_t1): ConvTranspose1d(16, 16, kernel_size=(4,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu1): ReLU()\n  (decoder_conv_t2): ConvTranspose1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n  (decoder_relu2): ReLU()\n  (decoder_conv_final): Conv1d(32, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n  (sigmoid): Sigmoid()\n)\n\n\n\n\n모델 학습\n정상 데이터만 사용해 모델을 학습합니다:\n\n# torch는 cnn-autoencoder-definition 셀에서 이미 import 됨\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# 정상 데이터로 학습\n# 'outliers'는 원본 'data' 배열의 인덱스입니다.\n# 'windows' 배열에서 이상치가 포함된 윈도우를 식별하여 제외합니다.\ncontaminated_window_indices = set()\n# 'outliers', 'window_size', 'windows' 변수는 이전 셀들에서 정의되어 있어야 합니다.\nfor outlier_data_idx in outliers: \n    start_contaminated_win_idx = max(0, outlier_data_idx - window_size + 1)\n    end_contaminated_win_idx = outlier_data_idx \n    \n    for win_idx in range(start_contaminated_win_idx, end_contaminated_win_idx + 1):\n        if win_idx &lt; len(windows): # 윈도우 인덱스가 유효한 범위 내에 있는지 확인\n            contaminated_window_indices.add(win_idx)\n\nnormal_windows_mask = np.ones(len(windows), dtype=bool)\nif contaminated_window_indices: # set이 비어있지 않은 경우에만 인덱싱\n    normal_windows_mask[list(contaminated_window_indices)] = False\n\nnormal_windows_np = windows[normal_windows_mask]\n\nif len(normal_windows_np) == 0:\n    print(\"경고: 학습에 사용할 정상 윈도우가 없습니다. Outlier 정의, window_size 또는 데이터 길이를 확인하세요.\")\nelse:\n    # PyTorch 데이터셋 및 로더 준비\n    normal_windows_torch = torch.tensor(normal_windows_np, dtype=torch.float32)\n    train_dataset = TensorDataset(normal_windows_torch) # 오토인코더는 입력과 타겟이 동일\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\n    # 모델 학습\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    model.to(device)\n    \n    epochs = 50 # 에포크 수 설정\n    print_every_epochs = 10\n\n    model.train() # 학습 모드\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_data_list in train_loader:\n            inputs = batch_data_list[0].to(device)\n            targets = inputs # 오토인코더의 타겟은 입력과 동일\n\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item() * inputs.size(0) # 배치 손실 누적 (loss.item()은 평균 손실)\n        \n        epoch_loss /= len(train_loader.dataset) # 에포크 평균 손실\n        if (epoch + 1) % print_every_epochs == 0:\n            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.6f}\")\n    print(\"모델 학습 완료.\")\n\nUsing device: cuda\nEpoch [10/50], Loss: 0.474734\nEpoch [20/50], Loss: 0.271595\nEpoch [30/50], Loss: 0.231434\nEpoch [40/50], Loss: 0.217045\nEpoch [50/50], Loss: 0.210623\n모델 학습 완료.\n\n\n\n\n재구성 오차 계산 및 이상치 탐지\n학습된 모델로 데이터를 복원하고, 재구성 오차를 계산합니다:\n\n# torch 및 numpy는 이전 셀들에서 이미 import 됨\n\n# 재구성 오차 계산\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() # 평가 모드\n\n# 전체 windows 데이터를 PyTorch 텐서로 변환하고 device로 이동\nall_windows_torch = torch.tensor(windows, dtype=torch.float32).to(device)\n\n# 메모리 부족을 방지하기 위해 배치 단위로 처리할 수 있으나, 현재 데이터는 작으므로 한번에 처리\nwith torch.no_grad(): # 그래디언트 계산 비활성화\n    reconstructed_torch = model(all_windows_torch)\n\n# 결과를 CPU로 옮기고 NumPy 배열로 변환\nreconstructed_np = reconstructed_torch.cpu().numpy()\n\n# MAE (Mean Absolute Error) 계산\n# 원본 windows (numpy 배열)와 reconstructed_np 모두 (N, 1, window_size) 형태\n# axis=(1, 2)는 채널과 시퀀스 길이에 대한 평균을 의미\nmae = np.mean(np.abs(windows - reconstructed_np), axis=(1, 2))\nprint(f\"계산된 MAE 값 (처음 5개): {mae[:5]}\")\n\n# 이상치 탐지를 위한 임계값 설정 (데이터 및 모델 성능에 따라 조정 필요)\n# 예: MAE의 평균 + (표준편차 * 특정 배수) 또는 분위수 사용\nthreshold = np.mean(mae) + 1.5 * np.std(mae) # 표준편차 배수를 2에서 1.5로 줄여 민감도 증가\nprint(f\"이상치 탐지 임계값 (MAE): {threshold:.4f}\")\n\nanomalies_indices_in_windows = np.where(mae &gt; threshold)[0] # 윈도우 배열 내의 인덱스\n\nprint(f\"이상치로 탐지된 윈도우의 수: {len(anomalies_indices_in_windows)}\")\nprint(f\"이상치로 탐지된 윈도우 인덱스: {anomalies_indices_in_windows}\")\n\n# 윈도우 인덱스를 원본 데이터 인덱스로 대략적으로 매핑 (윈도우의 시작점 기준)\n# 실제 이상치 발생 시점과 정확히 일치하지 않을 수 있음\nanomalies_approx_original_indices = anomalies_indices_in_windows \n# 좀 더 정확하게는 윈도우의 중간 지점 등을 고려할 수 있으나, 여기서는 시작점으로 단순화\n# anomalies_approx_original_indices = [idx + window_size // 2 for idx in anomalies_indices_in_windows]\nprint(f\"원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): {anomalies_approx_original_indices}\")\n\n계산된 MAE 값 (처음 5개): [0.13974112 0.11385794 0.07825096 0.08110552 0.07896732]\n이상치 탐지 임계값 (MAE): 0.9588\n이상치로 탐지된 윈도우의 수: 8\n이상치로 탐지된 윈도우 인덱스: [17 18 19 20 47 48 49 50]\n원본 데이터의 대략적인 이상치 인덱스 (윈도우 시작점 기준): [17 18 19 20 47 48 49 50]\n\n\n\n\n결과 시각화\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 4))\nplt.plot(data, label='원본 데이터', alpha=0.7) # 'data'는 data-generation에서 정의됨\nplt.scatter(outliers, data[outliers], color='red', s=100, label='실제 이상치 (Ground Truth)', marker='o', edgecolors='black') # 'outliers'는 data-generation에서 정의됨\n\n# anomalies_approx_original_indices가 비어있을 수 있으므로 확인\nif len(anomalies_approx_original_indices) &gt; 0:\n    # 탐지된 이상치 표시는 윈도우의 시작점을 기준으로 함\n    plt.scatter(anomalies_approx_original_indices, data[anomalies_approx_original_indices], \n                color='orange', marker='x', s=80, label='탐지된 이상치 (모델 예측)', alpha=0.8)\nelse:\n    print(\"탐지된 이상치가 없습니다.\")\n        \nplt.legend()\nplt.title('PyTorch CNN 오토인코더 기반 시계열 이상 탐지')\nplt.xlabel('시간 스텝')\nplt.ylabel('값')\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n# 재구성 오차(MAE) 시각화\nplt.figure(figsize=(10, 3))\nplt.plot(mae, label='재구성 오차 (MAE)', color='green')\nplt.axhline(threshold, color='red', linestyle='--', label=f'임계값 ({threshold:.2f})')\nplt.title('윈도우별 재구성 오차 (MAE) 및 임계값')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('MAE')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\nPyTorch CNN 오토인코더 기반 이상 탐지 결과\n\n\n\n\n\n\n\n\n\n\n\n\n\n탐지 결과 분석 및 고려사항\n시각화 결과와 재구성 오차 검토 시 다음 사항을 고려해야 한다.\n\n실제 이상치 vs. 탐지 이상치:\n\ndata-generation 단계에서 의도적으로 넣은 실제 이상치(outliers = [20, 50, 80])와 모델의 탐지 결과는 다를 수 있다.\n모든 실제 이상치가 탐지되지 않거나, 정상이 이상치로 잘못 탐지될 가능성이 항상 존재한다.\n현 예제는 임계값(np.mean(mae) + 1.5 * np.std(mae)) 조정을 통해 최소 하나의 이상치를 탐지하도록 유도했다. 실제 상황에서는 모델 성능, 데이터 특성, window_size, 임계값 설정에 따라 결과가 크게 달라진다.\n\n윈도우 경계 효과 (Edge Effects):\n\n시계열 데이터의 시작과 끝 부분 윈도우는 내부 윈도우에 비해 정보가 불완전할 수 있다 (이전/이후 데이터 부재).\nCNN 모델, 특히 패딩 사용 시, 경계 영역 윈도우는 학습된 주 정상 패턴과 달라 재구성 오차가 상대적으로 커질 수 있다.\n결과적으로, 시계열 양 끝부분에서 이상치가 아닌데도 이상치로 탐지되는 경향이 나타날 수 있다. MAE 그래프에서 초반 또는 후반부에 높은 오차가 관찰된다면 이 효과를 의심해볼 수 있다.\n\nwindow_size의 중요성:\n\nwindow_size는 모델이 학습할 패턴의 길이를 결정한다.\n너무 작으면 장기 패턴 파악이 어렵고, 너무 크면 짧은 순간의 이상치를 놓치거나 정상 변동에도 민감하게 반응할 수 있다.\n현재 window_size=10으로 설정했다. 데이터 특성에 맞춰 이 값을 조정하며 실험하는 과정이 중요하다.\n\n모델 및 임계값의 한계:\n\n여기서 사용한 CNN 오토인코더는 비교적 단순한 모델이다.\n더 복잡한 패턴이나 다양한 유형의 이상치를 탐지하려면 모델 구조 개선(예: LSTM, Transformer 기반 오토인코더)이나 다른 접근법을 고려해야 한다.\n고정 임계값 대신 동적 임계값을 사용하거나, 통계적 검정 기법을 결합하는 것도 탐지 성능을 높이는 데 도움이 될 수 있다.\n\n\n이런 점들을 고려해 모델 결과를 해석해야 하며, 실제 문제 적용 시에는 충분한 검증과 실험이 필수다."
  },
  {
    "objectID": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "href": "posts/ABC_week03_cnn_baseline/index.html#결론-및-다음-단계",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 3주차 - CNN으로 시계열 이상 탐지 (PyTorch)",
    "section": "결론 및 다음 단계",
    "text": "결론 및 다음 단계\n이번 주에는 PyTorch로 간단한 1D CNN 오토인코더를 만들고, 시계열 이상 탐지를 수행했다. 이 모델은 시계열 이상 탐지의 괜찮은 시작점이 될 수 있다. 재구성 오차를 기반으로 이상치를 찾는 과정과, 임계값 설정에 따라 탐지 결과가 어떻게 달라지는지 확인했다.\n다음 포스트에서는 실제 산업 데이터를 사용해 모델을 학습시키고, 성능을 개선할 다양한 방법(예: 더 복잡한 모델 구조, 다른 유형의 오토인코더, 동적 임계값 설정 등)을 살펴볼 예정이다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html",
    "href": "posts/ABC_week05_real_data_analysis/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "",
    "text": "안녕하세요, ABC 프로젝트 멘토링 8기 다섯 번째 기술노트입니다. Week04에서 개발한 CNN 오토인코더 모델을 실제 Kaggle 공개 데이터셋(주택 전력 사용량 3년치)에 적용해, 실전 환경에서의 이상 탐지 성능과 한계를 점검합니다. 이 과정을 통해, 이론적 모델이 실제 데이터에서 어떻게 동작하는지, 그리고 실무에서 마주칠 수 있는 문제와 해결책을 탐구합니다."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-소개-및-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "1. 데이터 소개 및 준비",
    "text": "1. 데이터 소개 및 준비\n이번 주에 사용할 데이터는 Kaggle에 공개된 ‘Residential Power Usage 3-Years Data’입니다. 한 가정의 3년간 전력 사용량이 분 단위로 기록된 시계열 데이터로, 실제 환경에서 발생하는 다양한 패턴과 이상 현상을 포함하고 있습니다.\n먼저, GitHub Raw URL을 통해 데이터를 불러오고, 시계열 분석을 위해 날짜 컬럼을 인덱스로 변환한 뒤, 전력 사용량 컬럼만 추출합니다.\n\n\n데이터 로딩 시작...\n데이터 로딩 완료.\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 35952 entries, 0 to 35951\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   StartDate    35952 non-null  object \n 1   Value (kWh)  35952 non-null  float64\n 2   day_of_week  35952 non-null  int64  \n 3   notes        35952 non-null  object \ndtypes: float64(1), int64(1), object(2)\nmemory usage: 1.1+ MB\n\n\n데이터는 ‘StartDate’, ‘Value (kWh)’, ‘day_of_week’, ‘notes’ 등의 컬럼으로 구성되어 있습니다. 시계열 분석을 위해 ‘StartDate’를 datetime 형식으로 변환하고 인덱스로 설정한 뒤, ’Value (kWh)’ 컬럼만 사용하겠습니다. 전체 데이터를 사용하여 분석을 진행합니다.\n\nprint(\"데이터 전처리 시작...\")\n# 'StartDate'를 datetime으로 변환하고 인덱스로 설정\ndf['StartDate'] = pd.to_datetime(df['StartDate'])\ndf = df.set_index('StartDate')\n\n# 시간순으로 정렬\ndf.sort_index(inplace=True)\nprint(\"데이터 시간순 정렬 완료.\")\n\n# 'Value (kWh)' 컬럼만 선택\n# df.info() 결과에 따라 'Value (kWh)'를 사용합니다.\nvalue_col = 'Value (kWh)'\ndf_value = df[[value_col]].copy()\n\n# 데이터 시각화 (전체 기간)\nplt.figure(figsize=(15, 6))\nplt.plot(df_value.index, df_value[value_col], label='전력 사용량 (전체 기간)')\nplt.title('시간에 따른 전력 사용량 (전체 기간)')\nplt.xlabel('날짜')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nprint(f\"전체 데이터 크기: {df_value.shape}\")\n\n데이터 전처리 시작...\n데이터 시간순 정렬 완료.\n\n\n\n\n\n\n\n\n\n전체 데이터 크기: (35952, 1)"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-윈도우-생성",
    "href": "posts/ABC_week05_real_data_analysis/index.html#데이터-전처리-및-윈도우-생성",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "2. 데이터 전처리 및 윈도우 생성",
    "text": "2. 데이터 전처리 및 윈도우 생성\n시계열 데이터의 추세(Trend) 영향을 줄이고, 각 윈도우의 지역적 패턴에 집중하기 위해 윈도우별로 독립적인 정규화를 수행합니다. 이 방법은 데이터의 전반적인 스케일 변화에 덜 민감한 모델을 만드는 데 도움이 됩니다.\n\nprint(\"윈도우 생성 및 정규화 시작...\")\n# 원본 데이터에서 바로 윈도우 생성\nraw_data = df_value[value_col].values\n\n# 슬라이딩 윈도우 생성 함수\ndef sliding_window(data, window_size, step_size=1):\n    n_windows = (len(data) - window_size) // step_size + 1\n    return np.array([data[i:i+window_size] for i in range(0, n_windows * step_size, step_size)])\n\n# 윈도우 생성 (윈도우 크기는 60분(1시간)으로 설정)\nwindow_size = 60\nraw_windows = sliding_window(raw_data, window_size)\n\n# 윈도우별 정규화\nscaled_windows = np.array([StandardScaler().fit_transform(window.reshape(-1, 1)).flatten() for window in raw_windows])\n\n# PyTorch 텐서 변환 (shape: [batch_size, channels, sequence_length])\nall_windows_torch = torch.from_numpy(scaled_windows).unsqueeze(1).float()  # [N, 1, window_size]\n\nprint(f\"생성된 윈도우 데이터 shape: {all_windows_torch.shape}\")\n\n윈도우 생성 및 정규화 시작...\n생성된 윈도우 데이터 shape: torch.Size([35893, 1, 60])"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#모델-학습을-위한-데이터-분할-및-하이퍼파라미터-최적화",
    "href": "posts/ABC_week05_real_data_analysis/index.html#모델-학습을-위한-데이터-분할-및-하이퍼파라미터-최적화",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "3. 모델 학습을 위한 데이터 분할 및 하이퍼파라미터 최적화",
    "text": "3. 모델 학습을 위한 데이터 분할 및 하이퍼파라미터 최적화\n실제 데이터에서는 초반부와 이후 구간의 패턴이 다를 수 있습니다(구간 변화, Regime Change). 전체 데이터를 학습하면 변화 자체를 이상으로 탐지할 수 있으므로, 패턴이 안정화된 구간만을 ’정상’으로 정의하고 해당 구간 데이터로만 모델을 학습합니다.\n실제 적용에서는 더 보수적인 접근이 필요하므로, 전체 데이터의 20-80% 구간을 안정적인 정상 구간으로 설정하고, 하이퍼파라미터도 과적합을 방지하도록 조정합니다.\n\n# 더 보수적인 학습 구간 설정 (전체 데이터의 20-80% 구간 사용)\ntotal_windows = len(all_windows_torch)\ntrain_start_idx = int(total_windows * 0.2)  # 20% 지점부터\ntrain_end_idx = int(total_windows * 0.8)    # 80% 지점까지\ntrain_windows_torch = all_windows_torch[train_start_idx:train_end_idx]\n\nprint(f\"전체 윈도우 수: {all_windows_torch.shape[0]}\")\nprint(f\"학습 구간: {train_start_idx} ~ {train_end_idx}\")\nprint(f\"학습에 사용할 윈도우 수: {train_windows_torch.shape[0]}\")\n\n전체 윈도우 수: 35893\n학습 구간: 7178 ~ 28714\n학습에 사용할 윈도우 수: 21536\n\n\n\nclass CNNAutoencoderWithDropout(nn.Module):\n    def __init__(self, input_channels=1, dropout_rate=0.1):\n        super(CNNAutoencoderWithDropout, self).__init__()\n        # Encoder\n        self.encoder_conv1 = nn.Conv1d(in_channels=input_channels, out_channels=32, kernel_size=7, padding=3)\n        self.encoder_relu1 = nn.ReLU()\n        self.encoder_drop1 = nn.Dropout(dropout_rate)\n        self.encoder_pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n        self.encoder_conv2 = nn.Conv1d(in_channels=32, out_channels=16, kernel_size=7, padding=3)\n        self.encoder_relu2 = nn.ReLU()\n        self.encoder_drop2 = nn.Dropout(dropout_rate)\n        self.encoder_pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n\n        # Decoder\n        self.decoder_conv_t1 = nn.ConvTranspose1d(in_channels=16, out_channels=16, kernel_size=2, stride=2)\n        self.decoder_relu1 = nn.ReLU()\n        self.decoder_drop3 = nn.Dropout(dropout_rate)\n        self.decoder_conv_t2 = nn.ConvTranspose1d(in_channels=16, out_channels=32, kernel_size=2, stride=2)\n        self.decoder_relu2 = nn.ReLU()\n        self.decoder_drop4 = nn.Dropout(dropout_rate)\n        self.decoder_conv_final = nn.Conv1d(in_channels=32, out_channels=input_channels, kernel_size=7, padding=3)\n\n    def forward(self, x):\n        # Encoder\n        x = self.encoder_conv1(x)\n        x = self.encoder_relu1(x)\n        x = self.encoder_drop1(x)\n        x = self.encoder_pool1(x)\n        x = self.encoder_conv2(x)\n        x = self.encoder_relu2(x)\n        x = self.encoder_drop2(x)\n        encoded = self.encoder_pool2(x)\n        \n        # Decoder\n        x = self.decoder_conv_t1(encoded)\n        x = self.decoder_relu1(x)\n        x = self.decoder_drop3(x)\n        x = self.decoder_conv_t2(x)\n        x = self.decoder_relu2(x)\n        x = self.decoder_drop4(x)\n        x = self.decoder_conv_final(x)\n        return x\n\n# Optuna 최적화 결과(best trial) 하이퍼파라미터 적용\n# lr=0.007392392751789862, dropout_rate=0.10244576584681189, optimizer=Adam\nmodel = CNNAutoencoderWithDropout(input_channels=1, dropout_rate=0.10244576584681189)\noptimizer = optim.Adam(model.parameters(), lr=0.007392392751789862)\ncriterion = nn.MSELoss()\n\n# 안정 구간 데이터로 학습\ndataset = TensorDataset(train_windows_torch)\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # 배치 크기 감소\n\nprint(\"모델 학습 시작 (개선된 안정 구간 및 하이퍼파라미터)...\")\nepochs = 15  # 에포크 증가\nfor epoch in range(epochs):\n    model.train()\n    epoch_loss = 0\n    for data in data_loader:\n        inputs = data[0]\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, inputs)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    if (epoch + 1) % 5 == 0:\n        avg_loss = epoch_loss / len(data_loader)\n        print(f\"Epoch [{epoch+1}/{epochs}], Avg Loss: {avg_loss:.6f}\")\nprint(\"모델 학습 완료.\")\n\n모델 학습 시작 (개선된 안정 구간 및 하이퍼파라미터)...\nEpoch [5/15], Avg Loss: 0.072463\nEpoch [10/15], Avg Loss: 0.069328\nEpoch [15/15], Avg Loss: 0.068391\n모델 학습 완료."
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#모델-평가-및-이상치-탐지",
    "href": "posts/ABC_week05_real_data_analysis/index.html#모델-평가-및-이상치-탐지",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "4. 모델 평가 및 이상치 탐지",
    "text": "4. 모델 평가 및 이상치 탐지\n학습된 모델로 전체 데이터의 재구성 오차를 계산하고, 학습 데이터의 재구성 오차 분포를 기반으로 임계값(Quantile 기준)을 설정해 이상치를 탐지합니다.\n\nmodel.eval()\n\n# 1. 학습 데이터의 재구성 오차를 기반으로 임계값 설정\nwith torch.no_grad():\n    reconstructed_train = model(train_windows_torch)\n    error_train = torch.mean((train_windows_torch - reconstructed_train)**2, dim=(1, 2))\n    train_reconstruction_error = error_train.numpy()\n\nquantile_level = 0.99  # 더 관대한 임계값 설정\nthreshold = np.quantile(train_reconstruction_error, quantile_level)\nprint(f\"임계값 (학습 데이터 기준 {quantile_level*100:.1f}% Quantile): {threshold:.6f}\")\n\n# 2. 전체 데이터에 대한 재구성 오차 계산\nwith torch.no_grad():\n    reconstructed_all = model(all_windows_torch)\n    error_all = torch.mean((all_windows_torch - reconstructed_all)**2, dim=(1, 2))\n    reconstruction_error = error_all.numpy()\n\n# 3. 임계값을 기준으로 전체 데이터에서 이상치 탐지\nanomaly_window_indices = np.where(reconstruction_error &gt; threshold)[0]\n\n# 윈도우 내 최대 오차 지점 탐색\npointwise_error = ((all_windows_torch - reconstructed_all)**2).squeeze().numpy()\npredicted_anomaly_points = []\nfor window_idx in anomaly_window_indices:\n    max_error_idx_in_window = np.argmax(pointwise_error[window_idx])\n    absolute_idx = window_idx + max_error_idx_in_window\n    predicted_anomaly_points.append(absolute_idx)\n\npredicted_anomaly_points = sorted(list(set(predicted_anomaly_points)))\nprint(f\"탐지된 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n\n# 안전성 체크: 유효한 인덱스만 필터링\npredicted_anomaly_points = [idx for idx in predicted_anomaly_points if idx &lt; len(raw_data)]\nprint(f\"유효한 이상치 포인트 수: {len(predicted_anomaly_points)}\")\n\n임계값 (학습 데이터 기준 99.0% Quantile): 0.128954\n탐지된 이상치 포인트 수: 134\n유효한 이상치 포인트 수: 134\n\n\n\n탐지 결과 시각화\n이상치로 탐지된 구간과 원본 데이터를 함께 시각화하여, 모델이 어떤 패턴을 ’이상’으로 간주했는지 확인합니다.\n\n# 시각화를 위해 원본 데이터 사용\noriginal_data = df_value[value_col].values\n\n# 3행 1열의 subplot 구성으로 더 상세한 분석 제공\nplt.figure(figsize=(15, 15))\n\n# 상단: 전체 데이터와 탐지 결과\nplt.subplot(3, 1, 1)\nplt.plot(original_data, label='원본 전력 사용량', alpha=0.7, color='blue', linewidth=0.8)\n\nif len(predicted_anomaly_points) &gt; 0:\n    # 유효한 인덱스만 사용\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(original_data)]\n    if len(valid_indices) &gt; 0:\n        plt.scatter(valid_indices, original_data[valid_indices],\n                    color='red', marker='x', s=80, linewidth=2, label=f'탐지된 이상치 ({len(valid_indices)}개)', zorder=5)\n\nplt.title('실제 전력 사용량 데이터 이상 탐지 결과 (전체)', fontsize=14, fontweight='bold')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 중간: 재구성 오차와 임계값\nplt.subplot(3, 1, 2)\nplt.plot(reconstruction_error, label='윈도우별 재구성 오차', color='steelblue', linewidth=1)\nplt.axhline(y=threshold, color='red', linestyle='--', linewidth=2, \n           label=f'임계값 ({threshold:.4f})')\nif len(anomaly_window_indices) &gt; 0:\n    plt.scatter(anomaly_window_indices, reconstruction_error[anomaly_window_indices], \n               c='red', s=60, alpha=0.8, label=f'이상 윈도우 ({len(anomaly_window_indices)}개)', zorder=5)\n\nplt.title('윈도우별 재구성 오차 분포', fontsize=14, fontweight='bold')\nplt.xlabel('윈도우 인덱스')\nplt.ylabel('재구성 오차 (MSE)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\n# 하단: 세분화된 구간 시각화 (1개월치 데이터)\nplt.subplot(3, 1, 3)\n# 전체 데이터에서 1개월치 정도 (약 44,640분 = 31일 * 24시간 * 60분)\nmonth_data_points = min(44640, len(original_data))\nstart_idx = 0\nend_idx = month_data_points\n\n# 1개월치 데이터 시각화\nmonth_data = original_data[start_idx:end_idx]\nplt.plot(range(start_idx, end_idx), month_data, \n         label='전력 사용량 (초기 1개월)', alpha=0.8, color='navy', linewidth=1)\n\n# 해당 구간의 이상치만 표시\nmonth_anomaly_count = 0\nif len(predicted_anomaly_points) &gt; 0:\n    valid_indices = [i for i in predicted_anomaly_points if i &lt; len(original_data)]\n    month_anomalies = [i for i in valid_indices if start_idx &lt;= i &lt; end_idx]\n    month_anomaly_count = len(month_anomalies)\n    if month_anomaly_count &gt; 0:\n        plt.scatter(month_anomalies, original_data[month_anomalies],\n                    color='red', marker='o', s=100, alpha=0.8, \n                    label=f'1개월 구간 이상치 ({month_anomaly_count}개)', zorder=5)\n\nplt.title('세분화된 이상 탐지 결과 (초기 1개월 구간)', fontsize=14, fontweight='bold')\nplt.xlabel('시간 스텝')\nplt.ylabel('사용량 (kWh)')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.5)\n\nplt.tight_layout()\nplt.show()\n\n# 탐지 결과 요약 출력\nprint(f\"\\n=== 이상 탐지 결과 요약 ===\")\nprint(f\"전체 데이터 길이: {len(original_data):,}분\")\nprint(f\"전체 탐지된 이상치: {len(valid_indices) if 'valid_indices' in locals() else 0}개\")\nprint(f\"1개월 구간 이상치: {month_anomaly_count}개\")\nprint(f\"이상치 비율: {(len(valid_indices) if 'valid_indices' in locals() else 0) / len(original_data) * 100:.3f}%\")\n\n\n\n\n\n\n\n\n\n=== 이상 탐지 결과 요약 ===\n전체 데이터 길이: 35,952분\n전체 탐지된 이상치: 134개\n1개월 구간 이상치: 134개\n이상치 비율: 0.373%"
  },
  {
    "objectID": "posts/ABC_week05_real_data_analysis/index.html#결론",
    "href": "posts/ABC_week05_real_data_analysis/index.html#결론",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 5주차 - 실제 전력 사용량 데이터를 활용한 이상 탐지",
    "section": "5. 결론",
    "text": "5. 결론\n이번 Week05에서는 지난 4주간 개발한 CNN 오토인코더 모델을 실제 Kaggle 데이터에 적용하여, 이론적 모델의 실무 적용 과정에서 발생하는 문제점들과 해결 방안을 탐구하였습니다.\n\n핵심 변경사항\n\n학습 구간 설정: 전체 데이터의 20-80% 구간만 학습에 사용함으로써 과탐지 문제를 효과적으로 해결\n하이퍼파라미터 최적화: Optuna를 통한 체계적 튜닝으로 lr=0.0074, dropout=0.102 등 최적값 도출\n임계값 조정: 99.5%에서 99% quantile로 조정하여 정상 구간의 오탐 현상 개선\n\n\n\n실무 적용 교훈\n실제 프로젝트에서는 모델 성능과 함께 도메인 지식 활용과 시각적 검증이 필수적입니다. 수치적 결과만으로는 모델의 실효성을 판단하기 어려우며, 반드시 시각화를 통한 검증 과정이 필요합니다.\n\n\n한계 및 향후 개선 방향\n\n정량적 평가를 위한 라벨 데이터 확보 필요\n다변량 모델 확장을 통한 복합적 요인 고려\n윈도우 크기 자동 최적화 알고리즘 개발\n실시간 탐지 시스템 구축 방안 연구\n\n\n\n마무리\nWeek01의 EDA부터 Week05의 실제 데이터 적용까지 시계열 이상 탐지의 전 과정을 체계적으로 수행하였습니다. 특히 이론적 모델과 실제 데이터 간의 성능 차이를 확인하고 이를 해결하는 과정에서 실무 적용을 위한 중요한 인사이트를 얻을 수 있었습니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html",
    "href": "posts/ABC_week02_time_series_anomaly/index.html",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "",
    "text": "안녕하세요 이번 포스트는 ABC 프로젝트 멘토링 8기 2주차 실습 기록입니다. 지난주엔 시계열 데이터 EDA랑 전처리만 했는데, 이번엔 간단한 머신러닝 모델로 이상치 탐지 기법을 소개하려 합니다."
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#데이터-준비",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "1. 데이터 준비",
    "text": "1. 데이터 준비\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nnp.random.seed(42)\nt = np.arange(0, 100, 1)\ny = np.sin(0.2 * t) + np.random.normal(0, 0.2, size=len(t))\n# 여러 위치에 인위적으로 이상치 추가\noutlier_indices = [15, 35, 55, 75, 90]\noutlier_values = [2, -2, 2.5, -2.5, 3]\nfor idx, val in zip(outlier_indices, outlier_values):\n    y[idx] += val\ndf = pd.DataFrame({'time': t, 'value': y})\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df.loc[outlier_indices, 'time'], df.loc[outlier_indices, 'value'], color='red', label='부여한 이상값')\nplt.legend()\nplt.title('이상값이 포함된 시계열 데이터')\nplt.show()\n\n\n\n\n이상값이 포함된 시계열 데이터"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#머신러닝-기반-이상-탐지-isolation-forest-dbscan-one-class-svm",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)",
    "text": "2. 머신러닝 기반 이상 탐지 (Isolation Forest, DBSCAN, One-Class SVM)\n\n모델별 특징 및 한계\n\n\n\n\n\n\n\n\n모델\n장점\n한계/주의점\n\n\n\n\nIsolation Forest\n대용량/고차원 데이터에 강함, 빠름\n이상치 비율(contamination) 추정 필요\n\n\nDBSCAN\n군집/밀도 기반, 파라미터 직관적\neps, min_samples에 민감, 1차원 한계\n\n\nOne-Class SVM\n비선형 경계, 소규모 데이터에 적합\n느릴 수 있음, 파라미터 튜닝 필요\n\n\n\n\n\nIsolation Forest\n\nfrom sklearn.ensemble import IsolationForest\nmodel = IsolationForest(contamination=0.05, random_state=42)\ndf['anomaly_isof'] = model.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_isof']==-1]['time'], df[df['anomaly_isof']==-1]['value'], color='red', label='탐지된 이상값')\nplt.legend()\nplt.title('Isolation Forest 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nIsolation Forest 기반 이상 탐지 결과\n\n\n\n\n\n\nDBSCAN (밀도 기반 이상 탐지)\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(df[['value']])\ndbscan = DBSCAN(eps=0.25, min_samples=3)  # eps와 min_samples를 조정해 민감도 조정\ndf['anomaly_dbscan'] = dbscan.fit_predict(X_scaled)\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_dbscan']==-1]['time'], df[df['anomaly_dbscan']==-1]['value'], color='orange', label='탐지된 이상값(DBSCAN)')\nplt.legend()\nplt.title('DBSCAN 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nDBSCAN 기반 이상 탐지 결과\n\n\n\n\n\n\nOne-Class SVM (서포트 벡터 머신 기반 이상 탐지)\n\nfrom sklearn.svm import OneClassSVM\n# 기본 파라미터로는 이상치 탐지가 잘 안 됨 (F1이 0.14 수준)\nocsvm = OneClassSVM(nu=0.05, kernel='rbf', gamma='auto')\ndf['anomaly_ocsvm'] = ocsvm.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm']==-1]['time'], df[df['anomaly_ocsvm']==-1]['value'], color='purple', label='탐지된 이상값(OCSVM)')\nplt.legend()\nplt.title('One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\nOne-Class SVM 기반 이상 탐지 결과\n\n\n\n\n\nSVM 파라미터 튜닝 시도\n\n# gamma 값을 더 크게, nu 값을 더 높게 조정해서 민감도를 높임\nocsvm_tuned = OneClassSVM(nu=0.12, kernel='rbf', gamma=2)\ndf['anomaly_ocsvm_tuned'] = ocsvm_tuned.fit_predict(df[['value']])\n\n\nplt.figure(figsize=(10,4))\nplt.plot(df['time'], df['value'], label='시계열 데이터')\nplt.scatter(df[df['anomaly_ocsvm_tuned']==-1]['time'], df[df['anomaly_ocsvm_tuned']==-1]['value'], color='blue', label='탐지된 이상값(튜닝 SVM)')\nplt.legend()\nplt.title('튜닝된 One-Class SVM 기반 이상 탐지 결과')\nplt.show()\n\n\n\n\n튜닝된 One-Class SVM 이상 탐지 결과\n\n\n\n\n\n\n\n이상치 탐지 및 평가지표(Precision, Recall, F1)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef anomaly_metrics(true_outliers, pred_outliers, n):\n    true = [1 if i in true_outliers else 0 for i in range(n)]\n    pred = [1 if i in pred_outliers else 0 for i in range(n)]\n    p = precision_score(true, pred)\n    r = recall_score(true, pred)\n    f1 = f1_score(true, pred)\n    return p, r, f1\n\nn = len(df)\ntrue_outliers = outlier_indices\npred_isof = df.index[df['anomaly_isof']==-1].tolist()\np_isof, r_isof, f1_isof = anomaly_metrics(true_outliers, pred_isof, n)\npred_dbscan = df.index[df['anomaly_dbscan']==-1].tolist()\np_dbscan, r_dbscan, f1_dbscan = anomaly_metrics(true_outliers, pred_dbscan, n)\npred_ocsvm = df.index[df['anomaly_ocsvm']==-1].tolist()\np_ocsvm, r_ocsvm, f1_ocsvm = anomaly_metrics(true_outliers, pred_ocsvm, n)\npred_ocsvm_tuned = df.index[df['anomaly_ocsvm_tuned']==-1].tolist()\np_ocsvm_t, r_ocsvm_t, f1_ocsvm_t = anomaly_metrics(true_outliers, pred_ocsvm_tuned, n)\n\nprint(f\"Isolation Forest - Precision: {p_isof:.2f}, Recall: {r_isof:.2f}, F1: {f1_isof:.2f}\")\nprint(f\"DBSCAN           - Precision: {p_dbscan:.2f}, Recall: {r_dbscan:.2f}, F1: {f1_dbscan:.2f}\")\nprint(f\"One-Class SVM    - Precision: {p_ocsvm:.2f}, Recall: {r_ocsvm:.2f}, F1: {f1_ocsvm:.2f}\")\nprint(f\"튜닝 SVM         - Precision: {p_ocsvm_t:.2f}, Recall: {r_ocsvm_t:.2f}, F1: {f1_ocsvm_t:.2f}\")\n\nIsolation Forest - Precision: 1.00, Recall: 1.00, F1: 1.00\nDBSCAN           - Precision: 1.00, Recall: 1.00, F1: 1.00\nOne-Class SVM    - Precision: 0.14, Recall: 0.40, F1: 0.21\n튜닝 SVM         - Precision: 0.21, Recall: 1.00, F1: 0.34"
  },
  {
    "objectID": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "href": "posts/ABC_week02_time_series_anomaly/index.html#결과-해석-및-정리",
    "title": "[2025 ABC 프로젝트 멘토링 8기] 2주차 - 시계열 이상 탐지와 머신러닝 기초 적용",
    "section": "3. 결과 해석 및 정리",
    "text": "3. 결과 해석 및 정리\n\nOne-Class SVM은 기본 파라미터로는 이상치 탐지가 잘 되지 않았으나, gamma와 nu를 조정해 튜닝하면 성능이 개선되는 것을 확인할 수 있다. 이 과정에서 파라미터 튜닝의 중요성을 경험했다.\n각 모델별로 이상치 탐지 결과와 평가지표(Precision, Recall, F1)가 다르게 나타난다. Isolation Forest는 인위적으로 넣은 이상치를 대부분 탐지했고, DBSCAN은 파라미터에 따라 민감하게 반응한다. One-Class SVM은 데이터 분포와 파라미터에 따라 결과가 크게 달라진다.\nPrecision(정밀도), Recall(재현율), F1-score는 모델의 이상치 탐지 성능을 종합적으로 평가하는 지표로, 실제 데이터 분석에서는 여러 방법을 비교하고 도메인 지식과 함께 해석하는 것이 중요하다."
  }
]